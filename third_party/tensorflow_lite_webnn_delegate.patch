diff --git a/tensorflow/lite/BUILD b/tensorflow/lite/BUILD
index 8414dd7fe67..4e6bcc44d97 100644
--- a/tensorflow/lite/BUILD
+++ b/tensorflow/lite/BUILD
@@ -690,6 +690,18 @@ cc_library(
     alwayslink = 1,
 )
 
+cc_library(
+    name = "tflite_with_webnn",
+    srcs = ["tflite_with_webnn.cc"],
+    copts = tflite_copts() + tflite_copts_warnings(),
+    linkstatic = True,
+    deps = [
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/delegates/webnn:webnn_delegate",
+    ],
+    alwayslink = 1,
+)
+
 # Enables applying XNNPACK delegate for float models in TFLite runtime.
 # WARNING: This build flag is experimental and subject to change.
 config_setting(
diff --git a/tensorflow/lite/CMakeLists.txt b/tensorflow/lite/CMakeLists.txt
index 40f9485b5d6..18421043b2c 100644
--- a/tensorflow/lite/CMakeLists.txt
+++ b/tensorflow/lite/CMakeLists.txt
@@ -69,6 +69,7 @@ option(TFLITE_ENABLE_GPU "Enable GPU" OFF)
 option(TFLITE_ENABLE_METAL "Enable Metal delegate (iOS only)" OFF)
 option(TFLITE_ENABLE_XNNPACK "Enable XNNPACK backend" ON)
 option(TFLITE_ENABLE_EXTERNAL_DELEGATE "Enable External Delegate backend" ON)
+option(TFLITE_ENABLE_WEBNN "Enable WebNN backend" OFF)
 
 option(TFLITE_KERNEL_TEST "Enable tflite kernel unit test" OFF)
 if(TFLITE_KERNEL_TEST AND ${CMAKE_CROSSCOMPILING})
@@ -399,6 +400,17 @@ if(TFLITE_ENABLE_XNNPACK)
   )
   list(APPEND TFLITE_TARGET_PUBLIC_OPTIONS "-DTFLITE_BUILD_WITH_XNNPACK_DELEGATE")
 endif()
+if(TFLITE_ENABLE_WEBNN)
+  include_directories(
+    "$ENV{WEBNN_NATIVE_DIR}/out/Release/gen/src/include"
+    "$ENV{WEBNN_NATIVE_DIR}/src/include"
+  )
+  populate_tflite_source_vars("delegates/webnn"
+    TFLITE_DELEGATES_WEBNN_SRCS
+    FILTER ".*(_test|_tester)\\.(cc|h)"
+  )
+  list(APPEND TFLITE_TARGET_PUBLIC_OPTIONS "-DTFLITE_BUILD_WITH_WEBNN_DELEGATE")
+endif()
 if(TFLITE_ENABLE_EXTERNAL_DELEGATE)
   populate_tflite_source_vars("delegates/external"
     TFLITE_DELEGATES_EXTERNAL_SRCS
@@ -475,6 +487,7 @@ add_library(tensorflow-lite
   ${TFLITE_DELEGATES_SRCS}
   ${TFLITE_DELEGATES_XNNPACK_SRCS}
   ${TFLITE_DELEGATES_EXTERNAL_SRCS}
+  ${TFLITE_DELEGATES_WEBNN_SRCS}
   ${TFLITE_EXPERIMENTAL_RESOURCE_SRCS}
   ${TFLITE_EXPERIMENTAL_RUY_PROFILER_SRCS}
   ${TFLITE_EXPERIMENTAL_RUY_SRCS}
diff --git a/tensorflow/lite/delegates/external/BUILD b/tensorflow/lite/delegates/external/BUILD
index 192091141be..a914f68c672 100644
--- a/tensorflow/lite/delegates/external/BUILD
+++ b/tensorflow/lite/delegates/external/BUILD
@@ -13,6 +13,11 @@
 # limitations under the License.
 # ==============================================================================
 
+load(
+    "//tensorflow/lite:build_def.bzl",
+    "tflite_cc_shared_object",
+)
+
 package(
     default_visibility = [
         "//visibility:public",
@@ -31,6 +36,31 @@ cc_library(
     ],
 )
 
+tflite_cc_shared_object(
+    name = "external_delegate_obj",
+    linkopts = select({
+        "//tensorflow:ios": [
+            "-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)",
+        ],
+        "//tensorflow:macos": [
+            "-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)",
+        ],
+        "//tensorflow:windows": [
+            "-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)",
+        ],
+        "//conditions:default": [
+            "-z defs",
+            "-Wl,--version-script,$(location //tensorflow/lite/c:version_script.lds)",
+        ],
+    }),
+    per_os_targets = True,
+    deps = [
+        ":external_delegate",
+        "//tensorflow/lite/c:exported_symbols.lds",
+        "//tensorflow/lite/c:version_script.lds",
+    ],
+)
+
 exports_files([
     "external_delegate.h",
-])
+])
\ No newline at end of file
diff --git a/tensorflow/lite/delegates/external/external_delegate.cc b/tensorflow/lite/delegates/external/external_delegate.cc
index 7fe5c5329dd..8fef8920cbd 100644
--- a/tensorflow/lite/delegates/external/external_delegate.cc
+++ b/tensorflow/lite/delegates/external/external_delegate.cc
@@ -154,14 +154,12 @@ ExternalDelegateWrapper::ExternalDelegateWrapper(
     external_delegate_ = external_lib_.create(ckeys.data(), cvalues.data(),
                                               ckeys.size(), nullptr);
     if (external_delegate_) {
-      wrapper_delegate_ = {
-          .data_ = reinterpret_cast<void*>(this),
-          .Prepare = DelegatePrepare,
-          .CopyFromBufferHandle = nullptr,
-          .CopyToBufferHandle = nullptr,
-          .FreeBufferHandle = nullptr,
-          .flags = external_delegate_->flags,
-      };
+      wrapper_delegate_.data_ = reinterpret_cast<void*>(this);
+      wrapper_delegate_.Prepare = DelegatePrepare;
+      wrapper_delegate_.CopyFromBufferHandle = nullptr;
+      wrapper_delegate_.CopyToBufferHandle = nullptr;
+      wrapper_delegate_.FreeBufferHandle = nullptr;
+      wrapper_delegate_.flags = external_delegate_->flags;
       if (external_delegate_->CopyFromBufferHandle) {
         wrapper_delegate_.CopyFromBufferHandle = DelegateCopyFromBufferHandle;
       }
diff --git a/tensorflow/lite/delegates/external/external_delegate.h b/tensorflow/lite/delegates/external/external_delegate.h
index 9121bd661b4..e15533d2664 100644
--- a/tensorflow/lite/delegates/external/external_delegate.h
+++ b/tensorflow/lite/delegates/external/external_delegate.h
@@ -17,6 +17,7 @@ limitations under the License.
 #define TENSORFLOW_LITE_DELEGATES_EXTERNAL_EXTERNAL_DELEGATE_H_
 
 #include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/c/c_api_types.h"  // IWYU pragma: export
 
 #ifdef __cplusplus
 extern "C" {
@@ -35,20 +36,20 @@ typedef struct TfLiteExternalDelegateOptions {
 } TfLiteExternalDelegateOptions;
 
 // Insert key/value to the options.
-TfLiteStatus TfLiteExternalDelegateOptionsInsert(
+TFL_CAPI_EXPORT TfLiteStatus TfLiteExternalDelegateOptionsInsert(
     TfLiteExternalDelegateOptions* options, const char* key, const char* value);
 
 // Populates TfLiteExternalDelegateOptions with the given shared library path.
-TfLiteExternalDelegateOptions TfLiteExternalDelegateOptionsDefault(
+TFL_CAPI_EXPORT TfLiteExternalDelegateOptions TfLiteExternalDelegateOptionsDefault(
     const char* lib_path);
 
 // Creates a new delegate instance that need to be destroyed with
 // `TfLiteExternalDelegateDelete` when delegate is no longer used by TFLite.
-TfLiteDelegate* TfLiteExternalDelegateCreate(
+TFL_CAPI_EXPORT TfLiteDelegate* TfLiteExternalDelegateCreate(
     const TfLiteExternalDelegateOptions* options);
 
 // Destroys a delegate created with `TfLiteExternalDelegateCreate` call.
-void TfLiteExternalDelegateDelete(TfLiteDelegate* delegate);
+TFL_CAPI_EXPORT void TfLiteExternalDelegateDelete(TfLiteDelegate* delegate);
 
 #ifdef __cplusplus
 }
diff --git a/tensorflow/lite/delegates/webnn/BUILD b/tensorflow/lite/delegates/webnn/BUILD
new file mode 100644
index 00000000000..b5d1ebc282b
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/BUILD
@@ -0,0 +1,697 @@
+load("//tensorflow/lite:special_rules.bzl", "tflite_portable_test_suite_combined")
+load("//tensorflow:tensorflow.bzl", "get_compatible_with_portable")
+load("//tensorflow/lite:build_def.bzl", "tflite_cc_shared_object")
+
+package(
+    default_visibility = ["//visibility:public"],
+    licenses = ["notice"],
+)
+
+EMSCRIPTEN_LINKOPTS = [
+    "-s ASSERTIONS=2",
+    "-s ERROR_ON_UNDEFINED_SYMBOLS=1",
+    "-s DEMANGLE_SUPPORT=1",
+    "-s EXIT_RUNTIME=1",
+    "-s ALLOW_MEMORY_GROWTH=1",
+    "-s TOTAL_MEMORY=134217728",
+]
+
+exports_files([
+    "webnn_delegate.h",
+])
+
+cc_library(
+    name = "webnn_delegate",
+    srcs = ["webnn_delegate.cc"],
+    hdrs = ["webnn_delegate.h"],
+    linkstatic = True,
+    deps = [
+        "//tensorflow/lite:kernel_api",
+        "//tensorflow/lite:minimal_logging",
+        "//tensorflow/lite:util",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/schema:schema_fbs",
+        "//tensorflow/lite/kernels/internal/utils:sparsity_format_converter",
+        "@FP16",
+    ],
+)
+
+cc_library(
+    name = "webnn_delegate_adaptor",
+    srcs = ["webnn_delegate_adaptor.cc"],
+    deps = [
+        ":webnn_delegate",
+        "//tensorflow/lite:shared_library",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/tools:command_line_flags",
+        "//tensorflow/lite/tools:logging",
+    ],
+)
+
+tflite_cc_shared_object(
+    name = "webnn_external_delegate_obj",
+    linkopts = select({
+        "//tensorflow:windows": [
+            "-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)",
+        ],
+        "//conditions:default": [
+            "-Wl,-z,defs",
+            "-Wl,--version-script,$(location //tensorflow/lite:tflite_version_script.lds)",
+        ],
+    }),
+    per_os_targets = True,
+    srcs = select({
+        "//tensorflow:windows": [
+            "webnn_delegate_adaptor.cc",
+        ],
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":webnn_delegate_adaptor",
+        "//tensorflow/lite/c:exported_symbols.lds",
+        "//tensorflow/lite:tflite_version_script.lds",
+    ],
+)
+
+cc_library(
+    name = "webnn_delegate_hdrs_only",
+    hdrs = ["webnn_delegate.h"],
+    compatible_with = get_compatible_with_portable(),
+    visibility = ["//tensorflow/lite:__subpackages__"],
+    deps = [
+        "//tensorflow/lite/c:common",
+    ],
+)
+
+cc_library(
+    name = "webnn_delegate_test_mode",
+    srcs = ["webnn_delegate.cc"],
+    hdrs = ["webnn_delegate.h"],
+    copts = ["-DWEBNN_DELEGATE_TEST_MODE=1"],
+    linkstatic = True,
+    deps = [
+        "//tensorflow/lite:kernel_api",
+        "//tensorflow/lite:minimal_logging",
+        "//tensorflow/lite:util",
+        "//tensorflow/lite:simple_memory_arena_debug_dump",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/schema:schema_fbs",
+        "//tensorflow/lite/kernels/internal/utils:sparsity_format_converter",
+        "//tensorflow/lite/kernels:padding",
+        "//tensorflow/lite/kernels/internal:compatibility",
+        "//tensorflow/lite/kernels/internal:tensor",
+        "//tensorflow/lite/tools/optimize:reduced_precision_support",
+        "@FP16",
+    ],
+)
+
+################################ Tester classes ################################
+
+cc_library(
+    name = "binary_elementwise_tester",
+    testonly = 1,
+    srcs = ["binary_elementwise_tester.cc"],
+    hdrs = ["binary_elementwise_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@FP16",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+cc_library(
+    name = "concatenation_tester",
+    testonly = 1,
+    srcs = ["concatenation_tester.cc"],
+    hdrs = ["concatenation_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+cc_library(
+    name = "conv_2d_tester",
+    testonly = 1,
+    srcs = ["conv_2d_tester.cc"],
+    hdrs = ["conv_2d_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@FP16",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+cc_library(
+    name = "depthwise_conv_2d_tester",
+    testonly = 1,
+    srcs = ["depthwise_conv_2d_tester.cc"],
+    hdrs = ["depthwise_conv_2d_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@FP16",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+cc_library(
+    name = "fully_connected_tester",
+    testonly = 1,
+    srcs = ["fully_connected_tester.cc"],
+    hdrs = ["fully_connected_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@FP16",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+cc_library(
+    name = "pad_tester",
+    testonly = 1,
+    srcs = ["pad_tester.cc"],
+    hdrs = ["pad_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+cc_library(
+    name = "pool_2d_tester",
+    testonly = 1,
+    srcs = ["pool_2d_tester.cc"],
+    hdrs = ["pool_2d_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+cc_library(
+    name = "reduce_tester",
+    testonly = 1,
+    srcs = ["reduce_tester.cc"],
+    hdrs = ["reduce_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+cc_library(
+    name = "reshape_tester",
+    testonly = 1,
+    srcs = ["reshape_tester.cc"],
+    hdrs = ["reshape_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+cc_library(
+    name = "resize_bilinear_tester",
+    testonly = 1,
+    srcs = ["resize_bilinear_tester.cc"],
+    hdrs = ["resize_bilinear_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+cc_library(
+    name = "split_tester",
+    testonly = 1,
+    srcs = ["split_tester.cc"],
+    hdrs = ["split_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+cc_library(
+    name = "softmax_tester",
+    testonly = 1,
+    srcs = ["softmax_tester.cc"],
+    hdrs = ["softmax_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+cc_library(
+    name = "transpose_conv_tester",
+    testonly = 1,
+    srcs = ["transpose_conv_tester.cc"],
+    hdrs = ["transpose_conv_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@FP16",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+cc_library(
+    name = "unary_elementwise_tester",
+    testonly = 1,
+    srcs = ["unary_elementwise_tester.cc"],
+    hdrs = ["unary_elementwise_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+cc_library(
+    name = "unpack_tester",
+    testonly = 1,
+    srcs = ["unpack_tester.cc"],
+    hdrs = ["unpack_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+############################## Integration tests ###############################
+
+cc_library(
+    name = "test_main",
+    testonly = 1,
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        "@com_google_googletest//:gtest_main",
+    ],
+)
+
+cc_test(
+    name = "add_test",
+    srcs = ["add_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":binary_elementwise_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "average_pool_2d_test",
+    srcs = ["average_pool_2d_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":pool_2d_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "concatenation_test",
+    srcs = ["concatenation_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":concatenation_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "conv_2d_test",
+    srcs = ["conv_2d_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":conv_2d_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "delegate_test",
+    srcs = ["delegate_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "depthwise_conv_2d_test",
+    srcs = ["depthwise_conv_2d_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":depthwise_conv_2d_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "fully_connected_test",
+    srcs = ["fully_connected_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":fully_connected_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "hard_swish_test",
+    srcs = ["hard_swish_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":test_main",
+        ":unary_elementwise_tester",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "logistic_test",
+    srcs = ["logistic_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":test_main",
+        ":unary_elementwise_tester",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "max_pool_2d_test",
+    srcs = ["max_pool_2d_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":pool_2d_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "mean_test",
+    srcs = ["mean_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":reduce_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "mul_test",
+    srcs = ["mul_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":binary_elementwise_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "pad_test",
+    srcs = ["pad_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":pad_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "relu_test",
+    srcs = ["relu_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":test_main",
+        ":unary_elementwise_tester",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "reshape_test",
+    srcs = ["reshape_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":reshape_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "resize_bilinear_test",
+    srcs = ["resize_bilinear_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":resize_bilinear_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "split_test",
+    srcs = ["split_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":split_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "softmax_test",
+    srcs = ["softmax_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":softmax_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "tanh_test",
+    srcs = ["tanh_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":test_main",
+        ":unary_elementwise_tester",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+# Disable this test temporarily as it contains int32 input tensor, which is not supported in WebNN at present.
+# cc_test(
+#     name = "transpose_conv_test",
+#     srcs = ["transpose_conv_test.cc"],
+#     linkopts = select({
+#         "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+#         "//conditions:default": [],
+#     }),
+#     deps = [
+#         ":test_main",
+#         ":transpose_conv_tester",
+#         "//tensorflow/lite/delegates/webnn:webnn_delegate_test_mode",
+#         "@com_google_googletest//:gtest",
+#     ],
+# )
+
+cc_test(
+    name = "unpack_test",
+    srcs = ["unpack_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":unpack_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
diff --git a/tensorflow/lite/delegates/webnn/add_test.cc b/tensorflow/lite/delegates/webnn/add_test.cc
new file mode 100644
index 00000000000..22a9fbe140e
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/add_test.cc
@@ -0,0 +1,913 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <gtest/gtest.h>
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include "tensorflow/lite/delegates/webnn/binary_elementwise_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Add, 4DBy4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy4DBroadcastChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, 1, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, 1, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy4DBroadcastWidth) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, width, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, width, 1})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy4DBroadcastHeight) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, 1, 1})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy4DBroadcastBatch) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, 1, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, 1, 1, 1})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy4DBroadcastHeightWidthChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 2DBy2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({batch, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 2DBy1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 2DBy0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic4DBroadcastChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, 1, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, 1, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic4DBroadcastWidth) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, width, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, width, 1})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic4DBroadcastHeight) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, 1, 1})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic4DBroadcastBatch) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, 1, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, 1, 1, 1})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic4DBroadcastHeightWidthChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({height, width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 2DByStatic2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({batch, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({batch, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 2DByStatic1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 2DByStatic0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, FP16Weights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .FP16Weights()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input2Static(true)
+      .FP16Weights()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, SparseWeights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .SparseWeights()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input2Static(true)
+      .SparseWeights()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, ReluActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .ReluActivation()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, Relu6Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Relu6Activation()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, ReluMinus1To1Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .ReluMinus1To1Activation()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, DISABLED_TanhActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .TanhActivation()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, DISABLED_SignBitActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .SignBitActivation()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/average_pool_2d_test.cc b/tensorflow/lite/delegates/webnn/average_pool_2d_test.cc
new file mode 100644
index 00000000000..7c251c57c6f
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/average_pool_2d_test.cc
@@ -0,0 +1,415 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/pool_2d_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(AveragePool2D, UnitPoolSamePadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(1)
+      .PoolingWidth(1)
+      .StrideHeight(1)
+      .StrideWidth(1)
+      .SamePadding()
+      .Test(BuiltinOperator_AVERAGE_POOL_2D, webnn_delegate.get());
+}
+
+TEST(AveragePool2D, UnitPoolValidPadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(1)
+      .PoolingWidth(1)
+      .StrideHeight(1)
+      .StrideWidth(1)
+      .ValidPadding()
+      .Test(BuiltinOperator_AVERAGE_POOL_2D, webnn_delegate.get());
+}
+
+TEST(AveragePool2D, EqualPoolAndStrideWithSamePadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 7), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  const int32_t pool_height = pool_rng();
+  const int32_t pool_width = pool_rng();
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_height)
+      .PoolingWidth(pool_width)
+      .StrideHeight(pool_height)
+      .StrideWidth(pool_width)
+      .SamePadding()
+      .Test(BuiltinOperator_AVERAGE_POOL_2D, webnn_delegate.get());
+}
+
+TEST(AveragePool2D, EqualPoolAndStrideWithValidPadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 7), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  const int32_t pool_height = pool_rng();
+  const int32_t pool_width = pool_rng();
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_height)
+      .PoolingWidth(pool_width)
+      .StrideHeight(pool_height)
+      .StrideWidth(pool_width)
+      .ValidPadding()
+      .Test(BuiltinOperator_AVERAGE_POOL_2D, webnn_delegate.get());
+}
+
+TEST(AveragePool2D, LargePoolSmallStrideWithSamePadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(4, 7), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_rng())
+      .PoolingWidth(pool_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SamePadding()
+      .Test(BuiltinOperator_AVERAGE_POOL_2D, webnn_delegate.get());
+}
+
+TEST(AveragePool2D, LargePoolSmallStrideWithValidPadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(4, 7), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_rng())
+      .PoolingWidth(pool_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .ValidPadding()
+      .Test(BuiltinOperator_AVERAGE_POOL_2D, webnn_delegate.get());
+}
+
+TEST(AveragePool2D, GlobalPooling) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  const int32_t height = input_rng();
+  const int32_t width = input_rng();
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(height)
+      .InputWidth(width)
+      .Channels(channel_rng())
+      .PoolingHeight(height)
+      .PoolingWidth(width)
+      .ValidPadding()
+      .Test(BuiltinOperator_AVERAGE_POOL_2D, webnn_delegate.get());
+}
+
+TEST(AveragePool2D, ReluActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_rng())
+      .PoolingWidth(pool_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .ReluActivation()
+      .Test(BuiltinOperator_AVERAGE_POOL_2D, webnn_delegate.get());
+}
+
+TEST(AveragePool2D, Relu6Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_rng())
+      .PoolingWidth(pool_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .Relu6Activation()
+      .Test(BuiltinOperator_AVERAGE_POOL_2D, webnn_delegate.get());
+}
+
+TEST(AveragePool2D, ReluMinus1To1Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_rng())
+      .PoolingWidth(pool_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .ReluMinus1To1Activation()
+      .Test(BuiltinOperator_AVERAGE_POOL_2D, webnn_delegate.get());
+}
+
+TEST(AveragePool2D, DISABLED_TanhActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_rng())
+      .PoolingWidth(pool_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .TanhActivation()
+      .Test(BuiltinOperator_AVERAGE_POOL_2D, webnn_delegate.get());
+}
+
+TEST(AveragePool2D, DISABLED_SignBitActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_rng())
+      .PoolingWidth(pool_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SignBitActivation()
+      .Test(BuiltinOperator_AVERAGE_POOL_2D, webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/binary_elementwise_tester.cc b/tensorflow/lite/delegates/webnn/binary_elementwise_tester.cc
new file mode 100644
index 00000000000..199e213c3a4
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/binary_elementwise_tester.cc
@@ -0,0 +1,415 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#include "tensorflow/lite/delegates/webnn/binary_elementwise_tester.h"
+
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <numeric>
+#include <random>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include <fp16.h>
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+std::vector<int32_t> BinaryElementwiseTester::OutputShape() const {
+  std::vector<int32_t> output_shape;
+  if (!input1_shape_.empty()) {
+    output_shape.insert(
+        output_shape.end(), input1_shape_.cbegin(),
+        input1_shape_.cbegin() +
+            std::max(input1_shape_.size(), input2_shape_.size()) -
+            input2_shape_.size());
+  }
+  if (!input2_shape_.empty()) {
+    output_shape.insert(
+        output_shape.end(), input2_shape_.cbegin(),
+        input2_shape_.cbegin() +
+            std::max(input2_shape_.size(), input1_shape_.size()) -
+            input1_shape_.size());
+  }
+  for (size_t i = std::min(input1_shape_.size(), input2_shape_.size()); i >= 1;
+       i--) {
+    output_shape.push_back(
+        std::max(*(input1_shape_.cend() - i), *(input2_shape_.cend() - i)));
+  }
+  return output_shape;
+}
+
+void BinaryElementwiseTester::Test(tflite::BuiltinOperator binary_op,
+                                   TfLiteDelegate* delegate) const {
+  if (Input1Static()) {
+    ASSERT_FALSE(Input2Static());
+  }
+  if (FP16Weights()) {
+    ASSERT_TRUE(Input1Static() || Input2Static());
+  }
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  std::uniform_real_distribution<float> input1_distribution(-25.0f, 25.0f);
+  std::uniform_real_distribution<float> input2_distribution(-25.0f, 25.0f);
+  switch (binary_op) {
+    case BuiltinOperator_DIV:
+      input1_distribution = std::uniform_real_distribution<float>(-5.0f, 5.0f);
+      input2_distribution = std::uniform_real_distribution<float>(0.1f, 1.0f);
+      break;
+    case BuiltinOperator_MUL:
+      input1_distribution = std::uniform_real_distribution<float>(-5.0f, 5.0f);
+      input2_distribution = std::uniform_real_distribution<float>(-5.0f, 5.0f);
+      break;
+    default:
+      break;
+  }
+  auto input1_rng = std::bind(input1_distribution, std::ref(rng));
+  auto input2_rng = std::bind(input2_distribution, std::ref(rng));
+
+  std::vector<char> buffer = CreateTfLiteModel(binary_op);
+  const Model* model = GetModel(buffer.data());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+
+  if (Input1Static() || Input2Static()) {
+    ASSERT_EQ(delegate_interpreter->inputs().size(), 1);
+    ASSERT_EQ(default_interpreter->inputs().size(), 1);
+  } else {
+    ASSERT_EQ(delegate_interpreter->inputs().size(), 2);
+    ASSERT_EQ(default_interpreter->inputs().size(), 2);
+  }
+
+  ASSERT_EQ(delegate_interpreter->outputs().size(), 1);
+  ASSERT_EQ(default_interpreter->outputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  if (!Input1Static()) {
+    float* default_input1_data = default_interpreter->typed_tensor<float>(
+        default_interpreter->inputs()[0]);
+    std::generate(default_input1_data,
+                  default_input1_data + ComputeSize(Input1Shape()),
+                  std::ref(input1_rng));
+
+    float* webnn_input1_data = delegate_interpreter->typed_tensor<float>(
+        delegate_interpreter->inputs()[0]);
+    std::copy(default_input1_data,
+              default_input1_data + ComputeSize(Input1Shape()),
+              webnn_input1_data);
+  }
+
+  if (!Input2Static()) {
+    float* default_input2_data = default_interpreter->typed_tensor<float>(
+        default_interpreter->inputs()[Input1Static() ? 0 : 1]);
+    std::generate(default_input2_data,
+                  default_input2_data + ComputeSize(Input2Shape()),
+                  std::ref(input2_rng));
+
+    float* webnn_input2_data = delegate_interpreter->typed_tensor<float>(
+        delegate_interpreter->inputs()[Input1Static() ? 0 : 1]);
+    std::copy(default_input2_data,
+              default_input2_data + ComputeSize(Input2Shape()),
+              webnn_input2_data);
+  }
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float* default_output_data = default_interpreter->typed_tensor<float>(
+      default_interpreter->outputs()[0]);
+  float* webnn_output_data = delegate_interpreter->typed_tensor<float>(
+      delegate_interpreter->outputs()[0]);
+
+  for (size_t i = 0; i < ComputeSize(OutputShape()); i++) {
+    ASSERT_NEAR(default_output_data[i], webnn_output_data[i],
+                std::numeric_limits<float>::epsilon() *
+                    std::max(std::abs(default_output_data[i]) * 2.0f, 1.0f));
+  }
+}
+
+std::vector<char> BinaryElementwiseTester::CreateTfLiteModel(
+    tflite::BuiltinOperator binary_op) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  std::uniform_real_distribution<float> input1_distribution(-25.0f, 25.0f);
+  std::uniform_real_distribution<float> input2_distribution(-25.0f, 25.0f);
+  switch (binary_op) {
+    case BuiltinOperator_DIV:
+      input1_distribution = std::uniform_real_distribution<float>(-5.0f, 5.0f);
+      input2_distribution = std::uniform_real_distribution<float>(0.1f, 1.0f);
+      break;
+    case BuiltinOperator_MUL:
+      input1_distribution = std::uniform_real_distribution<float>(-5.0f, 5.0f);
+      input2_distribution = std::uniform_real_distribution<float>(-5.0f, 5.0f);
+      break;
+    default:
+      break;
+  }
+  auto input1_rng = std::bind(input1_distribution, std::ref(rng));
+  auto input2_rng = std::bind(input2_distribution, std::ref(rng));
+
+  flatbuffers::FlatBufferBuilder builder;
+  std::vector<flatbuffers::Offset<OperatorCode>> operator_codes{
+      {CreateOperatorCode(builder, binary_op)}};
+  if (FP16Weights()) {
+    operator_codes.emplace_back(
+        CreateOperatorCode(builder, BuiltinOperator_DEQUANTIZE));
+  } else if (SparseWeights()) {
+    operator_codes.emplace_back(
+        CreateOperatorCode(builder, BuiltinOperator_DENSIFY));
+  }
+
+  std::vector<flatbuffers::Offset<Buffer>> buffers{{
+      CreateBuffer(builder, builder.CreateVector({})),
+  }};
+
+  int32_t input1_buffer = 0;
+  if (Input1Static()) {
+    if (FP16Weights()) {
+      std::vector<uint16_t> input1_data(ComputeSize(Input1Shape()));
+      std::generate(input1_data.begin(), input1_data.end(),
+                    std::bind(fp16_ieee_from_fp32_value, input1_rng));
+
+      buffers.push_back(CreateBuffer(
+          builder, builder.CreateVector(
+                       reinterpret_cast<const uint8_t*>(input1_data.data()),
+                       sizeof(uint16_t) * input1_data.size())));
+    } else {
+      std::vector<float> input1_data(ComputeSize(Input1Shape()));
+      std::generate(input1_data.begin(), input1_data.end(), input1_rng);
+
+      if (!SparseWeights()) {
+        input1_buffer = buffers.size();
+      }
+      buffers.push_back(CreateBuffer(
+          builder, builder.CreateVector(
+                       reinterpret_cast<const uint8_t*>(input1_data.data()),
+                       sizeof(float) * input1_data.size())));
+    }
+  }
+
+  int32_t input2_buffer = 0;
+  if (Input2Static()) {
+    if (FP16Weights()) {
+      std::vector<uint16_t> input2_data(ComputeSize(Input2Shape()));
+      std::generate(input2_data.begin(), input2_data.end(),
+                    std::bind(fp16_ieee_from_fp32_value, input1_rng));
+
+      buffers.push_back(CreateBuffer(
+          builder, builder.CreateVector(
+                       reinterpret_cast<const uint8_t*>(input2_data.data()),
+                       sizeof(uint16_t) * input2_data.size())));
+    } else {
+      std::vector<float> input2_data(ComputeSize(Input2Shape()));
+      std::generate(input2_data.begin(), input2_data.end(), input2_rng);
+
+      if (!SparseWeights()) {
+        input2_buffer = buffers.size();
+      }
+      buffers.push_back(CreateBuffer(
+          builder, builder.CreateVector(
+                       reinterpret_cast<const uint8_t*>(input2_data.data()),
+                       sizeof(float) * input2_data.size())));
+    }
+  }
+
+  const std::vector<int32_t> output_shape = OutputShape();
+  std::vector<flatbuffers::Offset<Tensor>> tensors;
+  std::vector<flatbuffers::Offset<Operator>> operators;
+  if (FP16Weights() && Input1Static()) {
+    tensors.emplace_back(
+        CreateTensor(builder,
+                     builder.CreateVector<int32_t>(Input1Shape().data(),
+                                                   Input1Shape().size()),
+                     TensorType_FLOAT16, 1));
+  } else if (SparseWeights() && Input1Static()) {
+    int dims_count = Input1Shape().size();
+    std::vector<flatbuffers::Offset<DimensionMetadata>> dim_metadata(
+        dims_count);
+    std::vector<int> traversal_order(dims_count);
+    for (int i = 0; i < dims_count; i++) {
+      traversal_order[i] = i;
+      dim_metadata[i] = CreateDimensionMetadata(builder, DimensionType_DENSE,
+                                                Input1Shape()[i]);
+    }
+    flatbuffers::Offset<SparsityParameters> sparsity_param =
+        CreateSparsityParameters(builder, builder.CreateVector(traversal_order),
+                                 0, builder.CreateVector(dim_metadata));
+    tensors.emplace_back(CreateTensor(
+        builder,
+        builder.CreateVector<int32_t>(Input1Shape().data(),
+                                      Input1Shape().size()),
+        TensorType_FLOAT32, /*buffer=*/1, /*name=*/0, /*quantization=*/0,
+        /*is_variable=*/false, /*sparsity=*/sparsity_param));
+  }
+  if (FP16Weights() && Input2Static()) {
+    tensors.emplace_back(
+        CreateTensor(builder,
+                     builder.CreateVector<int32_t>(Input2Shape().data(),
+                                                   Input2Shape().size()),
+                     TensorType_FLOAT16, 1));
+  } else if (SparseWeights() && Input2Static()) {
+    int dims_count = Input2Shape().size();
+    std::vector<flatbuffers::Offset<DimensionMetadata>> dim_metadata(
+        dims_count);
+    std::vector<int> traversal_order(dims_count);
+    for (int i = 0; i < dims_count; i++) {
+      traversal_order[i] = i;
+      dim_metadata[i] = CreateDimensionMetadata(builder, DimensionType_DENSE,
+                                                Input2Shape()[i]);
+    }
+    flatbuffers::Offset<SparsityParameters> sparsity_param =
+        CreateSparsityParameters(builder, builder.CreateVector(traversal_order),
+                                 0, builder.CreateVector(dim_metadata));
+    tensors.emplace_back(CreateTensor(
+        builder,
+        builder.CreateVector<int32_t>(Input2Shape().data(),
+                                      Input2Shape().size()),
+        TensorType_FLOAT32, /*buffer=*/1, /*name=*/0, /*quantization=*/0,
+        /*is_variable=*/false, /*sparsity=*/sparsity_param));
+  }
+  if (FP16Weights()) {
+    const std::array<int32_t, 1> dequantize_inputs{{0}};
+    const std::array<int32_t, 1> dequantize_outputs{{Input1Static() ? 1 : 2}};
+    operators.emplace_back(CreateOperator(
+        builder, /*opcode_index=*/1,
+        builder.CreateVector<int32_t>(dequantize_inputs.data(),
+                                      dequantize_inputs.size()),
+        builder.CreateVector<int32_t>(dequantize_outputs.data(),
+                                      dequantize_outputs.size())));
+  } else if (SparseWeights()) {
+    const std::array<int32_t, 1> densify_inputs{{0}};
+    const std::array<int32_t, 1> densify_outputs{{Input1Static() ? 1 : 2}};
+    operators.emplace_back(
+        CreateOperator(builder, /*opcode_index=*/1,
+                       builder.CreateVector<int32_t>(densify_inputs.data(),
+                                                     densify_inputs.size()),
+                       builder.CreateVector<int32_t>(densify_outputs.data(),
+                                                     densify_outputs.size())));
+  }
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(Input1Shape().data(), Input1Shape().size()),
+      TensorType_FLOAT32, input1_buffer));
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(Input2Shape().data(), Input2Shape().size()),
+      TensorType_FLOAT32, input2_buffer));
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(output_shape.data(), output_shape.size()),
+      TensorType_FLOAT32));
+
+  tflite::BuiltinOptions builtin_options_type = tflite::BuiltinOptions_NONE;
+  flatbuffers::Offset<void> builtin_options = 0;
+  switch (binary_op) {
+    case BuiltinOperator_ADD:
+      builtin_options_type = BuiltinOptions_AddOptions;
+      builtin_options = CreateAddOptions(builder, Activation()).Union();
+      break;
+    case BuiltinOperator_DIV:
+      builtin_options_type = BuiltinOptions_DivOptions;
+      builtin_options = CreateDivOptions(builder, Activation()).Union();
+      break;
+    case BuiltinOperator_MUL:
+      builtin_options_type = BuiltinOptions_MulOptions;
+      builtin_options = CreateMulOptions(builder, Activation()).Union();
+      break;
+    case BuiltinOperator_SUB:
+      builtin_options_type = BuiltinOptions_SubOptions;
+      builtin_options = CreateSubOptions(builder, Activation()).Union();
+      break;
+    default:
+      EXPECT_EQ(Activation(), ActivationFunctionType_NONE);
+  }
+
+  const std::array<int32_t, 2> op_inputs{
+      {static_cast<int>(tensors.size()) - 3,
+       static_cast<int>(tensors.size()) - 2}};
+  const std::array<int32_t, 1> op_outputs{
+      {static_cast<int>(tensors.size()) - 1}};
+  operators.emplace_back(CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()),
+      builtin_options_type, builtin_options));
+
+  std::vector<int32_t> subgraph_inputs;
+  if (!Input1Static()) {
+    subgraph_inputs.push_back(tensors.size() - 3);
+  }
+  if (!Input2Static()) {
+    subgraph_inputs.push_back(tensors.size() - 2);
+  }
+  const std::array<int32_t, 1> subgraph_outputs{
+      {static_cast<int>(tensors.size()) - 1}};
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(operators.data(), operators.size()));
+
+  flatbuffers::Offset<flatbuffers::String> description =
+      builder.CreateString("Binary operator model");
+
+  flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION,
+      builder.CreateVector(operator_codes.data(), operator_codes.size()),
+      builder.CreateVector(&subgraph, 1), description,
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+int32_t BinaryElementwiseTester::ComputeSize(
+    const std::vector<int32_t>& shape) {
+  return std::accumulate(shape.cbegin(), shape.cend(), 1,
+                         std::multiplies<int32_t>());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/binary_elementwise_tester.h b/tensorflow/lite/delegates/webnn/binary_elementwise_tester.h
new file mode 100644
index 00000000000..07c8381b0c1
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/binary_elementwise_tester.h
@@ -0,0 +1,140 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_BINARY_ELEMENTWISE_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_BINARY_ELEMENTWISE_TESTER_H_
+
+#include <cstdint>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+
+namespace tflite {
+namespace webnn {
+
+class BinaryElementwiseTester {
+ public:
+  BinaryElementwiseTester() = default;
+  BinaryElementwiseTester(const BinaryElementwiseTester&) = delete;
+  BinaryElementwiseTester& operator=(const BinaryElementwiseTester&) = delete;
+
+  inline BinaryElementwiseTester& Input1Shape(
+      std::initializer_list<int32_t> shape) {
+    for (auto it = shape.begin(); it != shape.end(); ++it) {
+      EXPECT_GT(*it, 0);
+    }
+    input1_shape_ = std::vector<int32_t>(shape.begin(), shape.end());
+    return *this;
+  }
+
+  inline const std::vector<int32_t>& Input1Shape() const {
+    return input1_shape_;
+  }
+
+  inline BinaryElementwiseTester& Input2Shape(
+      std::initializer_list<int32_t> shape) {
+    for (auto it = shape.begin(); it != shape.end(); ++it) {
+      EXPECT_GT(*it, 0);
+    }
+    input2_shape_ = std::vector<int32_t>(shape.begin(), shape.end());
+    return *this;
+  }
+
+  inline const std::vector<int32_t>& Input2Shape() const {
+    return input2_shape_;
+  }
+
+  std::vector<int32_t> OutputShape() const;
+
+  inline BinaryElementwiseTester& Input1Static(bool is_static) {
+    input1_static_ = is_static;
+    return *this;
+  }
+
+  inline bool Input1Static() const { return input1_static_; }
+
+  inline BinaryElementwiseTester& Input2Static(bool is_static) {
+    input2_static_ = is_static;
+    return *this;
+  }
+
+  inline bool Input2Static() const { return input2_static_; }
+
+  inline BinaryElementwiseTester& FP16Weights() {
+    fp16_weights_ = true;
+    return *this;
+  }
+
+  inline bool FP16Weights() const { return fp16_weights_; }
+
+  inline BinaryElementwiseTester& SparseWeights() {
+    sparse_weights_ = true;
+    return *this;
+  }
+
+  inline bool SparseWeights() const { return sparse_weights_; }
+
+  inline BinaryElementwiseTester& ReluActivation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU;
+    return *this;
+  }
+
+  inline BinaryElementwiseTester& Relu6Activation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU6;
+    return *this;
+  }
+
+  inline BinaryElementwiseTester& ReluMinus1To1Activation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU_N1_TO_1;
+    return *this;
+  }
+
+  inline BinaryElementwiseTester& TanhActivation() {
+    activation_ = ::tflite::ActivationFunctionType_TANH;
+    return *this;
+  }
+
+  inline BinaryElementwiseTester& SignBitActivation() {
+    activation_ = ::tflite::ActivationFunctionType_SIGN_BIT;
+    return *this;
+  }
+
+  void Test(tflite::BuiltinOperator binary_op, TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel(tflite::BuiltinOperator binary_op) const;
+
+  inline ::tflite::ActivationFunctionType Activation() const {
+    return activation_;
+  }
+
+  static int32_t ComputeSize(const std::vector<int32_t>& shape);
+
+  std::vector<int32_t> input1_shape_;
+  std::vector<int32_t> input2_shape_;
+  bool input1_static_ = false;
+  bool input2_static_ = false;
+  bool fp16_weights_ = false;
+  bool sparse_weights_ = false;
+  ::tflite::ActivationFunctionType activation_ =
+      ::tflite::ActivationFunctionType_NONE;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_BINARY_ELEMENTWISE_TESTER_H_
diff --git a/tensorflow/lite/delegates/webnn/concatenation_test.cc b/tensorflow/lite/delegates/webnn/concatenation_test.cc
new file mode 100644
index 00000000000..61e99474477
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/concatenation_test.cc
@@ -0,0 +1,348 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <algorithm>
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/concatenation_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Concatenation, 1D_2_inputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  const std::vector<int32_t> shape1({shape_rng()});
+  const std::vector<int32_t> shape2({shape_rng()});
+
+  for (int i = -1; i < 1; i++) {
+    // clang-format off
+    ConcatenationTester()
+        .InputShapes({shape1, shape2})
+        .Axis(i)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Concatenation, 2D_2_inputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+
+  for (int i = -2; i < 2; i++) {
+    // All dimensions must be the same, except for axis.
+    const std::vector<int32_t> shape1({shape_rng(), shape_rng()});
+    auto shape2 = SameShapeDifferentAxis(shape1, i, shape_rng());
+
+    // clang-format off
+    ConcatenationTester()
+        .InputShapes({shape1, shape2})
+        .Axis(i)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Concatenation, 3D_2_inputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+
+  for (int i = -3; i < 3; i++) {
+    // All dimensions must be the same, except for axis.
+    const std::vector<int32_t> shape1({shape_rng(), shape_rng(), shape_rng()});
+    auto shape2 = SameShapeDifferentAxis(shape1, i, shape_rng());
+
+    // clang-format off
+    ConcatenationTester()
+        .InputShapes({shape1, shape2})
+        .Axis(i)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Concatenation, 4D_2_inputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+
+  for (int i = -4; i < 4; i++) {
+    // All dimensions must be the same, except for axis.
+    const std::vector<int32_t> shape1(
+        {shape_rng(), shape_rng(), shape_rng(), shape_rng()});
+    auto shape2 = SameShapeDifferentAxis(shape1, i, shape_rng());
+
+    // clang-format off
+    ConcatenationTester()
+        .InputShapes({shape1, shape2})
+        .Axis(i)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Concatenation, 1D_of_3) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  const std::vector<int32_t> shape1({shape_rng()});
+  const std::vector<int32_t> shape2({shape_rng()});
+  const std::vector<int32_t> shape3({shape_rng()});
+
+  for (int i = -1; i < 1; i++) {
+    // clang-format off
+    ConcatenationTester()
+        .InputShapes({shape1, shape2, shape3})
+        .Axis(i)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Concatenation, 2D_of_3) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+
+  for (int i = -2; i < 2; i++) {
+    // All dimensions must be the same, except for axis.
+    const std::vector<int32_t> shape1({shape_rng(), shape_rng()});
+    auto shape2 = SameShapeDifferentAxis(shape1, i, shape_rng());
+    auto shape3 = SameShapeDifferentAxis(shape1, i, shape_rng());
+
+    // clang-format off
+    ConcatenationTester()
+        .InputShapes({shape1, shape2, shape3})
+        .Axis(i)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Concatenation, 3D_of_3) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+
+  for (int i = -3; i < 3; i++) {
+    // All dimensions must be the same, except for axis.
+    const std::vector<int32_t> shape1({shape_rng(), shape_rng(), shape_rng()});
+    auto shape2 = SameShapeDifferentAxis(shape1, i, shape_rng());
+    auto shape3 = SameShapeDifferentAxis(shape1, i, shape_rng());
+
+    // clang-format off
+    ConcatenationTester()
+        .InputShapes({shape1, shape2, shape3})
+        .Axis(i)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Concatenation, 4D_of_3) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+
+  for (int i = -4; i < 4; i++) {
+    // All dimensions must be the same, except for axis.
+    const std::vector<int32_t> shape1(
+        {shape_rng(), shape_rng(), shape_rng(), shape_rng()});
+    auto shape2 = SameShapeDifferentAxis(shape1, i, shape_rng());
+    auto shape3 = SameShapeDifferentAxis(shape1, i, shape_rng());
+
+    // clang-format off
+    ConcatenationTester()
+        .InputShapes({shape1, shape2, shape3})
+        .Axis(i)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Concatenation, 1D_of_4) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  const std::vector<int32_t> shape1({shape_rng()});
+  const std::vector<int32_t> shape2({shape_rng()});
+  const std::vector<int32_t> shape3({shape_rng()});
+  const std::vector<int32_t> shape4({shape_rng()});
+
+  for (int i = -1; i < 1; i++) {
+    // clang-format off
+    ConcatenationTester()
+        .InputShapes({shape1, shape2, shape3, shape4})
+        .Axis(i)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Concatenation, 2D_of_4) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+
+  for (int i = -2; i < 2; i++) {
+    // All dimensions must be the same, except for axis.
+    const std::vector<int32_t> shape1({shape_rng(), shape_rng()});
+    auto shape2 = SameShapeDifferentAxis(shape1, i, shape_rng());
+    auto shape3 = SameShapeDifferentAxis(shape1, i, shape_rng());
+    auto shape4 = SameShapeDifferentAxis(shape1, i, shape_rng());
+
+    // clang-format off
+    ConcatenationTester()
+        .InputShapes({shape1, shape2, shape3, shape4})
+        .Axis(i)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Concatenation, 3D_of_4) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+
+  for (int i = -3; i < 3; i++) {
+    // All dimensions must be the same, except for axis.
+    const std::vector<int32_t> shape1({shape_rng(), shape_rng(), shape_rng()});
+    auto shape2 = SameShapeDifferentAxis(shape1, i, shape_rng());
+    auto shape3 = SameShapeDifferentAxis(shape1, i, shape_rng());
+    auto shape4 = SameShapeDifferentAxis(shape1, i, shape_rng());
+
+    // clang-format off
+    ConcatenationTester()
+        .InputShapes({shape1, shape2, shape3, shape4})
+        .Axis(i)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Concatenation, 4D_of_4) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+
+  for (int i = -4; i < 4; i++) {
+    // All dimensions must be the same, except for axis.
+    const std::vector<int32_t> shape1(
+        {shape_rng(), shape_rng(), shape_rng(), shape_rng()});
+    auto shape2 = SameShapeDifferentAxis(shape1, i, shape_rng());
+    auto shape3 = SameShapeDifferentAxis(shape1, i, shape_rng());
+    auto shape4 = SameShapeDifferentAxis(shape1, i, shape_rng());
+
+    // clang-format off
+    ConcatenationTester()
+        .InputShapes({shape1, shape2, shape3, shape4})
+        .Axis(i)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+}  // namespace webnn
+}  // namespace tflite
\ No newline at end of file
diff --git a/tensorflow/lite/delegates/webnn/concatenation_tester.cc b/tensorflow/lite/delegates/webnn/concatenation_tester.cc
new file mode 100644
index 00000000000..b9e7c129f4a
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/concatenation_tester.cc
@@ -0,0 +1,235 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/concatenation_tester.h"
+
+#include <algorithm>
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <numeric>
+#include <random>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+std::vector<int32_t> SameShapeDifferentAxis(std::vector<int32_t> shape,
+                                            int axis, int32_t size) {
+  std::vector<int32_t> new_shape{shape};
+  new_shape[axis < 0 ? axis + shape.size() : axis] = size;
+  return new_shape;
+}
+
+template <class T>
+void ConcatenationTester::Test(Interpreter *delegate_interpreter,
+                               Interpreter *default_interpreter) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  std::uniform_int_distribution<int32_t> input_distribution(
+      std::numeric_limits<T>::min(), std::numeric_limits<T>::max());
+  auto input_rng = std::bind(input_distribution, std::ref(rng));
+
+  for (size_t i = 0; i < NumInputs(); i++) {
+    T *default_input_data = default_interpreter->typed_input_tensor<T>(i);
+    std::generate(default_input_data,
+                  default_input_data + ComputeSize(InputShape(i)),
+                  std::ref(input_rng));
+
+    T *webnn_input_data = delegate_interpreter->typed_input_tensor<T>(i);
+    std::copy(default_input_data,
+              default_input_data + ComputeSize(InputShape(i)),
+              webnn_input_data);
+  }
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  T *default_output_data = default_interpreter->typed_output_tensor<T>(0);
+  T *webnn_output_data = delegate_interpreter->typed_output_tensor<T>(0);
+
+  for (size_t i = 0; i < ComputeSize(OutputShape()); i++) {
+    ASSERT_EQ(static_cast<int32_t>(default_output_data[i]),
+              static_cast<int32_t>(webnn_output_data[i]));
+  }
+}
+
+template <>
+void ConcatenationTester::Test<float>(Interpreter *delegate_interpreter,
+                                      Interpreter *default_interpreter) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  std::uniform_real_distribution<float> input_distribution(-25.0f, 25.0f);
+  auto input_rng = std::bind(input_distribution, std::ref(rng));
+
+  for (size_t i = 0; i < NumInputs(); i++) {
+    float *default_input_data =
+        default_interpreter->typed_input_tensor<float>(i);
+    std::generate(default_input_data,
+                  default_input_data + ComputeSize(InputShape(i)),
+                  std::ref(input_rng));
+
+    float *webnn_input_data =
+        delegate_interpreter->typed_input_tensor<float>(i);
+    std::copy(default_input_data,
+              default_input_data + ComputeSize(InputShape(i)),
+              webnn_input_data);
+  }
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float *default_output_data =
+      default_interpreter->typed_output_tensor<float>(0);
+  float *webnn_output_data =
+      delegate_interpreter->typed_output_tensor<float>(0);
+
+  for (size_t i = 0; i < ComputeSize(OutputShape()); i++) {
+    ASSERT_EQ(default_output_data[i], webnn_output_data[i]);
+  }
+}
+
+void ConcatenationTester::Test(TensorType tensor_type,
+                               TfLiteDelegate *delegate) const {
+  std::vector<char> buffer = CreateTfLiteModel(tensor_type);
+  const Model *model = GetModel(buffer.data());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+  ASSERT_EQ(delegate_interpreter->inputs().size(), NumInputs());
+  ASSERT_EQ(default_interpreter->inputs().size(), NumInputs());
+  ASSERT_EQ(delegate_interpreter->outputs().size(), 1);
+  ASSERT_EQ(default_interpreter->outputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  switch (tensor_type) {
+    case TensorType_FLOAT32:
+      Test<float>(delegate_interpreter.get(), default_interpreter.get());
+      break;
+    case TensorType_INT8:
+      Test<int8_t>(delegate_interpreter.get(), default_interpreter.get());
+      break;
+    case TensorType_UINT8:
+      Test<uint8_t>(delegate_interpreter.get(), default_interpreter.get());
+      break;
+    default:
+      GTEST_FAIL();
+  }
+}
+
+std::vector<char> ConcatenationTester::CreateTfLiteModel(
+    TensorType tensor_type) const {
+  flatbuffers::FlatBufferBuilder builder;
+  flatbuffers::Offset<OperatorCode> operator_code =
+      CreateOperatorCode(builder, BuiltinOperator_CONCATENATION, 0);
+
+  std::vector<flatbuffers::Offset<Buffer>> buffers{{
+      CreateBuffer(builder, builder.CreateVector({})),
+  }};
+
+  std::vector<flatbuffers::Offset<Tensor>> tensors;
+  for (size_t i = 0; i < NumInputs(); i++) {
+    tensors.push_back(
+        CreateTensor(builder,
+                     builder.CreateVector<int32_t>(InputShape(i).data(),
+                                                   InputShape(i).size()),
+                     tensor_type,
+                     /*buffer=*/0, /*name=*/0,
+                     CreateQuantizationParameters(
+                         builder, /*min=*/0, /*max=*/0,
+                         builder.CreateVector<float>({/*scale=*/1.0f}),
+                         builder.CreateVector<int64_t>({/*zero_point=*/0}))));
+  }
+
+  tensors.push_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(OutputShape().data(), OutputShape().size()),
+      tensor_type,
+      /*buffer=*/0, /*name=*/0,
+      CreateQuantizationParameters(
+          builder, /*min=*/0, /*max=*/0,
+          builder.CreateVector<float>({/*scale=*/1.0f}),
+          builder.CreateVector<int64_t>({/*zero_point=*/0}))));
+
+  std::vector<int32_t> op_inputs;
+  for (size_t i = 0; i < NumInputs(); i++) {
+    op_inputs.push_back(static_cast<int32_t>(i));
+  }
+
+  const std::array<int32_t, 1> op_outputs{static_cast<int32_t>(NumInputs())};
+  BuiltinOptions builtin_options_type = tflite::BuiltinOptions_NONE;
+  flatbuffers::Offset<void> builtin_options = 0;
+  builtin_options_type = tflite::BuiltinOptions_ConcatenationOptions;
+  builtin_options = CreateConcatenationOptions(builder, Axis()).Union();
+  const flatbuffers::Offset<Operator> op = CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()),
+      builtin_options_type, builtin_options);
+
+  const std::vector<int32_t> subgraph_inputs = op_inputs;
+  const std::array<int32_t, 1> subgraph_outputs = op_outputs;
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(&op, 1));
+
+  const flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION, builder.CreateVector(&operator_code, 1),
+      builder.CreateVector(&subgraph, 1),
+      builder.CreateString("Concatenation model"),
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+int32_t ConcatenationTester::ComputeSize(const std::vector<int32_t> &shape) {
+  return std::accumulate(shape.cbegin(), shape.cend(), 1,
+                         std::multiplies<int32_t>());
+}
+
+}  // namespace webnn
+}  // namespace tflite
\ No newline at end of file
diff --git a/tensorflow/lite/delegates/webnn/concatenation_tester.h b/tensorflow/lite/delegates/webnn/concatenation_tester.h
new file mode 100644
index 00000000000..030dd408e9d
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/concatenation_tester.h
@@ -0,0 +1,91 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_CONCATENATION_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_CONCATENATION_TESTER_H_
+
+#include <cstdint>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+
+namespace tflite {
+namespace webnn {
+
+// Creates a new shape with the same dimensions as `shape`, except for the axis
+// dimension, which will have the value `size`.
+std::vector<int32_t> SameShapeDifferentAxis(std::vector<int32_t> shape,
+                                            int axis, int32_t size);
+
+class ConcatenationTester {
+ public:
+  ConcatenationTester() = default;
+  ConcatenationTester(const ConcatenationTester&) = delete;
+  ConcatenationTester& operator=(const ConcatenationTester&) = delete;
+
+  inline ConcatenationTester& Axis(int axis) {
+    axis_ = axis;
+    return *this;
+  }
+
+  inline const int Axis() const { return axis_; }
+
+  inline ConcatenationTester& InputShapes(
+      const std::initializer_list<std::vector<int32_t>> shapes) {
+    for (auto shape : shapes) {
+      for (auto it = shape.begin(); it != shape.end(); ++it) {
+        EXPECT_GT(*it, 0);
+      }
+    }
+    input_shapes_ = shapes;
+    return *this;
+  }
+
+  inline std::vector<int32_t> InputShape(size_t i) const {
+    return input_shapes_[i];
+  }
+
+  inline size_t NumInputs() const { return input_shapes_.size(); }
+
+  std::vector<int32_t> OutputShape() const {
+    std::vector<int32_t> output_shape = InputShape(0);
+    int concat_axis = Axis() < 0 ? Axis() + output_shape.size() : Axis();
+    size_t axis_dim_size = 0;
+    for (size_t i = 0; i < NumInputs(); i++) {
+      axis_dim_size += InputShape(i)[concat_axis];
+    }
+    output_shape[concat_axis] = axis_dim_size;
+    return output_shape;
+  }
+
+  template <typename T>
+  void Test(Interpreter* delegate_interpreter,
+            Interpreter* default_interpreter) const;
+  void Test(TensorType tensor_type, TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel(TensorType tensor_type) const;
+
+  static int32_t ComputeSize(const std::vector<int32_t>& shape);
+
+  int axis_;
+  std::vector<int32_t> output_shape_;
+  std::vector<std::vector<int32_t>> input_shapes_;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_CONCATENATION_TESTER_H_
\ No newline at end of file
diff --git a/tensorflow/lite/delegates/webnn/conv_2d_test.cc b/tensorflow/lite/delegates/webnn/conv_2d_test.cc
new file mode 100644
index 00000000000..506cf2d6bd6
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/conv_2d_test.cc
@@ -0,0 +1,723 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/conv_2d_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Conv2D, 1x1) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(1)
+      .KernelWidth(1)
+      .ValidPadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, 3x3) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(3)
+      .KernelWidth(3)
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, 3x3Stride2) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(3)
+      .KernelWidth(3)
+      .StrideHeight(2)
+      .StrideWidth(2)
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, SmallKernelWithSamePadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 7), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, SmallKernelWithValidPadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 7), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .ValidPadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, StrideWithSamePadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, StrideWithValidPadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .ValidPadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, DilationWithSamePadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto dilation_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .DilationHeight(dilation_rng())
+      .DilationWidth(dilation_rng())
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, DilationWithValidPadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto dilation_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .DilationHeight(dilation_rng())
+      .DilationWidth(dilation_rng())
+      .ValidPadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, FP16Weights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .FP16Weights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, INT8Weights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .INT8Weights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, INT8ChannelWiseWeights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .INT8ChannelWiseWeights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, SparseWeights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SparseWeights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, SparseFP16Weights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SparseWeights()
+      .FP16Weights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, SparseINT8Weights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SparseWeights()
+      .INT8Weights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, SparseINT8ChannelWiseWeights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SparseWeights()
+      .INT8ChannelWiseWeights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, ReluActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .ReluActivation()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, Relu6Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .Relu6Activation()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, ReluMinus1To1Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .ReluMinus1To1Activation()
+      .Test(webnn_delegate.get());
+}
+
+// webnn_native does not support this case
+TEST(Conv2D, DISABLED_TanhActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .TanhActivation()
+      .Test(webnn_delegate.get());
+}
+
+TEST(Conv2D, DISABLED_SignBitActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 16), std::ref(rng));
+
+  Conv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SignBitActivation()
+      .Test(webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/conv_2d_tester.cc b/tensorflow/lite/delegates/webnn/conv_2d_tester.cc
new file mode 100644
index 00000000000..c76965d0264
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/conv_2d_tester.cc
@@ -0,0 +1,343 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/conv_2d_tester.h"
+
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <random>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include <fp16.h>
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+void Conv2DTester::Test(TfLiteDelegate* delegate) const {
+  std::vector<char> buffer = CreateTfLiteModel();
+  const Model* model = GetModel(buffer.data());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+
+  ASSERT_EQ(delegate_interpreter->inputs().size(), 1);
+  ASSERT_EQ(default_interpreter->inputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->outputs().size(), 1);
+  ASSERT_EQ(default_interpreter->outputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto input_rng =
+      std::bind(std::uniform_real_distribution<float>(), std::ref(rng));
+  float* default_input_data = default_interpreter->typed_tensor<float>(
+      default_interpreter->inputs()[0]);
+  std::generate(default_input_data,
+                default_input_data + BatchSize() * InputHeight() *
+                                         InputWidth() * InputChannels(),
+                input_rng);
+
+  float* delegate_input_data = delegate_interpreter->typed_tensor<float>(
+      delegate_interpreter->inputs()[0]);
+  std::copy(default_input_data,
+            default_input_data +
+                BatchSize() * InputHeight() * InputWidth() * InputChannels(),
+            delegate_input_data);
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float* default_output_data = default_interpreter->typed_tensor<float>(
+      default_interpreter->outputs()[0]);
+  float* delegate_output_data = delegate_interpreter->typed_tensor<float>(
+      delegate_interpreter->outputs()[0]);
+
+  for (int32_t i = 0; i < BatchSize(); i++) {
+    for (int32_t y = 0; y < OutputHeight(); y++) {
+      for (int32_t x = 0; x < OutputWidth(); x++) {
+        for (int32_t c = 0; c < OutputChannels(); c++) {
+          const int32_t index = ((i * OutputHeight() + y) * OutputWidth() + x) *
+                                    OutputChannels() +
+                                c;
+          ASSERT_NEAR(default_output_data[index], delegate_output_data[index],
+                      std::abs(default_output_data[index]) * 3.0e-6f)
+              << "batch " << i << " / " << BatchSize() << ", y position " << y
+              << " / " << OutputHeight() << ", x position " << x << " / "
+              << OutputWidth() << ", channel " << c << " / "
+              << OutputChannels();
+        }
+      }
+    }
+  }
+}
+
+std::vector<char> Conv2DTester::CreateTfLiteModel() const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto range_rng = std::bind(
+      std::uniform_real_distribution<float>(-25.0f, 25.0f), std::ref(rng));
+
+  flatbuffers::FlatBufferBuilder builder;
+  std::vector<flatbuffers::Offset<OperatorCode>> operator_codes{
+      {CreateOperatorCode(builder, BuiltinOperator_CONV_2D)}};
+  std::vector<flatbuffers::Offset<tflite::Operator>> operators;
+  std::vector<flatbuffers::Offset<tflite::Buffer>> buffers{
+      {CreateBuffer(builder, builder.CreateVector({}))}};
+
+  if (SparseWeights()) {
+    operator_codes.emplace_back(
+        CreateOperatorCode(builder, BuiltinOperator_DENSIFY));
+    const std::array<int32_t, 1> densify_filter_inputs{{0}};
+    const std::array<int32_t, 1> densify_filter_outputs{
+        {FP16Weights() ? 1 : 2}};
+    operators.emplace_back(CreateOperator(
+        builder, /*opcode_index=*/operator_codes.size() - 1,
+        builder.CreateVector<int32_t>(densify_filter_inputs.data(),
+                                      densify_filter_inputs.size()),
+        builder.CreateVector<int32_t>(densify_filter_outputs.data(),
+                                      densify_filter_outputs.size())));
+  }
+
+  if (FP16Weights()) {
+    operator_codes.emplace_back(
+        CreateOperatorCode(builder, BuiltinOperator_DEQUANTIZE));
+
+    std::vector<uint16_t> filter_data(OutputChannels() * KernelHeight() *
+                                      KernelWidth() * InputChannels());
+    std::vector<uint16_t> bias_data(OutputChannels());
+    for (int32_t oc = 0; oc < OutputChannels(); oc++) {
+      // Use the same range of all-positive or all-negative values to generate
+      // all weights within the same output channel, but different ranges for
+      // different output channels. This ensures that no catastrophic
+      // cancellation occur, but test covers both positive and negative inputs.
+      const float range = range_rng();
+      auto value_rng =
+          std::bind(fp16_ieee_from_fp32_value,
+                    std::bind(std::uniform_real_distribution<float>(
+                                  std::min(range, 0.0f), std::max(range, 0.0f)),
+                              std::ref(rng)));
+      bias_data[oc] = value_rng();
+      for (int32_t ic = 0; ic < InputChannels(); ic++) {
+        for (int32_t y = 0; y < KernelHeight(); y++) {
+          for (int32_t x = 0; x < KernelWidth(); x++) {
+            const int32_t index =
+                ((oc * KernelHeight() + y) * KernelWidth() + x) *
+                    InputChannels() +
+                ic;
+            filter_data[index] = value_rng();
+          }
+        }
+      }
+    }
+
+    buffers.emplace_back(CreateBuffer(
+        builder, builder.CreateVector(
+                     reinterpret_cast<const uint8_t*>(filter_data.data()),
+                     sizeof(uint16_t) * filter_data.size())));
+    buffers.emplace_back(CreateBuffer(
+        builder,
+        builder.CreateVector(reinterpret_cast<const uint8_t*>(bias_data.data()),
+                             sizeof(uint16_t) * bias_data.size())));
+
+    const std::array<int32_t, 1> dequantize_filter_inputs{
+        {SparseWeights() ? 1 : 0}};
+    const std::array<int32_t, 1> dequantize_filter_outputs{
+        {SparseWeights() ? 4 : 3}};
+    operators.emplace_back(CreateOperator(
+        builder, /*opcode_index=*/operator_codes.size() - 1,
+        builder.CreateVector<int32_t>(dequantize_filter_inputs.data(),
+                                      dequantize_filter_inputs.size()),
+        builder.CreateVector<int32_t>(dequantize_filter_outputs.data(),
+                                      dequantize_filter_outputs.size())));
+    const std::array<int32_t, 1> dequantize_bias_inputs{
+        {SparseWeights() ? 2 : 1}};
+    const std::array<int32_t, 1> dequantize_bias_outputs{
+        {SparseWeights() ? 5 : 4}};
+    operators.emplace_back(CreateOperator(
+        builder, /*opcode_index=*/operator_codes.size() - 1,
+        builder.CreateVector<int32_t>(dequantize_bias_inputs.data(),
+                                      dequantize_bias_inputs.size()),
+        builder.CreateVector<int32_t>(dequantize_bias_outputs.data(),
+                                      dequantize_bias_outputs.size())));
+  } else {
+    std::vector<float> filter_data(OutputChannels() * KernelHeight() *
+                                   KernelWidth() * InputChannels());
+    std::vector<float> bias_data(OutputChannels());
+    for (int32_t oc = 0; oc < OutputChannels(); oc++) {
+      // Use the same range of all-positive or all-negative values to generate
+      // all weights within the same output channel, but different ranges for
+      // different output channels. This ensures that no catastrophic
+      // cancellation occur, but test covers both positive and negative inputs.
+      const float range = range_rng();
+      auto value_rng =
+          std::bind(std::uniform_real_distribution<float>(
+                        std::min(range, 0.0f), std::max(range, 0.0f)),
+                    std::ref(rng));
+      bias_data[oc] = value_rng();
+      for (int32_t ic = 0; ic < InputChannels(); ic++) {
+        for (int32_t y = 0; y < KernelHeight(); y++) {
+          for (int32_t x = 0; x < KernelWidth(); x++) {
+            const int32_t index =
+                ((oc * KernelHeight() + y) * KernelWidth() + x) *
+                    InputChannels() +
+                ic;
+            filter_data[index] = value_rng();
+          }
+        }
+      }
+    }
+
+    buffers.emplace_back(CreateBuffer(
+        builder, builder.CreateVector(
+                     reinterpret_cast<const uint8_t*>(filter_data.data()),
+                     sizeof(float) * filter_data.size())));
+    buffers.emplace_back(CreateBuffer(
+        builder,
+        builder.CreateVector(reinterpret_cast<const uint8_t*>(bias_data.data()),
+                             sizeof(float) * bias_data.size())));
+  }
+
+  const std::array<int32_t, 4> input_shape{
+      {BatchSize(), InputHeight(), InputWidth(), InputChannels()}};
+  const std::array<int32_t, 4> output_shape{
+      {BatchSize(), OutputHeight(), OutputWidth(), OutputChannels()}};
+  const std::array<int32_t, 4> filter_shape{
+      {OutputChannels(), KernelHeight(), KernelWidth(), InputChannels()}};
+  const std::array<int32_t, 1> bias_shape{{OutputChannels()}};
+
+  std::vector<flatbuffers::Offset<tflite::Tensor>> tensors;
+  if (SparseWeights()) {
+    // Sparse tensor in TFLite can be in different formats. Here we choose the
+    // simplest configuration that
+    //   1. all dimensions are dense,
+    //   2. in-order traversal, and
+    //   3. no block configuration.
+    int dims_count = filter_shape.size();
+    std::vector<flatbuffers::Offset<DimensionMetadata>> dim_metadata(
+        dims_count);
+    std::vector<int> traversal_order(dims_count);
+    for (int i = 0; i < dims_count; i++) {
+      traversal_order[i] = i;
+      dim_metadata[i] = CreateDimensionMetadata(builder, DimensionType_DENSE,
+                                                filter_shape[i]);
+    }
+    flatbuffers::Offset<SparsityParameters> sparsity_param =
+        CreateSparsityParameters(builder, builder.CreateVector(traversal_order),
+                                 0, builder.CreateVector(dim_metadata));
+    tensors.emplace_back(CreateTensor(
+        builder,
+        builder.CreateVector<int32_t>(filter_shape.data(), filter_shape.size()),
+        /*type=*/FP16Weights() ? TensorType_FLOAT16 : TensorType_FLOAT32,
+        /*buffer=*/1, /*name=*/0, /*quantization=*/0,
+        /*is_variable=*/false, /*sparsity=*/sparsity_param));
+  }
+  if (FP16Weights()) {
+    tensors.emplace_back(CreateTensor(
+        builder,
+        builder.CreateVector<int32_t>(filter_shape.data(), filter_shape.size()),
+        TensorType_FLOAT16, /*buffer=*/SparseWeights() ? 0 : 1));
+    tensors.emplace_back(CreateTensor(
+        builder,
+        builder.CreateVector<int32_t>(bias_shape.data(), bias_shape.size()),
+        TensorType_FLOAT16, /*buffer=*/2));
+  }
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(input_shape.data(), input_shape.size()),
+      TensorType_FLOAT32));
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(filter_shape.data(), filter_shape.size()),
+      TensorType_FLOAT32, /*buffer=*/FP16Weights() || SparseWeights() ? 0 : 1));
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(bias_shape.data(), bias_shape.size()),
+      TensorType_FLOAT32, /*buffer=*/FP16Weights() ? 0 : 2));
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(output_shape.data(), output_shape.size()),
+      TensorType_FLOAT32));
+
+  const std::array<int32_t, 3> op_inputs{
+      {static_cast<int>(tensors.size()) - 4,
+       static_cast<int>(tensors.size()) - 3,
+       static_cast<int>(tensors.size()) - 2}};
+  const std::array<int32_t, 1> op_outputs{
+      {static_cast<int>(tensors.size()) - 1}};
+
+  flatbuffers::Offset<Conv2DOptions> conv2d_options =
+      CreateConv2DOptions(builder, Padding(), StrideWidth(), StrideHeight(),
+                          Activation(), DilationWidth(), DilationHeight());
+  operators.emplace_back(CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()),
+      BuiltinOptions_Conv2DOptions, conv2d_options.Union()));
+
+  const std::array<int32_t, 1> subgraph_inputs{
+      {static_cast<int>(tensors.size()) - 4}};
+  const std::array<int32_t, 1> subgraph_outputs{
+      {static_cast<int>(tensors.size()) - 1}};
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(operators.data(), operators.size()));
+
+  flatbuffers::Offset<flatbuffers::String> description =
+      builder.CreateString("Conv2D model");
+
+  flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION,
+      builder.CreateVector(operator_codes.data(), operator_codes.size()),
+      builder.CreateVector(&subgraph, 1), description,
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+}  // namespace webnn
+}  // namespace tflite
\ No newline at end of file
diff --git a/tensorflow/lite/delegates/webnn/conv_2d_tester.h b/tensorflow/lite/delegates/webnn/conv_2d_tester.h
new file mode 100644
index 00000000000..ccd7565409c
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/conv_2d_tester.h
@@ -0,0 +1,250 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_CONV_2D_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_CONV_2D_TESTER_H_
+
+#include <cstdint>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+
+namespace tflite {
+namespace webnn {
+
+class Conv2DTester {
+ public:
+  Conv2DTester() = default;
+  Conv2DTester(const Conv2DTester&) = delete;
+  Conv2DTester& operator=(const Conv2DTester&) = delete;
+
+  inline Conv2DTester& BatchSize(int32_t batch_size) {
+    EXPECT_GT(batch_size, 0);
+    batch_size_ = batch_size;
+    return *this;
+  }
+
+  inline int32_t BatchSize() const { return batch_size_; }
+
+  inline Conv2DTester& InputChannels(int32_t input_channels) {
+    EXPECT_GT(input_channels, 0);
+    input_channels_ = input_channels;
+    return *this;
+  }
+
+  inline int32_t InputChannels() const { return input_channels_; }
+
+  inline Conv2DTester& OutputChannels(int32_t output_channels) {
+    EXPECT_GT(output_channels, 0);
+    output_channels_ = output_channels;
+    return *this;
+  }
+
+  inline int32_t OutputChannels() const { return output_channels_; }
+
+  inline Conv2DTester& InputHeight(int32_t input_height) {
+    EXPECT_GT(input_height, 0);
+    input_height_ = input_height;
+    return *this;
+  }
+
+  inline int32_t InputHeight() const { return input_height_; }
+
+  inline Conv2DTester& InputWidth(int32_t input_width) {
+    EXPECT_GT(input_width, 0);
+    input_width_ = input_width;
+    return *this;
+  }
+
+  inline int32_t InputWidth() const { return input_width_; }
+
+  inline int32_t OutputWidth() const {
+    if (Padding() == ::tflite::Padding_SAME) {
+      EXPECT_GE(InputWidth(), 1);
+      return (InputWidth() - 1) / StrideWidth() + 1;
+    } else {
+      EXPECT_GE(InputWidth(), DilatedKernelWidth());
+      return 1 + (InputWidth() - DilatedKernelWidth()) / StrideWidth();
+    }
+  }
+
+  inline int32_t OutputHeight() const {
+    if (Padding() == ::tflite::Padding_SAME) {
+      EXPECT_GE(InputHeight(), 1);
+      return (InputHeight() - 1) / StrideHeight() + 1;
+    } else {
+      EXPECT_GE(InputHeight(), DilatedKernelHeight());
+      return 1 + (InputHeight() - DilatedKernelHeight()) / StrideHeight();
+    }
+  }
+
+  inline Conv2DTester& KernelHeight(int32_t kernel_height) {
+    EXPECT_GT(kernel_height, 0);
+    kernel_height_ = kernel_height;
+    return *this;
+  }
+
+  inline int32_t KernelHeight() const { return kernel_height_; }
+
+  inline Conv2DTester& KernelWidth(int32_t kernel_width) {
+    EXPECT_GT(kernel_width, 0);
+    kernel_width_ = kernel_width;
+    return *this;
+  }
+
+  inline int32_t KernelWidth() const { return kernel_width_; }
+
+  inline Conv2DTester& StrideHeight(int32_t stride_height) {
+    EXPECT_GT(stride_height, 0);
+    stride_height_ = stride_height;
+    return *this;
+  }
+
+  inline int32_t StrideHeight() const { return stride_height_; }
+
+  inline Conv2DTester& StrideWidth(int32_t stride_width) {
+    EXPECT_GT(stride_width, 0);
+    stride_width_ = stride_width;
+    return *this;
+  }
+
+  inline int32_t StrideWidth() const { return stride_width_; }
+
+  inline Conv2DTester& DilationHeight(int32_t dilation_height) {
+    EXPECT_GT(dilation_height, 0);
+    dilation_height_ = dilation_height;
+    return *this;
+  }
+
+  inline int32_t DilationHeight() const { return dilation_height_; }
+
+  inline Conv2DTester& DilationWidth(int32_t dilation_width) {
+    EXPECT_GT(dilation_width, 0);
+    dilation_width_ = dilation_width;
+    return *this;
+  }
+
+  inline int32_t DilationWidth() const { return dilation_width_; }
+
+  inline int32_t DilatedKernelHeight() const {
+    return (KernelHeight() - 1) * DilationHeight() + 1;
+  }
+
+  inline int32_t DilatedKernelWidth() const {
+    return (KernelWidth() - 1) * DilationWidth() + 1;
+  }
+
+  inline Conv2DTester& FP16Weights() {
+    fp16_weights_ = true;
+    return *this;
+  }
+
+  inline bool FP16Weights() const { return fp16_weights_; }
+
+  inline Conv2DTester& INT8Weights() {
+    int8_weights_ = true;
+    return *this;
+  }
+
+  inline bool INT8Weights() const { return int8_weights_; }
+
+  inline Conv2DTester& INT8ChannelWiseWeights() {
+    int8_channel_wise_weights_ = true;
+    return *this;
+  }
+
+  inline bool INT8ChannelWiseWeights() const {
+    return int8_channel_wise_weights_;
+  }
+
+  inline Conv2DTester& SparseWeights() {
+    sparse_weights_ = true;
+    return *this;
+  }
+
+  inline bool SparseWeights() const { return sparse_weights_; }
+
+  inline Conv2DTester& SamePadding() {
+    padding_ = ::tflite::Padding_SAME;
+    return *this;
+  }
+
+  inline Conv2DTester& ValidPadding() {
+    padding_ = ::tflite::Padding_VALID;
+    return *this;
+  }
+
+  inline Conv2DTester& ReluActivation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU;
+    return *this;
+  }
+
+  inline Conv2DTester& Relu6Activation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU6;
+    return *this;
+  }
+
+  inline Conv2DTester& ReluMinus1To1Activation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU_N1_TO_1;
+    return *this;
+  }
+
+  inline Conv2DTester& TanhActivation() {
+    activation_ = ::tflite::ActivationFunctionType_TANH;
+    return *this;
+  }
+
+  inline Conv2DTester& SignBitActivation() {
+    activation_ = ::tflite::ActivationFunctionType_SIGN_BIT;
+    return *this;
+  }
+
+  void Test(TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel() const;
+
+  inline ::tflite::Padding Padding() const { return padding_; }
+
+  inline ::tflite::ActivationFunctionType Activation() const {
+    return activation_;
+  }
+
+  int32_t batch_size_ = 1;
+  int32_t input_channels_ = 1;
+  int32_t output_channels_ = 1;
+  int32_t input_height_ = 1;
+  int32_t input_width_ = 1;
+  int32_t kernel_height_ = 1;
+  int32_t kernel_width_ = 1;
+  int32_t stride_height_ = 1;
+  int32_t stride_width_ = 1;
+  int32_t dilation_height_ = 1;
+  int32_t dilation_width_ = 1;
+  bool fp16_weights_ = false;
+  bool int8_weights_ = false;
+  bool int8_channel_wise_weights_ = false;
+  bool sparse_weights_ = false;
+  ::tflite::Padding padding_ = ::tflite::Padding_VALID;
+  ::tflite::ActivationFunctionType activation_ =
+      ::tflite::ActivationFunctionType_NONE;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_CONV_2D_TESTER_H_
diff --git a/tensorflow/lite/delegates/webnn/delegate_test.cc b/tensorflow/lite/delegates/webnn/delegate_test.cc
new file mode 100644
index 00000000000..1a7a924cbd2
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/delegate_test.cc
@@ -0,0 +1,73 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <gtest/gtest.h>
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Delegate, CreateWithDefaultParams) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+}
+
+TEST(Delegate, CreateWithGpuPreferenceParam) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  delegate_options.deviceType = 1;
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+}
+
+TEST(Delegate, CreateWithCpuPreferenceParam) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  delegate_options.deviceType = 2;
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+}
+
+TEST(Delegate, CreateWithHighPowerPreferenceParam) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  delegate_options.powerPreference = 2;
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+}
+
+TEST(Delegate, CreateWithLowPowerPreferenceParam) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  delegate_options.powerPreference = 1;
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/depthwise_conv_2d_test.cc b/tensorflow/lite/delegates/webnn/depthwise_conv_2d_test.cc
new file mode 100644
index 00000000000..403cf0db94d
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/depthwise_conv_2d_test.cc
@@ -0,0 +1,566 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/depthwise_conv_2d_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(DepthwiseConv2D, 1x1) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(1)
+      .KernelWidth(1)
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, 2x2) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(2)
+      .KernelWidth(2)
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, 3x3) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(3)
+      .KernelWidth(3)
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, 3x3Stride2) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(3)
+      .KernelWidth(3)
+      .StrideHeight(2)
+      .StrideWidth(2)
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, 5x5) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(3)
+      .KernelWidth(3)
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, 5x5Stride2) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(3)
+      .KernelWidth(3)
+      .StrideHeight(2)
+      .StrideWidth(2)
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, SmallKernelWithSamePadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 7), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, SmallKernelWithValidPadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 7), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .ValidPadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, StrideWithSamePadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, StrideWithValidPadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .ValidPadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, DilationWithSamePadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto dilation_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .DilationHeight(dilation_rng())
+      .DilationWidth(dilation_rng())
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, DilationWithValidPadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto dilation_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .DilationHeight(dilation_rng())
+      .DilationWidth(dilation_rng())
+      .ValidPadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, DepthMultiplier) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+  auto multiplier_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 8), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .DepthMultiplier(multiplier_rng())
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, FP16Weights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .FP16Weights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, SparseWeights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SparseWeights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, ReluActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .ReluActivation()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, Relu6Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .Relu6Activation()
+      .Test(webnn_delegate.get());
+}
+
+TEST(DepthwiseConv2D, ReluMinus1To1Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 32), std::ref(rng));
+
+  DepthwiseConv2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .InputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .ReluMinus1To1Activation()
+      .Test(webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/depthwise_conv_2d_tester.cc b/tensorflow/lite/delegates/webnn/depthwise_conv_2d_tester.cc
new file mode 100644
index 00000000000..3ec705ab9dd
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/depthwise_conv_2d_tester.cc
@@ -0,0 +1,338 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/depthwise_conv_2d_tester.h"
+
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <random>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include <fp16.h>
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+void DepthwiseConv2DTester::Test(TfLiteDelegate* delegate) const {
+  std::vector<char> buffer = CreateTfLiteModel();
+  const Model* model = GetModel(buffer.data());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+
+  ASSERT_EQ(delegate_interpreter->inputs().size(), 1);
+  ASSERT_EQ(default_interpreter->inputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->outputs().size(), 1);
+  ASSERT_EQ(default_interpreter->outputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto input_rng =
+      std::bind(std::uniform_real_distribution<float>(), std::ref(rng));
+  float* default_input_data = default_interpreter->typed_tensor<float>(
+      default_interpreter->inputs()[0]);
+  std::generate(default_input_data,
+                default_input_data + BatchSize() * InputHeight() *
+                                         InputWidth() * InputChannels(),
+                input_rng);
+
+  float* delegate_input_data = delegate_interpreter->typed_tensor<float>(
+      delegate_interpreter->inputs()[0]);
+  std::copy(default_input_data,
+            default_input_data +
+                BatchSize() * InputHeight() * InputWidth() * InputChannels(),
+            delegate_input_data);
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float* default_output_data = default_interpreter->typed_tensor<float>(
+      default_interpreter->outputs()[0]);
+  float* delegate_output_data = delegate_interpreter->typed_tensor<float>(
+      delegate_interpreter->outputs()[0]);
+
+  for (int32_t i = 0; i < BatchSize(); i++) {
+    for (int32_t y = 0; y < OutputHeight(); y++) {
+      for (int32_t x = 0; x < OutputWidth(); x++) {
+        for (int32_t c = 0; c < OutputChannels(); c++) {
+          const int32_t index = ((i * OutputHeight() + y) * OutputWidth() + x) *
+                                    OutputChannels() +
+                                c;
+          ASSERT_NEAR(default_output_data[index], delegate_output_data[index],
+                      std::abs(default_output_data[index]) * 3.0e-6f)
+              << "batch " << i << " / " << BatchSize() << ", y position " << y
+              << " / " << OutputHeight() << ", x position " << x << " / "
+              << OutputWidth() << ", channel " << c << " / "
+              << OutputChannels();
+        }
+      }
+    }
+  }
+}
+
+std::vector<char> DepthwiseConv2DTester::CreateTfLiteModel() const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto range_rng = std::bind(
+      std::uniform_real_distribution<float>(-25.0f, 25.0f), std::ref(rng));
+
+  flatbuffers::FlatBufferBuilder builder;
+  std::vector<flatbuffers::Offset<OperatorCode>> operator_codes{
+      {CreateOperatorCode(builder, BuiltinOperator_DEPTHWISE_CONV_2D)}};
+  std::vector<flatbuffers::Offset<tflite::Operator>> operators;
+  std::vector<flatbuffers::Offset<tflite::Buffer>> buffers{
+      {CreateBuffer(builder, builder.CreateVector({}))}};
+
+  if (FP16Weights()) {
+    operator_codes.emplace_back(
+        CreateOperatorCode(builder, BuiltinOperator_DEQUANTIZE));
+
+    std::vector<uint16_t> filter_data(KernelHeight() * KernelWidth() *
+                                      OutputChannels());
+    std::vector<uint16_t> bias_data(OutputChannels());
+    for (int32_t ic = 0; ic < InputChannels(); ic++) {
+      // Use the same range of all-positive or all-negative values to generate
+      // all pixels within the same batch index & channel, but different ranges
+      // for different channels or batches. This ensures that no catastrophic
+      // cancellation occur, but test covers both positive and negative inputs.
+      const float range = range_rng();
+      auto value_rng =
+          std::bind(fp16_ieee_from_fp32_value,
+                    std::bind(std::uniform_real_distribution<float>(
+                                  std::min(range, 0.0f), std::max(range, 0.0f)),
+                              std::ref(rng)));
+      for (int32_t m = 0; m < DepthMultiplier(); m++) {
+        const int32_t oc = ic * DepthMultiplier() + m;
+        bias_data[oc] = value_rng();
+        for (int32_t y = 0; y < KernelHeight(); y++) {
+          for (int32_t x = 0; x < KernelWidth(); x++) {
+            const int32_t index =
+                (y * KernelWidth() + x) * OutputChannels() + oc;
+            filter_data[index] = value_rng();
+          }
+        }
+      }
+    }
+
+    buffers.emplace_back(CreateBuffer(
+        builder, builder.CreateVector(
+                     reinterpret_cast<const uint8_t*>(filter_data.data()),
+                     sizeof(uint16_t) * filter_data.size())));
+    buffers.emplace_back(CreateBuffer(
+        builder,
+        builder.CreateVector(reinterpret_cast<const uint8_t*>(bias_data.data()),
+                             sizeof(uint16_t) * bias_data.size())));
+
+    const std::array<int32_t, 1> dequantize_filter_inputs{{0}};
+    const std::array<int32_t, 1> dequantize_filter_outputs{{3}};
+    operators.emplace_back(CreateOperator(
+        builder, /*opcode_index=*/1,
+        builder.CreateVector<int32_t>(dequantize_filter_inputs.data(),
+                                      dequantize_filter_inputs.size()),
+        builder.CreateVector<int32_t>(dequantize_filter_outputs.data(),
+                                      dequantize_filter_outputs.size())));
+    const std::array<int32_t, 1> dequantize_bias_inputs{{1}};
+    const std::array<int32_t, 1> dequantize_bias_outputs{{4}};
+    operators.emplace_back(CreateOperator(
+        builder, /*opcode_index=*/1,
+        builder.CreateVector<int32_t>(dequantize_bias_inputs.data(),
+                                      dequantize_bias_inputs.size()),
+        builder.CreateVector<int32_t>(dequantize_bias_outputs.data(),
+                                      dequantize_bias_outputs.size())));
+  } else {
+    std::vector<float> filter_data(KernelHeight() * KernelWidth() *
+                                   OutputChannels());
+    std::vector<float> bias_data(OutputChannels());
+    for (int32_t ic = 0; ic < InputChannels(); ic++) {
+      // Use the same range of all-positive or all-negative values to generate
+      // all pixels within the same batch index & channel, but different ranges
+      // for different channels or batches. This ensures that no catastrophic
+      // cancellation occur, but test covers both positive and negative inputs.
+      const float range = range_rng();
+      auto value_rng =
+          std::bind(std::uniform_real_distribution<float>(
+                        std::min(range, 0.0f), std::max(range, 0.0f)),
+                    std::ref(rng));
+      for (int32_t m = 0; m < DepthMultiplier(); m++) {
+        const int32_t oc = ic * DepthMultiplier() + m;
+        bias_data[oc] = value_rng();
+        for (int32_t y = 0; y < KernelHeight(); y++) {
+          for (int32_t x = 0; x < KernelWidth(); x++) {
+            const int32_t index =
+                (y * KernelWidth() + x) * OutputChannels() + oc;
+            filter_data[index] = value_rng();
+          }
+        }
+      }
+    }
+
+    buffers.emplace_back(CreateBuffer(
+        builder, builder.CreateVector(
+                     reinterpret_cast<const uint8_t*>(filter_data.data()),
+                     sizeof(float) * filter_data.size())));
+    buffers.emplace_back(CreateBuffer(
+        builder,
+        builder.CreateVector(reinterpret_cast<const uint8_t*>(bias_data.data()),
+                             sizeof(float) * bias_data.size())));
+
+    if (SparseWeights()) {
+      operator_codes.emplace_back(
+          CreateOperatorCode(builder, BuiltinOperator_DENSIFY));
+      const std::array<int32_t, 1> densify_filter_inputs{{0}};
+      const std::array<int32_t, 1> densify_filter_outputs{{2}};
+      operators.emplace_back(CreateOperator(
+          builder, /*opcode_index=*/1,
+          builder.CreateVector<int32_t>(densify_filter_inputs.data(),
+                                        densify_filter_inputs.size()),
+          builder.CreateVector<int32_t>(densify_filter_outputs.data(),
+                                        densify_filter_outputs.size())));
+    }
+  }
+
+  const std::array<int32_t, 4> input_shape{
+      {BatchSize(), InputHeight(), InputWidth(), InputChannels()}};
+  const std::array<int32_t, 4> output_shape{
+      {BatchSize(), OutputHeight(), OutputWidth(), OutputChannels()}};
+  const std::array<int32_t, 4> filter_shape{
+      {1, KernelHeight(), KernelWidth(), OutputChannels()}};
+  const std::array<int32_t, 1> bias_shape{{OutputChannels()}};
+
+  std::vector<flatbuffers::Offset<tflite::Tensor>> tensors;
+  if (FP16Weights()) {
+    tensors.emplace_back(CreateTensor(
+        builder,
+        builder.CreateVector<int32_t>(filter_shape.data(), filter_shape.size()),
+        TensorType_FLOAT16, /*buffer=*/1));
+    tensors.emplace_back(CreateTensor(
+        builder,
+        builder.CreateVector<int32_t>(bias_shape.data(), bias_shape.size()),
+        TensorType_FLOAT16, /*buffer=*/2));
+  } else if (SparseWeights()) {
+    // Sparse tensor in TFLite can be in different formats. Here we choose the
+    // simplest configuration that
+    //   1. all dimensions are dense,
+    //   2. in-order traversal, and
+    //   3. no block configuration.
+    int dims_count = filter_shape.size();
+    std::vector<flatbuffers::Offset<DimensionMetadata>> dim_metadata(
+        dims_count);
+    std::vector<int> traversal_order(dims_count);
+    for (int i = 0; i < dims_count; i++) {
+      traversal_order[i] = i;
+      dim_metadata[i] = CreateDimensionMetadata(builder, DimensionType_DENSE,
+                                                filter_shape[i]);
+    }
+    flatbuffers::Offset<SparsityParameters> sparsity_param =
+        CreateSparsityParameters(builder, builder.CreateVector(traversal_order),
+                                 0, builder.CreateVector(dim_metadata));
+    tensors.emplace_back(CreateTensor(
+        builder,
+        builder.CreateVector<int32_t>(filter_shape.data(), filter_shape.size()),
+        TensorType_FLOAT32, /*buffer=*/1, /*name=*/0, /*quantization=*/0,
+        /*is_variable=*/false, /*sparsity=*/sparsity_param));
+  }
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(input_shape.data(), input_shape.size()),
+      TensorType_FLOAT32));
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(filter_shape.data(), filter_shape.size()),
+      TensorType_FLOAT32, /*buffer=*/FP16Weights() || SparseWeights() ? 0 : 1));
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(bias_shape.data(), bias_shape.size()),
+      TensorType_FLOAT32, /*buffer=*/FP16Weights() ? 0 : 2));
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(output_shape.data(), output_shape.size()),
+      TensorType_FLOAT32));
+
+  const std::array<int32_t, 3> op_inputs{
+      {static_cast<int>(tensors.size()) - 4,
+       static_cast<int>(tensors.size()) - 3,
+       static_cast<int>(tensors.size()) - 2}};
+  const std::array<int32_t, 1> op_outputs{
+      {static_cast<int>(tensors.size()) - 1}};
+
+  flatbuffers::Offset<DepthwiseConv2DOptions> depthwise_conv2d_options =
+      CreateDepthwiseConv2DOptions(
+          builder, Padding(), StrideWidth(), StrideHeight(), DepthMultiplier(),
+          Activation(), DilationWidth(), DilationHeight());
+  operators.emplace_back(CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()),
+      BuiltinOptions_DepthwiseConv2DOptions, depthwise_conv2d_options.Union()));
+
+  const std::array<int32_t, 1> subgraph_inputs{
+      {static_cast<int>(tensors.size()) - 4}};
+  const std::array<int32_t, 1> subgraph_outputs{
+      {static_cast<int>(tensors.size()) - 1}};
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(operators.data(), operators.size()));
+
+  flatbuffers::Offset<flatbuffers::String> description =
+      builder.CreateString("DepthwiseConv2D model");
+
+  flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION,
+      builder.CreateVector(operator_codes.data(), operator_codes.size()),
+      builder.CreateVector(&subgraph, 1), description,
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+}  // namespace webnn
+}  // namespace tflite
\ No newline at end of file
diff --git a/tensorflow/lite/delegates/webnn/depthwise_conv_2d_tester.h b/tensorflow/lite/delegates/webnn/depthwise_conv_2d_tester.h
new file mode 100644
index 00000000000..5cc8285bcd0
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/depthwise_conv_2d_tester.h
@@ -0,0 +1,254 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_DEPTHWISE_CONV_2D_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_DEPTHWISE_CONV_2D_TESTER_H_
+
+#include <cstdint>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+
+namespace tflite {
+namespace webnn {
+
+class DepthwiseConv2DTester {
+ public:
+  DepthwiseConv2DTester() = default;
+  DepthwiseConv2DTester(const DepthwiseConv2DTester&) = delete;
+  DepthwiseConv2DTester& operator=(const DepthwiseConv2DTester&) = delete;
+
+  inline DepthwiseConv2DTester& BatchSize(int32_t batch_size) {
+    EXPECT_GT(batch_size, 0);
+    batch_size_ = batch_size;
+    return *this;
+  }
+
+  inline int32_t BatchSize() const { return batch_size_; }
+
+  inline DepthwiseConv2DTester& InputChannels(int32_t input_channels) {
+    EXPECT_GT(input_channels, 0);
+    input_channels_ = input_channels;
+    return *this;
+  }
+
+  inline int32_t InputChannels() const { return input_channels_; }
+
+  inline DepthwiseConv2DTester& DepthMultiplier(int32_t depth_multiplier) {
+    EXPECT_GT(depth_multiplier, 0);
+    depth_multiplier_ = depth_multiplier;
+    return *this;
+  }
+
+  inline int32_t DepthMultiplier() const { return depth_multiplier_; }
+
+  inline int32_t OutputChannels() const {
+    return DepthMultiplier() * InputChannels();
+  }
+
+  inline DepthwiseConv2DTester& InputHeight(int32_t input_height) {
+    EXPECT_GT(input_height, 0);
+    input_height_ = input_height;
+    return *this;
+  }
+
+  inline int32_t InputHeight() const { return input_height_; }
+
+  inline DepthwiseConv2DTester& InputWidth(int32_t input_width) {
+    EXPECT_GT(input_width, 0);
+    input_width_ = input_width;
+    return *this;
+  }
+
+  inline int32_t InputWidth() const { return input_width_; }
+
+  inline int32_t OutputWidth() const {
+    if (Padding() == ::tflite::Padding_SAME) {
+      EXPECT_GE(InputWidth(), 1);
+      return (InputWidth() - 1) / StrideWidth() + 1;
+    } else {
+      EXPECT_GE(InputWidth(), DilatedKernelWidth());
+      return 1 + (InputWidth() - DilatedKernelWidth()) / StrideWidth();
+    }
+  }
+
+  inline int32_t OutputHeight() const {
+    if (Padding() == ::tflite::Padding_SAME) {
+      EXPECT_GE(InputHeight(), 1);
+      return (InputHeight() - 1) / StrideHeight() + 1;
+    } else {
+      EXPECT_GE(InputHeight(), DilatedKernelHeight());
+      return 1 + (InputHeight() - DilatedKernelHeight()) / StrideHeight();
+    }
+  }
+
+  inline DepthwiseConv2DTester& KernelHeight(int32_t kernel_height) {
+    EXPECT_GT(kernel_height, 0);
+    kernel_height_ = kernel_height;
+    return *this;
+  }
+
+  inline int32_t KernelHeight() const { return kernel_height_; }
+
+  inline DepthwiseConv2DTester& KernelWidth(int32_t kernel_width) {
+    EXPECT_GT(kernel_width, 0);
+    kernel_width_ = kernel_width;
+    return *this;
+  }
+
+  inline int32_t KernelWidth() const { return kernel_width_; }
+
+  inline DepthwiseConv2DTester& StrideHeight(int32_t stride_height) {
+    EXPECT_GT(stride_height, 0);
+    stride_height_ = stride_height;
+    return *this;
+  }
+
+  inline int32_t StrideHeight() const { return stride_height_; }
+
+  inline DepthwiseConv2DTester& StrideWidth(int32_t stride_width) {
+    EXPECT_GT(stride_width, 0);
+    stride_width_ = stride_width;
+    return *this;
+  }
+
+  inline int32_t StrideWidth() const { return stride_width_; }
+
+  inline DepthwiseConv2DTester& DilationHeight(int32_t dilation_height) {
+    EXPECT_GT(dilation_height, 0);
+    dilation_height_ = dilation_height;
+    return *this;
+  }
+
+  inline int32_t DilationHeight() const { return dilation_height_; }
+
+  inline DepthwiseConv2DTester& DilationWidth(int32_t dilation_width) {
+    EXPECT_GT(dilation_width, 0);
+    dilation_width_ = dilation_width;
+    return *this;
+  }
+
+  inline int32_t DilationWidth() const { return dilation_width_; }
+
+  inline int32_t DilatedKernelHeight() const {
+    return (KernelHeight() - 1) * DilationHeight() + 1;
+  }
+
+  inline int32_t DilatedKernelWidth() const {
+    return (KernelWidth() - 1) * DilationWidth() + 1;
+  }
+
+  inline DepthwiseConv2DTester& FP16Weights() {
+    fp16_weights_ = true;
+    return *this;
+  }
+
+  inline bool FP16Weights() const { return fp16_weights_; }
+
+  inline DepthwiseConv2DTester& INT8Weights() {
+    int8_weights_ = true;
+    return *this;
+  }
+
+  inline bool INT8Weights() const { return int8_weights_; }
+
+  inline DepthwiseConv2DTester& INT8ChannelWiseWeights() {
+    int8_channel_wise_weights_ = true;
+    return *this;
+  }
+
+  inline bool INT8ChannelWiseWeights() const {
+    return int8_channel_wise_weights_;
+  }
+
+  inline DepthwiseConv2DTester& SparseWeights() {
+    sparse_weights_ = true;
+    return *this;
+  }
+
+  inline bool SparseWeights() const { return sparse_weights_; }
+
+  inline DepthwiseConv2DTester& SamePadding() {
+    padding_ = ::tflite::Padding_SAME;
+    return *this;
+  }
+
+  inline DepthwiseConv2DTester& ValidPadding() {
+    padding_ = ::tflite::Padding_VALID;
+    return *this;
+  }
+
+  inline DepthwiseConv2DTester& ReluActivation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU;
+    return *this;
+  }
+
+  inline DepthwiseConv2DTester& Relu6Activation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU6;
+    return *this;
+  }
+
+  inline DepthwiseConv2DTester& ReluMinus1To1Activation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU_N1_TO_1;
+    return *this;
+  }
+
+  inline DepthwiseConv2DTester& TanhActivation() {
+    activation_ = ::tflite::ActivationFunctionType_TANH;
+    return *this;
+  }
+
+  inline DepthwiseConv2DTester& SignBitActivation() {
+    activation_ = ::tflite::ActivationFunctionType_SIGN_BIT;
+    return *this;
+  }
+
+  void Test(TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel() const;
+
+  inline ::tflite::Padding Padding() const { return padding_; }
+
+  inline ::tflite::ActivationFunctionType Activation() const {
+    return activation_;
+  }
+
+  int32_t batch_size_ = 1;
+  int32_t input_channels_ = 1;
+  int32_t depth_multiplier_ = 1;
+  int32_t input_height_ = 1;
+  int32_t input_width_ = 1;
+  int32_t kernel_height_ = 1;
+  int32_t kernel_width_ = 1;
+  int32_t stride_height_ = 1;
+  int32_t stride_width_ = 1;
+  int32_t dilation_height_ = 1;
+  int32_t dilation_width_ = 1;
+  bool fp16_weights_ = false;
+  bool int8_weights_ = false;
+  bool int8_channel_wise_weights_ = false;
+  bool sparse_weights_ = false;
+  ::tflite::Padding padding_ = ::tflite::Padding_VALID;
+  ::tflite::ActivationFunctionType activation_ =
+      ::tflite::ActivationFunctionType_NONE;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_DEPTHWISE_CONV_2D_TESTER_H_
diff --git a/tensorflow/lite/delegates/webnn/fully_connected_test.cc b/tensorflow/lite/delegates/webnn/fully_connected_test.cc
new file mode 100644
index 00000000000..714e6941fdc
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/fully_connected_test.cc
@@ -0,0 +1,504 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/fully_connected_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(FullyConnected, 1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+
+  FullyConnectedTester()
+      .InputShape({input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, 1DKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .KeepDims(true)
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, 2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = batch_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, 2DKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = batch_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .KeepDims(true)
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, 3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto width = shape_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, width, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, 3DReshape) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto width = shape_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, width, input_channels})
+      .InputChannels(width * input_channels)
+      .OutputChannels(output_channels)
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, 3DKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto width = shape_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, width, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .KeepDims(true)
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, 4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, height, width, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, 4DKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, height, width, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .KeepDims(true)
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, NoBias) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = batch_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .NoBias()
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, FP16Weights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = batch_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .FP16Weights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, FP16WeightsNoBias) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = batch_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .FP16Weights()
+      .NoBias()
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, INT8Weights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = batch_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .INT8Weights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, INT8WeightsNoBias) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = batch_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .INT8Weights()
+      .NoBias()
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, INT8ChannelWiseWeights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = batch_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .INT8ChannelWiseWeights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, INT8ChannelWiseWeightsNoBias) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = batch_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .INT8ChannelWiseWeights()
+      .NoBias()
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, ReluActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = batch_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .ReluActivation()
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, Relu6Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = batch_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .Relu6Activation()
+      .Test(webnn_delegate.get());
+}
+
+TEST(FullyConnected, ReluMinus1To1Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  auto channels_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 9), std::ref(rng));
+  const auto batch = batch_rng();
+  const auto input_channels = channels_rng();
+  const auto output_channels = channels_rng();
+
+  FullyConnectedTester()
+      .InputShape({batch, input_channels})
+      .InputChannels(input_channels)
+      .OutputChannels(output_channels)
+      .ReluMinus1To1Activation()
+      .Test(webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/fully_connected_tester.cc b/tensorflow/lite/delegates/webnn/fully_connected_tester.cc
new file mode 100644
index 00000000000..8c9ecbd8a2a
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/fully_connected_tester.cc
@@ -0,0 +1,315 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/fully_connected_tester.h"
+
+#include <fp16.h>
+#include <gtest/gtest.h>
+
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <numeric>
+#include <random>
+#include <vector>
+
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+std::vector<int32_t> FullyConnectedTester::OutputShape() const {
+  EXPECT_NE(input_shape_.size(), 0);
+  if (KeepDims()) {
+    std::vector<int32_t> output_shape(input_shape_.cbegin(),
+                                      input_shape_.cend() - 1);
+    output_shape.push_back(OutputChannels());
+    return output_shape;
+  } else {
+    EXPECT_EQ(InputSize() % InputChannels(), 0);
+    return std::vector<int32_t>(
+        {InputSize() / InputChannels(), OutputChannels()});
+  }
+}
+
+void FullyConnectedTester::Test(TfLiteDelegate* delegate) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto input_rng =
+      std::bind(std::uniform_real_distribution<float>(), std::ref(rng));
+
+  std::vector<char> buffer = CreateTfLiteModel();
+  const Model* model = GetModel(buffer.data());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+
+  ASSERT_EQ(delegate_interpreter->inputs().size(), 1);
+  ASSERT_EQ(default_interpreter->inputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->outputs().size(), 1);
+  ASSERT_EQ(default_interpreter->outputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  float* default_input_data = default_interpreter->typed_tensor<float>(
+      default_interpreter->inputs()[0]);
+  std::generate(default_input_data, default_input_data + InputSize(),
+                std::ref(input_rng));
+
+  float* delegate_input_data = delegate_interpreter->typed_tensor<float>(
+      delegate_interpreter->inputs()[0]);
+  std::copy(default_input_data, default_input_data + InputSize(),
+            delegate_input_data);
+
+  std::string input_data_string = "Input data:\n";
+  for (size_t i = 0; i < InputSize(); i++) {
+    input_data_string += std::to_string(default_input_data[i]) + " ";
+    if (i % InputChannels() == InputChannels() - 1) input_data_string += "\n";
+  }
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float* default_output_data = default_interpreter->typed_tensor<float>(
+      default_interpreter->outputs()[0]);
+  float* delegate_output_data = delegate_interpreter->typed_tensor<float>(
+      delegate_interpreter->outputs()[0]);
+
+  for (size_t i = 0; i < ComputeSize(OutputShape()); i++) {
+    ASSERT_NEAR(default_output_data[i], delegate_output_data[i],
+                std::numeric_limits<float>::epsilon() *
+                    std::max(std::abs(default_output_data[i]) * 10.0f, 1.0f))
+        << "Input Size:" << InputSize() << " Input Channels:" << InputChannels()
+        << " Input Data:\n"
+        << input_data_string;
+  }
+}
+
+std::vector<char> FullyConnectedTester::CreateTfLiteModel() const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto range_rng = std::bind(
+      std::uniform_real_distribution<float>(-25.0f, 25.0f), std::ref(rng));
+
+  flatbuffers::FlatBufferBuilder builder;
+  std::vector<flatbuffers::Offset<OperatorCode>> operator_codes{
+      {CreateOperatorCode(builder, BuiltinOperator_FULLY_CONNECTED)}};
+  std::vector<flatbuffers::Offset<Operator>> operators;
+  std::vector<flatbuffers::Offset<Buffer>> buffers{
+      {CreateBuffer(builder, builder.CreateVector({}))}};
+
+  if (FP16Weights()) {
+    operator_codes.emplace_back(
+        CreateOperatorCode(builder, BuiltinOperator_DEQUANTIZE));
+
+    std::vector<uint16_t> filter_data(InputChannels() * OutputChannels());
+    std::vector<uint16_t> bias_data(OutputChannels());
+
+    for (int32_t oc = 0; oc < OutputChannels(); oc++) {
+      // Use the same range of all-positive or all-negative values to generate
+      // all filter & bias weights within the same channel, but different ranges
+      // for different output channels. This ensures that no catastrophic
+      // cancellation occur, but test covers both positive and negative inputs.
+      const float range = range_rng();
+      auto value_rng =
+          std::bind(fp16_ieee_from_fp32_value,
+                    std::bind(std::uniform_real_distribution<float>(
+                                  std::min(range, 0.0f), std::max(range, 0.0f)),
+                              std::ref(rng)));
+
+      bias_data[oc] = value_rng();
+      for (int32_t ic = 0; ic < InputChannels(); ic++) {
+        filter_data[oc * InputChannels() + ic] = value_rng();
+      }
+    }
+
+    buffers.emplace_back(CreateBuffer(
+        builder, builder.CreateVector(
+                     reinterpret_cast<const uint8_t*>(filter_data.data()),
+                     sizeof(uint16_t) * filter_data.size())));
+    if (HasBias()) {
+      buffers.emplace_back(CreateBuffer(
+          builder, builder.CreateVector(
+                       reinterpret_cast<const uint8_t*>(bias_data.data()),
+                       sizeof(uint16_t) * bias_data.size())));
+    }
+
+    const std::array<int32_t, 1> dequantize_filter_inputs{{0}};
+    const std::array<int32_t, 1> dequantize_filter_outputs{
+        {2 + static_cast<int32_t>(HasBias())}};
+    operators.emplace_back(CreateOperator(
+        builder, /*opcode_index=*/1,
+        builder.CreateVector<int32_t>(dequantize_filter_inputs.data(),
+                                      dequantize_filter_inputs.size()),
+        builder.CreateVector<int32_t>(dequantize_filter_outputs.data(),
+                                      dequantize_filter_outputs.size())));
+    if (HasBias()) {
+      const std::array<int32_t, 1> dequantize_bias_inputs{{1}};
+      const std::array<int32_t, 1> dequantize_bias_outputs{{4}};
+      operators.emplace_back(CreateOperator(
+          builder, /*opcode_index=*/1,
+          builder.CreateVector<int32_t>(dequantize_bias_inputs.data(),
+                                        dequantize_bias_inputs.size()),
+          builder.CreateVector<int32_t>(dequantize_bias_outputs.data(),
+                                        dequantize_bias_outputs.size())));
+    }
+  } else {
+    std::vector<float> filter_data(InputChannels() * OutputChannels());
+    std::vector<float> bias_data(OutputChannels());
+
+    for (int32_t oc = 0; oc < OutputChannels(); oc++) {
+      // Use the same range of all-positive or all-negative values to generate
+      // all filter & bias weights within the same channel, but different ranges
+      // for different output channels. This ensures that no catastrophic
+      // cancellation occur, but test covers both positive and negative inputs.
+      const float range = range_rng();
+      auto value_rng =
+          std::bind(std::uniform_real_distribution<float>(
+                        std::min(range, 0.0f), std::max(range, 0.0f)),
+                    std::ref(rng));
+
+      bias_data[oc] = value_rng();
+      for (int32_t ic = 0; ic < InputChannels(); ic++) {
+        filter_data[oc * InputChannels() + ic] = value_rng();
+      }
+    }
+
+    buffers.emplace_back(CreateBuffer(
+        builder, builder.CreateVector(
+                     reinterpret_cast<const uint8_t*>(filter_data.data()),
+                     sizeof(float) * filter_data.size())));
+    if (HasBias()) {
+      buffers.emplace_back(CreateBuffer(
+          builder, builder.CreateVector(
+                       reinterpret_cast<const uint8_t*>(bias_data.data()),
+                       sizeof(float) * bias_data.size())));
+    }
+  }
+
+  const std::array<int32_t, 2> filter_shape{
+      {OutputChannels(), InputChannels()}};
+  const std::array<int32_t, 1> bias_shape{{OutputChannels()}};
+
+  const std::vector<int32_t> output_shape = OutputShape();
+  std::vector<flatbuffers::Offset<Tensor>> tensors;
+  if (FP16Weights()) {
+    tensors.emplace_back(CreateTensor(
+        builder,
+        builder.CreateVector<int32_t>(filter_shape.data(), filter_shape.size()),
+        TensorType_FLOAT16, /*buffer=*/1));
+    if (HasBias()) {
+      tensors.emplace_back(CreateTensor(
+          builder,
+          builder.CreateVector<int32_t>(bias_shape.data(), bias_shape.size()),
+          TensorType_FLOAT16, /*buffer=*/2));
+    }
+  }
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(InputShape().data(), InputShape().size()),
+      TensorType_FLOAT32));
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(filter_shape.data(), filter_shape.size()),
+      TensorType_FLOAT32, /*buffer=*/FP16Weights() ? 0 : 1));
+  if (HasBias()) {
+    tensors.emplace_back(CreateTensor(
+        builder,
+        builder.CreateVector<int32_t>(bias_shape.data(), bias_shape.size()),
+        TensorType_FLOAT32, /*buffer=*/FP16Weights() ? 0 : 2));
+  }
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(output_shape.data(), output_shape.size()),
+      TensorType_FLOAT32));
+
+  flatbuffers::Offset<FullyConnectedOptions> fully_connected_options =
+      CreateFullyConnectedOptions(builder, Activation(),
+                                  FullyConnectedOptionsWeightsFormat_DEFAULT,
+                                  KeepDims());
+
+  std::vector<int32_t> op_inputs{{static_cast<int32_t>(tensors.size()) - 3,
+                                  static_cast<int32_t>(tensors.size()) - 2}};
+  if (HasBias()) {
+    op_inputs.insert(op_inputs.begin(),
+                     static_cast<int32_t>(tensors.size()) - 4);
+  }
+  const std::array<int32_t, 1> op_outputs{
+      {static_cast<int32_t>(tensors.size()) - 1}};
+  operators.emplace_back(CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()),
+      BuiltinOptions_FullyConnectedOptions, fully_connected_options.Union()));
+
+  const std::array<int32_t, 1> subgraph_inputs{
+      {static_cast<int>(tensors.size()) - 3 - static_cast<int32_t>(HasBias())}};
+  const std::array<int32_t, 1> subgraph_outputs{
+      {static_cast<int>(tensors.size()) - 1}};
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(operators.data(), operators.size()));
+
+  flatbuffers::Offset<flatbuffers::String> description =
+      builder.CreateString("Fully Connected model");
+
+  flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION,
+      builder.CreateVector(operator_codes.data(), operator_codes.size()),
+      builder.CreateVector(&subgraph, 1), description,
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+int32_t FullyConnectedTester::ComputeSize(const std::vector<int32_t>& shape) {
+  return std::accumulate(shape.cbegin(), shape.cend(), 1,
+                         std::multiplies<int32_t>());
+}
+
+}  // namespace webnn
+}  // namespace tflite
\ No newline at end of file
diff --git a/tensorflow/lite/delegates/webnn/fully_connected_tester.h b/tensorflow/lite/delegates/webnn/fully_connected_tester.h
new file mode 100644
index 00000000000..b317a757323
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/fully_connected_tester.h
@@ -0,0 +1,151 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_FULLY_CONNECTED_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_FULLY_CONNECTED_TESTER_H_
+
+#include <cstdint>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+
+namespace tflite {
+namespace webnn {
+
+class FullyConnectedTester {
+ public:
+  FullyConnectedTester() = default;
+  FullyConnectedTester(const FullyConnectedTester&) = delete;
+  FullyConnectedTester& operator=(const FullyConnectedTester&) = delete;
+
+  inline FullyConnectedTester& InputShape(
+      std::initializer_list<int32_t> shape) {
+    for (auto it = shape.begin(); it != shape.end(); ++it) {
+      EXPECT_GT(*it, 0);
+    }
+    input_shape_ = std::vector<int32_t>(shape.begin(), shape.end());
+    input_size_ = ComputeSize(input_shape_);
+    return *this;
+  }
+
+  inline const std::vector<int32_t>& InputShape() const { return input_shape_; }
+
+  inline int32_t InputSize() const { return input_size_; }
+
+  inline FullyConnectedTester& InputChannels(int32_t input_channels) {
+    EXPECT_GT(input_channels, 0);
+    input_channels_ = input_channels;
+    return *this;
+  }
+
+  inline int32_t InputChannels() const { return input_channels_; }
+
+  inline FullyConnectedTester& OutputChannels(int32_t output_channels) {
+    EXPECT_GT(output_channels, 0);
+    output_channels_ = output_channels;
+    return *this;
+  }
+
+  inline int32_t OutputChannels() const { return output_channels_; }
+
+  std::vector<int32_t> OutputShape() const;
+
+  inline FullyConnectedTester& KeepDims(bool keep_dims) {
+    keep_dims_ = keep_dims;
+    return *this;
+  }
+
+  inline bool KeepDims() const { return keep_dims_; }
+
+  inline FullyConnectedTester& FP16Weights() {
+    fp16_weights_ = true;
+    return *this;
+  }
+
+  inline bool FP16Weights() const { return fp16_weights_; }
+
+  inline FullyConnectedTester& INT8Weights() {
+    int8_weights_ = true;
+    return *this;
+  }
+
+  inline bool INT8Weights() const { return int8_weights_; }
+
+  inline FullyConnectedTester& INT8ChannelWiseWeights() {
+    int8_channel_wise_weights_ = true;
+    return *this;
+  }
+
+  inline bool INT8ChannelWiseWeights() const {
+    return int8_channel_wise_weights_;
+  }
+
+  inline FullyConnectedTester& NoBias() {
+    has_bias_ = false;
+    return *this;
+  }
+
+  inline FullyConnectedTester& WithBias() {
+    has_bias_ = true;
+    return *this;
+  }
+
+  inline FullyConnectedTester& ReluActivation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU;
+    return *this;
+  }
+
+  inline FullyConnectedTester& Relu6Activation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU6;
+    return *this;
+  }
+
+  inline FullyConnectedTester& ReluMinus1To1Activation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU_N1_TO_1;
+    return *this;
+  }
+
+  void Test(TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel() const;
+
+  inline bool HasBias() const { return has_bias_; }
+
+  inline ::tflite::ActivationFunctionType Activation() const {
+    return activation_;
+  }
+
+  static int32_t ComputeSize(const std::vector<int32_t>& shape);
+
+  std::vector<int32_t> input_shape_;
+  int32_t input_size_ = 1;
+  int32_t input_channels_ = 1;
+  int32_t output_channels_ = 1;
+  bool keep_dims_ = false;
+  bool fp16_weights_ = false;
+  bool int8_weights_ = false;
+  bool int8_channel_wise_weights_ = false;
+  bool has_bias_ = true;
+  ::tflite::ActivationFunctionType activation_ =
+      ::tflite::ActivationFunctionType_NONE;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_FULLY_CONNECTED_TESTER_H_
diff --git a/tensorflow/lite/delegates/webnn/hard_swish_test.cc b/tensorflow/lite/delegates/webnn/hard_swish_test.cc
new file mode 100644
index 00000000000..fb8b14603ae
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/hard_swish_test.cc
@@ -0,0 +1,106 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/unary_elementwise_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(HardSwish, 4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  UnaryElementwiseTester()
+      .Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_HARD_SWISH, webnn_delegate.get());
+}
+
+TEST(HardSwish, 3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  UnaryElementwiseTester()
+      .Shape({batch, width, channels})
+      .Test(BuiltinOperator_HARD_SWISH, webnn_delegate.get());
+}
+
+TEST(HardSwish, 2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  UnaryElementwiseTester()
+      .Shape({batch, channels})
+      .Test(BuiltinOperator_HARD_SWISH, webnn_delegate.get());
+}
+
+TEST(HardSwish, 1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+
+  UnaryElementwiseTester().Shape({batch}).Test(BuiltinOperator_HARD_SWISH,
+                                               webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/logistic_test.cc b/tensorflow/lite/delegates/webnn/logistic_test.cc
new file mode 100644
index 00000000000..a456ff0af52
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/logistic_test.cc
@@ -0,0 +1,109 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/unary_elementwise_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Logistic, 4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  UnaryElementwiseTester()
+      .Shape({batch, height, width, channels})
+      .RelativeTolerance(1.0e+4f)
+      .Test(BuiltinOperator_LOGISTIC, webnn_delegate.get());
+}
+
+TEST(Logistic, 3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  UnaryElementwiseTester()
+      .Shape({batch, width, channels})
+      .RelativeTolerance(1.0e+4f)
+      .Test(BuiltinOperator_LOGISTIC, webnn_delegate.get());
+}
+
+TEST(Logistic, 2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  UnaryElementwiseTester()
+      .Shape({batch, channels})
+      .RelativeTolerance(1.0e+4f)
+      .Test(BuiltinOperator_LOGISTIC, webnn_delegate.get());
+}
+
+TEST(Logistic, 1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+
+  UnaryElementwiseTester().Shape({batch}).RelativeTolerance(1.0e+4f).Test(
+      BuiltinOperator_LOGISTIC, webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/max_pool_2d_test.cc b/tensorflow/lite/delegates/webnn/max_pool_2d_test.cc
new file mode 100644
index 00000000000..4b2b9d88506
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/max_pool_2d_test.cc
@@ -0,0 +1,415 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/pool_2d_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(MaxPool2D, UnitPoolSamePadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(1)
+      .PoolingWidth(1)
+      .StrideHeight(1)
+      .StrideWidth(1)
+      .SamePadding()
+      .Test(BuiltinOperator_MAX_POOL_2D, webnn_delegate.get());
+}
+
+TEST(MaxPool2D, UnitPoolValidPadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(1)
+      .PoolingWidth(1)
+      .StrideHeight(1)
+      .StrideWidth(1)
+      .ValidPadding()
+      .Test(BuiltinOperator_MAX_POOL_2D, webnn_delegate.get());
+}
+
+TEST(MaxPool2D, EqualPoolAndStrideWithSamePadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 7), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  const int32_t pool_height = pool_rng();
+  const int32_t pool_width = pool_rng();
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_height)
+      .PoolingWidth(pool_width)
+      .StrideHeight(pool_height)
+      .StrideWidth(pool_width)
+      .SamePadding()
+      .Test(BuiltinOperator_MAX_POOL_2D, webnn_delegate.get());
+}
+
+TEST(MaxPool2D, EqualPoolAndStrideWithValidPadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 7), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  const int32_t pool_height = pool_rng();
+  const int32_t pool_width = pool_rng();
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_height)
+      .PoolingWidth(pool_width)
+      .StrideHeight(pool_height)
+      .StrideWidth(pool_width)
+      .ValidPadding()
+      .Test(BuiltinOperator_MAX_POOL_2D, webnn_delegate.get());
+}
+
+TEST(MaxPool2D, LargePoolSmallStrideWithSamePadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(4, 7), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_rng())
+      .PoolingWidth(pool_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SamePadding()
+      .Test(BuiltinOperator_MAX_POOL_2D, webnn_delegate.get());
+}
+
+TEST(MaxPool2D, LargePoolSmallStrideWithValidPadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(4, 7), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_rng())
+      .PoolingWidth(pool_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .ValidPadding()
+      .Test(BuiltinOperator_MAX_POOL_2D, webnn_delegate.get());
+}
+
+TEST(MaxPool2D, GlobalPooling) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  const int32_t height = input_rng();
+  const int32_t width = input_rng();
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(height)
+      .InputWidth(width)
+      .Channels(channel_rng())
+      .PoolingHeight(height)
+      .PoolingWidth(width)
+      .ValidPadding()
+      .Test(BuiltinOperator_MAX_POOL_2D, webnn_delegate.get());
+}
+
+TEST(MaxPool2D, ReluActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_rng())
+      .PoolingWidth(pool_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .ReluActivation()
+      .Test(BuiltinOperator_MAX_POOL_2D, webnn_delegate.get());
+}
+
+TEST(MaxPool2D, Relu6Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_rng())
+      .PoolingWidth(pool_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .Relu6Activation()
+      .Test(BuiltinOperator_MAX_POOL_2D, webnn_delegate.get());
+}
+
+TEST(MaxPool2D, ReluMinus1To1Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_rng())
+      .PoolingWidth(pool_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .ReluMinus1To1Activation()
+      .Test(BuiltinOperator_MAX_POOL_2D, webnn_delegate.get());
+}
+
+TEST(MaxPool2D, DISABLED_TanhActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_rng())
+      .PoolingWidth(pool_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .TanhActivation()
+      .Test(BuiltinOperator_MAX_POOL_2D, webnn_delegate.get());
+}
+
+TEST(MaxPool2D, DISABLED_SignBitActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto input_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto pool_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 16), std::ref(rng));
+
+  Pool2DTester()
+      .BatchSize(batch_rng())
+      .InputHeight(input_rng())
+      .InputWidth(input_rng())
+      .Channels(channel_rng())
+      .PoolingHeight(pool_rng())
+      .PoolingWidth(pool_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SignBitActivation()
+      .Test(BuiltinOperator_MAX_POOL_2D, webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/mean_test.cc b/tensorflow/lite/delegates/webnn/mean_test.cc
new file mode 100644
index 00000000000..9b02e59658a
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/mean_test.cc
@@ -0,0 +1,521 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/reduce_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Mean, DISABLED_4DReduceBatchSqueezeDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, height, width, channels})
+      .Axes({0})
+      .KeepDims(false)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_4DReduceBatchKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, height, width, channels})
+      .Axes({0})
+      .KeepDims(true)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_4DReduceHeightSqueezeDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, height, width, channels})
+      .Axes({1})
+      .KeepDims(false)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_4DReduceHeightKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, height, width, channels})
+      .Axes({1})
+      .KeepDims(true)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_4DReduceWidthSqueezeDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, height, width, channels})
+      .Axes({2})
+      .KeepDims(false)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_4DReduceWidthKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, height, width, channels})
+      .Axes({2})
+      .KeepDims(true)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, 4DReduceHeightWidthSqueezeDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, height, width, channels})
+      .Axes({1, 2})
+      .KeepDims(false)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+
+  ReduceTester()
+      .InputShape({batch, height, width, channels})
+      .Axes({2, 1})
+      .KeepDims(false)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, 4DReduceHeightWidthKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, height, width, channels})
+      .Axes({1, 2})
+      .KeepDims(true)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+
+  ReduceTester()
+      .InputShape({batch, height, width, channels})
+      .Axes({2, 1})
+      .KeepDims(true)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_4DReduceChannelsSqueezeDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, height, width, channels})
+      .Axes({3})
+      .KeepDims(false)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_4DReduceChannelsKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, height, width, channels})
+      .Axes({3})
+      .KeepDims(true)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_3DReduceBatchSqueezeDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, width, channels})
+      .Axes({0})
+      .KeepDims(false)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_3DReduceBatchKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, width, channels})
+      .Axes({0})
+      .KeepDims(true)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_3DReduceWidthSqueezeDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, width, channels})
+      .Axes({1})
+      .KeepDims(false)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_3DReduceWidthKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, width, channels})
+      .Axes({1})
+      .KeepDims(true)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_3DReduceChannelsSqueezeDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, width, channels})
+      .Axes({2})
+      .KeepDims(false)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_3DReduceChannelsKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, width, channels})
+      .Axes({2})
+      .KeepDims(true)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_2DReduceBatchSqueezeDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, channels})
+      .Axes({0})
+      .KeepDims(false)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_2DReduceBatchKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, channels})
+      .Axes({0})
+      .KeepDims(true)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_2DReduceChannelsSqueezeDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, channels})
+      .Axes({1})
+      .KeepDims(false)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_2DReduceChannelsKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  ReduceTester()
+      .InputShape({batch, channels})
+      .Axes({1})
+      .KeepDims(true)
+      .Test(BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_1DSqueezeDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+
+  ReduceTester().InputShape({batch}).Axes({0}).KeepDims(false).Test(
+      BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+TEST(Mean, DISABLED_1DKeepDims) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+
+  ReduceTester().InputShape({batch}).Axes({0}).KeepDims(true).Test(
+      BuiltinOperator_MEAN, webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/mul_test.cc b/tensorflow/lite/delegates/webnn/mul_test.cc
new file mode 100644
index 00000000000..b0fb851c10e
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/mul_test.cc
@@ -0,0 +1,913 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <gtest/gtest.h>
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include "tensorflow/lite/delegates/webnn/binary_elementwise_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Mul, 4DBy4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy4DBroadcastChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, 1, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, 1, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy4DBroadcastWidth) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, width, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, width, 1})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy4DBroadcastHeight) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, 1, 1})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy4DBroadcastBatch) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, 1, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, 1, 1, 1})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy4DBroadcastHeightWidthChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 2DBy2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({batch, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 2DBy1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 2DBy0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic4DBroadcastChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, 1, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, 1, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic4DBroadcastWidth) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, width, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, width, 1})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic4DBroadcastHeight) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, 1, 1})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic4DBroadcastBatch) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, 1, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, 1, 1, 1})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic4DBroadcastHeightWidthChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({height, width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 2DByStatic2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({batch, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({batch, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 2DByStatic1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 2DByStatic0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, FP16Weights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .FP16Weights()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input2Static(true)
+      .FP16Weights()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, SparseWeights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .SparseWeights()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input2Static(true)
+      .SparseWeights()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, ReluActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .ReluActivation()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, Relu6Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Relu6Activation()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, ReluMinus1To1Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .ReluMinus1To1Activation()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, DISABLED_TanhActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .TanhActivation()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, DISABLED_SignBitActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .SignBitActivation()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/pad_test.cc b/tensorflow/lite/delegates/webnn/pad_test.cc
new file mode 100644
index 00000000000..5e742ffbd35
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/pad_test.cc
@@ -0,0 +1,281 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/pad_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Pad, Full4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto pad_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 3), std::ref(rng));
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  PadTester()
+      .InputPrePaddings({pad_rng(), pad_rng(), pad_rng(), pad_rng()})
+      .InputPostPaddings({pad_rng(), pad_rng(), pad_rng(), pad_rng()})
+      .InputShape({shape_rng(), shape_rng(), shape_rng(), shape_rng()})
+      .Test(webnn_delegate.get());
+}
+
+TEST(Pad, Batch4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto pad_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 3), std::ref(rng));
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  PadTester()
+      .InputPrePaddings({pad_rng(), 0, 0, 0})
+      .InputPostPaddings({pad_rng(), 0, 0, 0})
+      .InputShape({shape_rng(), shape_rng(), shape_rng(), shape_rng()})
+      .Test(webnn_delegate.get());
+}
+
+TEST(Pad, HeightAndWidth4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto pad_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 3), std::ref(rng));
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  PadTester()
+      .InputPrePaddings({0, pad_rng(), pad_rng(), 0})
+      .InputPostPaddings({0, pad_rng(), pad_rng(), 0})
+      .InputShape({shape_rng(), shape_rng(), shape_rng(), shape_rng()})
+      .Test(webnn_delegate.get());
+}
+
+TEST(Pad, Channels4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto pad_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 3), std::ref(rng));
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  PadTester()
+      .InputPrePaddings({0, 0, 0, pad_rng()})
+      .InputPostPaddings({0, 0, 0, pad_rng()})
+      .InputShape({shape_rng(), shape_rng(), shape_rng(), shape_rng()})
+      .Test(webnn_delegate.get());
+}
+
+TEST(Pad, Full3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto pad_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 3), std::ref(rng));
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  PadTester()
+      .InputPrePaddings({pad_rng(), pad_rng(), pad_rng()})
+      .InputPostPaddings({pad_rng(), pad_rng(), pad_rng()})
+      .InputShape({shape_rng(), shape_rng(), shape_rng()})
+      .Test(webnn_delegate.get());
+}
+
+TEST(Pad, Batch3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto pad_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 3), std::ref(rng));
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  PadTester()
+      .InputPrePaddings({pad_rng(), 0, 0})
+      .InputPostPaddings({pad_rng(), 0, 0})
+      .InputShape({shape_rng(), shape_rng(), shape_rng()})
+      .Test(webnn_delegate.get());
+}
+
+TEST(Pad, Width3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto pad_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 3), std::ref(rng));
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  PadTester()
+      .InputPrePaddings({0, pad_rng(), 0})
+      .InputPostPaddings({0, pad_rng(), 0})
+      .InputShape({shape_rng(), shape_rng(), shape_rng()})
+      .Test(webnn_delegate.get());
+}
+
+TEST(Pad, Channels3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto pad_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 3), std::ref(rng));
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  PadTester()
+      .InputPrePaddings({0, 0, pad_rng()})
+      .InputPostPaddings({0, 0, pad_rng()})
+      .InputShape({shape_rng(), shape_rng(), shape_rng()})
+      .Test(webnn_delegate.get());
+}
+
+TEST(Pad, Full2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto pad_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 3), std::ref(rng));
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  PadTester()
+      .InputPrePaddings({pad_rng(), pad_rng()})
+      .InputPostPaddings({pad_rng(), pad_rng()})
+      .InputShape({shape_rng(), shape_rng()})
+      .Test(webnn_delegate.get());
+}
+
+TEST(Pad, Batch2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto pad_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 3), std::ref(rng));
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  PadTester()
+      .InputPrePaddings({pad_rng(), 0})
+      .InputPostPaddings({pad_rng(), 0})
+      .InputShape({shape_rng(), shape_rng()})
+      .Test(webnn_delegate.get());
+}
+
+TEST(Pad, Channels2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto pad_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 3), std::ref(rng));
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  PadTester()
+      .InputPrePaddings({0, pad_rng()})
+      .InputPostPaddings({0, pad_rng()})
+      .InputShape({shape_rng(), shape_rng()})
+      .Test(webnn_delegate.get());
+}
+
+TEST(Pad, 1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto pad_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 3), std::ref(rng));
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  PadTester()
+      .InputPrePaddings({pad_rng(), pad_rng()})
+      .InputPostPaddings({pad_rng(), pad_rng()})
+      .InputShape({shape_rng(), shape_rng()})
+      .Test(webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/pad_tester.cc b/tensorflow/lite/delegates/webnn/pad_tester.cc
new file mode 100644
index 00000000000..0a353fd0d14
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/pad_tester.cc
@@ -0,0 +1,191 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/pad_tester.h"
+
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <numeric>
+#include <random>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+std::vector<int32_t> PadTester::OutputShape() const {
+  std::vector<int32_t> output_shape;
+  output_shape.reserve(InputShape().size());
+  for (size_t i = 0; i < InputShape().size(); i++) {
+    int32_t output_dim = InputShape()[i];
+    if (i < InputPrePaddings().size()) {
+      output_dim += InputPrePaddings()[i];
+    }
+    if (i < InputPostPaddings().size()) {
+      output_dim += InputPostPaddings()[i];
+    }
+    output_shape.push_back(output_dim);
+  }
+  return output_shape;
+}
+
+void PadTester::Test(TfLiteDelegate* delegate) const {
+  ASSERT_EQ(InputPrePaddings().size(), InputPostPaddings().size());
+  ASSERT_LE(InputPrePaddings().size(), InputShape().size());
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto input_rng =
+      std::bind(std::uniform_real_distribution<float>(), std::ref(rng));
+
+  std::vector<char> buffer = CreateTfLiteModel();
+  const Model* model = GetModel(buffer.data());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+
+  ASSERT_EQ(delegate_interpreter->inputs().size(), 1);
+  ASSERT_EQ(default_interpreter->inputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->outputs().size(), 1);
+  ASSERT_EQ(default_interpreter->outputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  float* default_input_data = default_interpreter->typed_input_tensor<float>(0);
+  std::generate(default_input_data,
+                default_input_data + ComputeSize(InputShape()),
+                std::ref(input_rng));
+
+  float* delegate_input_data =
+      delegate_interpreter->typed_input_tensor<float>(0);
+  std::copy(default_input_data, default_input_data + ComputeSize(InputShape()),
+            delegate_input_data);
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float* default_output_data =
+      default_interpreter->typed_output_tensor<float>(0);
+  float* delegate_output_data =
+      delegate_interpreter->typed_output_tensor<float>(0);
+
+  for (size_t i = 0; i < ComputeSize(OutputShape()); i++) {
+    ASSERT_EQ(default_output_data[i], delegate_output_data[i]);
+  }
+}
+
+std::vector<char> PadTester::CreateTfLiteModel() const {
+  flatbuffers::FlatBufferBuilder builder;
+  flatbuffers::Offset<OperatorCode> operator_code =
+      CreateOperatorCode(builder, BuiltinOperator_PAD);
+
+  std::vector<int32_t> paddings(InputPrePaddings().size() +
+                                InputPostPaddings().size());
+  for (size_t i = 0; i < InputPrePaddings().size(); i++) {
+    paddings[i * 2] = InputPrePaddings()[i];
+    paddings[i * 2 + 1] = InputPostPaddings()[i];
+  }
+  const std::array<flatbuffers::Offset<Buffer>, 2> buffers{{
+      CreateBuffer(builder, builder.CreateVector({})),
+      CreateBuffer(builder,
+                   builder.CreateVector(
+                       reinterpret_cast<const uint8_t*>(paddings.data()),
+                       sizeof(int32_t) * paddings.size())),
+  }};
+
+  const std::vector<int32_t> output_shape = OutputShape();
+  const std::array<int32_t, 2> paddings_shape{
+      {static_cast<int32_t>(InputPrePaddings().size()), 2}};
+  const std::array<flatbuffers::Offset<Tensor>, 3> tensors{{
+      CreateTensor(builder,
+                   builder.CreateVector<int32_t>(InputShape().data(),
+                                                 InputShape().size()),
+                   TensorType_FLOAT32),
+      CreateTensor(builder,
+                   builder.CreateVector<int32_t>(paddings_shape.data(),
+                                                 paddings_shape.size()),
+                   TensorType_INT32, /*buffer=*/1),
+      CreateTensor(builder,
+                   builder.CreateVector<int32_t>(output_shape.data(),
+                                                 output_shape.size()),
+                   TensorType_FLOAT32),
+  }};
+
+  const std::array<int32_t, 2> op_inputs{{0, 1}};
+  const std::array<int32_t, 1> op_outputs{{2}};
+  flatbuffers::Offset<Operator> op = CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()));
+
+  const std::array<int32_t, 1> subgraph_inputs{{0}};
+  const std::array<int32_t, 1> subgraph_outputs{{2}};
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(&op, 1));
+
+  flatbuffers::Offset<flatbuffers::String> description =
+      builder.CreateString("Pad model");
+
+  flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION, builder.CreateVector(&operator_code, 1),
+      builder.CreateVector(&subgraph, 1), description,
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+int32_t PadTester::ComputeSize(const std::vector<int32_t>& shape) {
+  return std::accumulate(shape.cbegin(), shape.cend(), 1,
+                         std::multiplies<int32_t>());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/pad_tester.h b/tensorflow/lite/delegates/webnn/pad_tester.h
new file mode 100644
index 00000000000..435fd7db580
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/pad_tester.h
@@ -0,0 +1,87 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_PAD_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_PAD_TESTER_H_
+
+#include <cstdint>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+
+namespace tflite {
+namespace webnn {
+
+class PadTester {
+ public:
+  PadTester() = default;
+  PadTester(const PadTester&) = delete;
+  PadTester& operator=(const PadTester&) = delete;
+
+  inline PadTester& InputShape(std::initializer_list<int32_t> shape) {
+    for (auto it = shape.begin(); it != shape.end(); ++it) {
+      EXPECT_GT(*it, 0);
+    }
+    input_shape_ = std::vector<int32_t>(shape.begin(), shape.end());
+    return *this;
+  }
+
+  inline const std::vector<int32_t>& InputShape() const { return input_shape_; }
+
+  inline PadTester& InputPrePaddings(std::initializer_list<int32_t> paddings) {
+    for (auto it = paddings.begin(); it != paddings.end(); ++it) {
+      EXPECT_GE(*it, 0);
+    }
+    input_pre_paddings_ =
+        std::vector<int32_t>(paddings.begin(), paddings.end());
+    return *this;
+  }
+
+  inline const std::vector<int32_t> InputPrePaddings() const {
+    return input_pre_paddings_;
+  }
+
+  inline PadTester& InputPostPaddings(std::initializer_list<int32_t> paddings) {
+    for (auto it = paddings.begin(); it != paddings.end(); ++it) {
+      EXPECT_GE(*it, 0);
+    }
+    input_post_paddings_ =
+        std::vector<int32_t>(paddings.begin(), paddings.end());
+    return *this;
+  }
+
+  inline const std::vector<int32_t> InputPostPaddings() const {
+    return input_post_paddings_;
+  }
+
+  std::vector<int32_t> OutputShape() const;
+
+  void Test(TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel() const;
+
+  static int32_t ComputeSize(const std::vector<int32_t>& shape);
+
+  std::vector<int32_t> input_shape_;
+  std::vector<int32_t> input_pre_paddings_;
+  std::vector<int32_t> input_post_paddings_;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_PAD_TESTER_H_
diff --git a/tensorflow/lite/delegates/webnn/pool_2d_tester.cc b/tensorflow/lite/delegates/webnn/pool_2d_tester.cc
new file mode 100644
index 00000000000..c233573be8e
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/pool_2d_tester.cc
@@ -0,0 +1,200 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/pool_2d_tester.h"
+
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <random>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+void Pool2DTester::Test(tflite::BuiltinOperator pool_op,
+                        TfLiteDelegate* delegate) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto range_rng = std::bind(
+      std::uniform_real_distribution<float>(-25.0f, 25.0f), std::ref(rng));
+
+  std::vector<char> buffer = CreateTfLiteModel(pool_op);
+  const tflite::Model* model = tflite::GetModel(buffer.data());
+
+  std::unique_ptr<tflite::Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      tflite::InterpreterBuilder(
+          model,
+          tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<tflite::Interpreter> default_interpreter;
+  ASSERT_EQ(
+      tflite::InterpreterBuilder(
+          model,
+          tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+
+  ASSERT_EQ(delegate_interpreter->inputs().size(), 1);
+  ASSERT_EQ(default_interpreter->inputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->outputs().size(), 1);
+  ASSERT_EQ(default_interpreter->outputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  float* default_input_data = default_interpreter->typed_input_tensor<float>(0);
+  for (int32_t i = 0; i < BatchSize(); i++) {
+    for (int32_t c = 0; c < Channels(); c++) {
+      // Use the same range of all-positive or all-negative values to generate
+      // all pixels within the same batch index & channel, but different ranges
+      // for different channels or batches. This ensures that no catastrophic
+      // cancellation occur, but test covers both positive and negative inputs.
+      const float range = range_rng();
+      auto value_rng =
+          std::bind(std::uniform_real_distribution<float>(
+                        std::min(range, 0.0f), std::max(range, 0.0f)),
+                    std::ref(rng));
+      for (int32_t y = 0; y < InputHeight(); y++) {
+        for (int32_t x = 0; x < InputWidth(); x++) {
+          const int32_t index =
+              ((i * InputHeight() + y) * InputWidth() + x) * Channels() + c;
+          default_input_data[index] = value_rng();
+        }
+      }
+    }
+  }
+
+  float* webnn_input_data =
+      delegate_interpreter->typed_input_tensor<float>(0);
+  std::copy(default_input_data,
+            default_input_data +
+                BatchSize() * InputHeight() * InputWidth() * Channels(),
+            webnn_input_data);
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float* default_output_data =
+      default_interpreter->typed_output_tensor<float>(0);
+  float* webnn_output_data =
+      delegate_interpreter->typed_output_tensor<float>(0);
+
+  for (int32_t i = 0; i < BatchSize(); i++) {
+    for (int32_t y = 0; y < OutputHeight(); y++) {
+      for (int32_t x = 0; x < OutputWidth(); x++) {
+        for (int32_t c = 0; c < Channels(); c++) {
+          const int32_t index =
+              ((i * OutputHeight() + y) * OutputWidth() + x) * Channels() + c;
+          if (pool_op == BuiltinOperator_MAX_POOL_2D) {
+            // MaxPooling results must be exact
+            ASSERT_EQ(default_output_data[index], webnn_output_data[index])
+                << "batch " << i << " / " << BatchSize() << ", y position " << y
+                << " / " << OutputHeight() << ", x position " << x << " / "
+                << OutputWidth() << ", channel " << c << " / " << Channels();
+          } else {
+            ASSERT_NEAR(default_output_data[index], webnn_output_data[index],
+                        std::abs(default_output_data[index]) * 3.0e-6f)
+                << "batch " << i << " / " << BatchSize() << ", y position " << y
+                << " / " << OutputHeight() << ", x position " << x << " / "
+                << OutputWidth() << ", channel " << c << " / " << Channels();
+          }
+        }
+      }
+    }
+  }
+}
+
+std::vector<char> Pool2DTester::CreateTfLiteModel(
+    tflite::BuiltinOperator pool_op) const {
+  flatbuffers::FlatBufferBuilder builder;
+  flatbuffers::Offset<tflite::OperatorCode> operator_code =
+      CreateOperatorCode(builder, pool_op, 0);
+
+  flatbuffers::Offset<tflite::Pool2DOptions> pool_2d_options =
+      CreatePool2DOptions(builder, Padding(), StrideWidth(), StrideHeight(),
+                          PoolingWidth(), PoolingHeight(), Activation());
+
+  const flatbuffers::Offset<tflite::Buffer> null_buffer =
+      tflite::CreateBuffer(builder, builder.CreateVector({}));
+
+  const std::array<int32_t, 4> input_shape{
+      {BatchSize(), InputHeight(), InputWidth(), Channels()}};
+  const std::array<int32_t, 4> output_shape{
+      {BatchSize(), OutputHeight(), OutputWidth(), Channels()}};
+
+  const std::array<flatbuffers::Offset<tflite::Tensor>, 2> tensors{{
+      tflite::CreateTensor(
+          builder,
+          builder.CreateVector<int32_t>(input_shape.data(), input_shape.size()),
+          tflite::TensorType_FLOAT32),
+      tflite::CreateTensor(builder,
+                           builder.CreateVector<int32_t>(output_shape.data(),
+                                                         output_shape.size()),
+                           tflite::TensorType_FLOAT32),
+  }};
+
+  const std::array<int32_t, 1> op_inputs{{0}};
+  const std::array<int32_t, 1> op_outputs{{1}};
+
+  flatbuffers::Offset<tflite::Operator> op = CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()),
+      tflite::BuiltinOptions_Pool2DOptions, pool_2d_options.Union());
+
+  const std::array<int32_t, 1> subgraph_inputs{{0}};
+  const std::array<int32_t, 1> subgraph_outputs{{1}};
+  flatbuffers::Offset<tflite::SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(&op, 1));
+
+  flatbuffers::Offset<flatbuffers::String> description =
+      builder.CreateString("Pool2D model");
+
+  flatbuffers::Offset<tflite::Model> model_buffer = tflite::CreateModel(
+      builder, TFLITE_SCHEMA_VERSION, builder.CreateVector(&operator_code, 1),
+      builder.CreateVector(&subgraph, 1), description,
+      builder.CreateVector(&null_buffer, 1));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/pool_2d_tester.h b/tensorflow/lite/delegates/webnn/pool_2d_tester.h
new file mode 100644
index 00000000000..826dfbc9a62
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/pool_2d_tester.h
@@ -0,0 +1,177 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_POOL_2D_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_POOL_2D_TESTER_H_
+
+#include <cstdint>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+
+namespace tflite {
+namespace webnn {
+
+class Pool2DTester {
+ public:
+  Pool2DTester() = default;
+  Pool2DTester(const Pool2DTester&) = delete;
+  Pool2DTester& operator=(const Pool2DTester&) = delete;
+
+  inline Pool2DTester& BatchSize(int32_t batch_size) {
+    EXPECT_GT(batch_size, 0);
+    batch_size_ = batch_size;
+    return *this;
+  }
+
+  inline int32_t BatchSize() const { return batch_size_; }
+
+  inline Pool2DTester& Channels(int32_t channels) {
+    EXPECT_GT(channels, 0);
+    channels_ = channels;
+    return *this;
+  }
+
+  inline int32_t Channels() const { return channels_; }
+
+  inline Pool2DTester& InputHeight(int32_t input_height) {
+    EXPECT_GT(input_height, 0);
+    input_height_ = input_height;
+    return *this;
+  }
+
+  inline int32_t InputHeight() const { return input_height_; }
+
+  inline Pool2DTester& InputWidth(int32_t input_width) {
+    EXPECT_GT(input_width, 0);
+    input_width_ = input_width;
+    return *this;
+  }
+
+  inline int32_t InputWidth() const { return input_width_; }
+
+  inline int32_t OutputWidth() const {
+    if (Padding() == ::tflite::Padding_SAME) {
+      return (InputWidth() - 1) / StrideWidth() + 1;
+    } else {
+      return (InputWidth() - PoolingWidth()) / StrideWidth() + 1;
+    }
+  }
+
+  inline int32_t OutputHeight() const {
+    if (Padding() == ::tflite::Padding_SAME) {
+      return (InputHeight() - 1) / StrideHeight() + 1;
+    } else {
+      return (InputHeight() - PoolingHeight()) / StrideHeight() + 1;
+    }
+  }
+
+  inline Pool2DTester& PoolingHeight(int32_t pooling_height) {
+    EXPECT_GT(pooling_height, 0);
+    pooling_height_ = pooling_height;
+    return *this;
+  }
+
+  inline int32_t PoolingHeight() const { return pooling_height_; }
+
+  inline Pool2DTester& PoolingWidth(int32_t pooling_width) {
+    EXPECT_GT(pooling_width, 0);
+    pooling_width_ = pooling_width;
+    return *this;
+  }
+
+  inline int32_t PoolingWidth() const { return pooling_width_; }
+
+  inline Pool2DTester& StrideHeight(int32_t stride_height) {
+    EXPECT_GT(stride_height, 0);
+    stride_height_ = stride_height;
+    return *this;
+  }
+
+  inline int32_t StrideHeight() const { return stride_height_; }
+
+  inline Pool2DTester& StrideWidth(int32_t stride_width) {
+    EXPECT_GT(stride_width, 0);
+    stride_width_ = stride_width;
+    return *this;
+  }
+
+  inline int32_t StrideWidth() const { return stride_width_; }
+
+  inline Pool2DTester& SamePadding() {
+    padding_ = ::tflite::Padding_SAME;
+    return *this;
+  }
+
+  inline Pool2DTester& ValidPadding() {
+    padding_ = ::tflite::Padding_VALID;
+    return *this;
+  }
+
+  inline Pool2DTester& ReluActivation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU;
+    return *this;
+  }
+
+  inline Pool2DTester& Relu6Activation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU6;
+    return *this;
+  }
+
+  inline Pool2DTester& ReluMinus1To1Activation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU_N1_TO_1;
+    return *this;
+  }
+
+  inline Pool2DTester& TanhActivation() {
+    activation_ = ::tflite::ActivationFunctionType_TANH;
+    return *this;
+  }
+
+  inline Pool2DTester& SignBitActivation() {
+    activation_ = ::tflite::ActivationFunctionType_SIGN_BIT;
+    return *this;
+  }
+
+  void Test(tflite::BuiltinOperator pool_op, TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel(tflite::BuiltinOperator pool_op) const;
+
+  inline ::tflite::Padding Padding() const { return padding_; }
+
+  inline ::tflite::ActivationFunctionType Activation() const {
+    return activation_;
+  }
+
+  int32_t batch_size_ = 1;
+  int32_t channels_ = 1;
+  int32_t input_height_ = 1;
+  int32_t input_width_ = 1;
+  int32_t pooling_height_ = 1;
+  int32_t pooling_width_ = 1;
+  int32_t stride_height_ = 1;
+  int32_t stride_width_ = 1;
+  ::tflite::Padding padding_ = ::tflite::Padding_VALID;
+  ::tflite::ActivationFunctionType activation_ =
+      ::tflite::ActivationFunctionType_NONE;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_POOL_2D_TESTER_H_
diff --git a/tensorflow/lite/delegates/webnn/reduce_tester.cc b/tensorflow/lite/delegates/webnn/reduce_tester.cc
new file mode 100644
index 00000000000..642cfbf26ee
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/reduce_tester.cc
@@ -0,0 +1,175 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/reduce_tester.h"
+
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <numeric>
+#include <random>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+void ReduceTester::Test(tflite::BuiltinOperator reduce_op,
+                        TfLiteDelegate* delegate) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto input_rng =
+      std::bind(std::uniform_real_distribution<float>(), std::ref(rng));
+
+  std::vector<char> buffer = CreateTfLiteModel(reduce_op);
+  const Model* model = GetModel(buffer.data());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+
+  ASSERT_EQ(delegate_interpreter->inputs().size(), 1);
+  ASSERT_EQ(default_interpreter->inputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->outputs().size(), 1);
+  ASSERT_EQ(default_interpreter->outputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  float* default_input_data = default_interpreter->typed_input_tensor<float>(0);
+  std::generate(default_input_data, default_input_data + InputSize(),
+                std::ref(input_rng));
+
+  float* delegate_input_data =
+      delegate_interpreter->typed_input_tensor<float>(0);
+  std::copy(default_input_data, default_input_data + InputSize(),
+            delegate_input_data);
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float* default_output_data =
+      default_interpreter->typed_output_tensor<float>(0);
+  float* delegate_output_data =
+      delegate_interpreter->typed_output_tensor<float>(0);
+
+  const int32_t output_size = OutputSize();
+  for (size_t i = 0; i < output_size; i++) {
+    ASSERT_NEAR(
+        default_output_data[i], delegate_output_data[i],
+        std::numeric_limits<float>::epsilon() *
+            std::max(std::abs(default_output_data[i]) * RelativeTolerance(),
+                     1.0f));
+  }
+}
+
+std::vector<char> ReduceTester::CreateTfLiteModel(
+    tflite::BuiltinOperator reduce_op) const {
+  flatbuffers::FlatBufferBuilder builder;
+  flatbuffers::Offset<OperatorCode> operator_code =
+      CreateOperatorCode(builder, reduce_op);
+
+  const std::array<flatbuffers::Offset<Buffer>, 2> buffers{{
+      CreateBuffer(builder, builder.CreateVector({})),
+      CreateBuffer(builder, builder.CreateVector(
+                                reinterpret_cast<const uint8_t*>(Axes().data()),
+                                sizeof(int32_t) * Axes().size())),
+  }};
+
+  const std::vector<int32_t> output_shape = OutputShape();
+  const std::array<int32_t, 1> axes_shape{
+      {static_cast<int32_t>(Axes().size())}};
+  const std::array<flatbuffers::Offset<Tensor>, 3> tensors{{
+      CreateTensor(builder,
+                   builder.CreateVector<int32_t>(InputShape().data(),
+                                                 InputShape().size()),
+                   TensorType_FLOAT32),
+      CreateTensor(
+          builder,
+          builder.CreateVector<int32_t>(axes_shape.data(), axes_shape.size()),
+          TensorType_INT32, /*buffer=*/1),
+      CreateTensor(builder,
+                   builder.CreateVector<int32_t>(output_shape.data(),
+                                                 output_shape.size()),
+                   TensorType_FLOAT32),
+  }};
+
+  const flatbuffers::Offset<ReducerOptions> reducer_options =
+      CreateReducerOptions(builder, KeepDims());
+
+  const std::array<int32_t, 2> op_inputs{{0, 1}};
+  const std::array<int32_t, 1> op_outputs{{2}};
+  flatbuffers::Offset<Operator> op = CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()),
+      tflite::BuiltinOptions_ReducerOptions, reducer_options.Union());
+
+  const std::array<int32_t, 1> subgraph_inputs{{0}};
+  const std::array<int32_t, 1> subgraph_outputs{{2}};
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(&op, 1));
+
+  flatbuffers::Offset<flatbuffers::String> description =
+      builder.CreateString("Reduce model");
+
+  flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION, builder.CreateVector(&operator_code, 1),
+      builder.CreateVector(&subgraph, 1), description,
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+int32_t ReduceTester::ComputeSize(const std::vector<int32_t>& shape) {
+  return std::accumulate(shape.cbegin(), shape.cend(), 1,
+                         std::multiplies<int32_t>());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/reduce_tester.h b/tensorflow/lite/delegates/webnn/reduce_tester.h
new file mode 100644
index 00000000000..15f7114f8ab
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/reduce_tester.h
@@ -0,0 +1,117 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_REDUCE_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_REDUCE_TESTER_H_
+
+#include <cstdint>
+#include <unordered_set>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+
+namespace tflite {
+namespace webnn {
+
+class ReduceTester {
+ public:
+  ReduceTester() = default;
+  ReduceTester(const ReduceTester&) = delete;
+  ReduceTester& operator=(const ReduceTester&) = delete;
+
+  inline ReduceTester& InputShape(std::initializer_list<int32_t> shape) {
+    for (auto it = shape.begin(); it != shape.end(); ++it) {
+      EXPECT_GT(*it, 0);
+    }
+    input_shape_ = std::vector<int32_t>(shape.begin(), shape.end());
+    input_size_ = ReduceTester::ComputeSize(input_shape_);
+    return *this;
+  }
+
+  inline const std::vector<int32_t>& InputShape() const { return input_shape_; }
+
+  inline int32_t InputSize() const { return input_size_; }
+
+  inline ReduceTester& Axes(std::initializer_list<int32_t> axes) {
+    for (auto it = axes.begin(); it != axes.end(); ++it) {
+      EXPECT_GE(*it, 0);
+    }
+    axes_ = std::vector<int32_t>(axes.begin(), axes.end());
+    return *this;
+  }
+
+  inline const std::vector<int32_t>& Axes() const { return axes_; }
+
+  inline ReduceTester& KeepDims(bool keep_dims) {
+    keep_dims_ = keep_dims;
+    return *this;
+  }
+
+  inline bool KeepDims() const { return keep_dims_; }
+
+  inline std::vector<int32_t> OutputShape() const {
+    std::vector<int32_t> output_shape;
+    output_shape.reserve(InputShape().size());
+    std::unordered_set<int32_t> axes_set(Axes().cbegin(), Axes().cend());
+    for (int32_t i = 0; i < InputShape().size(); i++) {
+      if (axes_set.count(i) != 0) {
+        if (KeepDims()) {
+          output_shape.push_back(1);
+        }
+      } else {
+        output_shape.push_back(InputShape()[i]);
+      }
+    }
+    return output_shape;
+  }
+
+  inline int32_t OutputSize() const {
+    int32_t output_size = 1;
+    std::unordered_set<int32_t> axes_set(Axes().cbegin(), Axes().cend());
+    for (int32_t i = 0; i < InputShape().size(); i++) {
+      if (axes_set.count(i) == 0) {
+        output_size *= InputShape()[i];
+      }
+    }
+    return output_size;
+  }
+
+  inline ReduceTester& RelativeTolerance(float relative_tolerance) {
+    relative_tolerance_ = relative_tolerance;
+    return *this;
+  }
+
+  inline float RelativeTolerance() const { return relative_tolerance_; }
+
+  void Test(tflite::BuiltinOperator reduce_op, TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel(tflite::BuiltinOperator reduce_op) const;
+
+  static int32_t ComputeSize(const std::vector<int32_t>& shape);
+
+  std::vector<int32_t> input_shape_;
+  std::vector<int32_t> axes_;
+  int32_t input_size_;
+  bool keep_dims_ = true;
+  float relative_tolerance_ = 10.0f;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_REDUCE_TESTER_H_
diff --git a/tensorflow/lite/delegates/webnn/relu_test.cc b/tensorflow/lite/delegates/webnn/relu_test.cc
new file mode 100644
index 00000000000..203b61a8f17
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/relu_test.cc
@@ -0,0 +1,106 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/unary_elementwise_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Relu, 4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  UnaryElementwiseTester()
+      .Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_RELU, webnn_delegate.get());
+}
+
+TEST(Relu, 3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  UnaryElementwiseTester()
+      .Shape({batch, width, channels})
+      .Test(BuiltinOperator_RELU, webnn_delegate.get());
+}
+
+TEST(Relu, 2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  UnaryElementwiseTester()
+      .Shape({batch, channels})
+      .Test(BuiltinOperator_RELU, webnn_delegate.get());
+}
+
+TEST(Relu, 1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+
+  UnaryElementwiseTester().Shape({batch}).Test(BuiltinOperator_RELU,
+                                               webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/reshape_test.cc b/tensorflow/lite/delegates/webnn/reshape_test.cc
new file mode 100644
index 00000000000..c6dc6383228
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/reshape_test.cc
@@ -0,0 +1,219 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <algorithm>
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/reshape_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Reshape, 4DShapeAsInput) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  const std::vector<int32_t> input_shape{
+      {shape_rng(), shape_rng(), shape_rng(), shape_rng()}};
+  std::vector<int32_t> output_shape(input_shape.cbegin(), input_shape.cend());
+  std::shuffle(output_shape.begin(), output_shape.end(), rng);
+
+  ReshapeTester()
+      .InputShape(input_shape)
+      .OutputShape(output_shape)
+      .OutputShapeAsInput(true)
+      .Test(webnn_delegate.get());
+}
+
+TEST(Reshape, 4DShapeAsParam) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  const std::vector<int32_t> input_shape{
+      {shape_rng(), shape_rng(), shape_rng(), shape_rng()}};
+  std::vector<int32_t> output_shape(input_shape.cbegin(), input_shape.cend());
+  std::shuffle(output_shape.begin(), output_shape.end(), rng);
+
+  ReshapeTester()
+      .InputShape(input_shape)
+      .OutputShape(output_shape)
+      .OutputShapeAsInput(false)
+      .Test(webnn_delegate.get());
+}
+
+TEST(Reshape, 3DShapeAsInput) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  const std::vector<int32_t> input_shape{
+      {shape_rng(), shape_rng(), shape_rng()}};
+  std::vector<int32_t> output_shape(input_shape.cbegin(), input_shape.cend());
+  std::shuffle(output_shape.begin(), output_shape.end(), rng);
+
+  ReshapeTester()
+      .InputShape(input_shape)
+      .OutputShape(output_shape)
+      .OutputShapeAsInput(true)
+      .Test(webnn_delegate.get());
+}
+
+TEST(Reshape, 3DShapeAsParam) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  const std::vector<int32_t> input_shape{
+      {shape_rng(), shape_rng(), shape_rng()}};
+  std::vector<int32_t> output_shape(input_shape.cbegin(), input_shape.cend());
+  std::shuffle(output_shape.begin(), output_shape.end(), rng);
+
+  ReshapeTester()
+      .InputShape(input_shape)
+      .OutputShape(output_shape)
+      .OutputShapeAsInput(false)
+      .Test(webnn_delegate.get());
+}
+
+TEST(Reshape, 2DShapeAsInput) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  const std::vector<int32_t> input_shape{{shape_rng(), shape_rng()}};
+  std::vector<int32_t> output_shape(input_shape.cbegin(), input_shape.cend());
+  std::shuffle(output_shape.begin(), output_shape.end(), rng);
+
+  ReshapeTester()
+      .InputShape(input_shape)
+      .OutputShape(output_shape)
+      .OutputShapeAsInput(true)
+      .Test(webnn_delegate.get());
+}
+
+TEST(Reshape, 2DShapeAsParam) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  const std::vector<int32_t> input_shape{{shape_rng(), shape_rng()}};
+  std::vector<int32_t> output_shape(input_shape.cbegin(), input_shape.cend());
+  std::shuffle(output_shape.begin(), output_shape.end(), rng);
+
+  ReshapeTester()
+      .InputShape(input_shape)
+      .OutputShape(output_shape)
+      .OutputShapeAsInput(false)
+      .Test(webnn_delegate.get());
+}
+
+TEST(Reshape, 1DShapeAsInput) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  const std::vector<int32_t> shape({shape_rng()});
+
+  ReshapeTester()
+      .InputShape(shape)
+      .OutputShape(shape)
+      .OutputShapeAsInput(true)
+      .Test(webnn_delegate.get());
+}
+
+TEST(Reshape, 1DShapeAsParam) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  const std::vector<int32_t> shape({shape_rng()});
+
+  ReshapeTester()
+      .InputShape(shape)
+      .OutputShape(shape)
+      .OutputShapeAsInput(false)
+      .Test(webnn_delegate.get());
+}
+
+TEST(Reshape, 0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  ReshapeTester()
+      .InputShape(std::vector<int32_t>())
+      .OutputShape(std::vector<int32_t>())
+      .Test(webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/reshape_tester.cc b/tensorflow/lite/delegates/webnn/reshape_tester.cc
new file mode 100644
index 00000000000..e37a65e5516
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/reshape_tester.cc
@@ -0,0 +1,183 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/reshape_tester.h"
+
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <numeric>
+#include <random>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+#include "tensorflow/lite/minimal_logging.h"
+namespace tflite {
+namespace webnn {
+
+void ReshapeTester::Test(TfLiteDelegate* delegate) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto f32rng =
+      std::bind(std::uniform_real_distribution<float>(), std::ref(rng));
+
+  ASSERT_EQ(InputSize(), OutputSize());
+  std::vector<char> buffer = CreateTfLiteModel();
+  const Model* model = GetModel(buffer.data());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+
+  ASSERT_EQ(delegate_interpreter->inputs().size(), 1);
+  ASSERT_EQ(default_interpreter->inputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->outputs().size(), 1);
+  ASSERT_EQ(default_interpreter->outputs().size(), 1);
+    
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  float* default_input_data = default_interpreter->typed_input_tensor<float>(0);
+  std::generate(default_input_data, default_input_data + InputSize(),
+                std::ref(f32rng));
+
+  float* delegate_input_data =
+      delegate_interpreter->typed_input_tensor<float>(0);
+  std::copy(default_input_data, default_input_data + InputSize(),
+            delegate_input_data);
+  
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float* default_output_data =
+      default_interpreter->typed_output_tensor<float>(0);
+  float* delegate_output_data =
+      delegate_interpreter->typed_output_tensor<float>(0);
+
+  for (size_t i = 0; i < OutputSize(); i++) {
+    ASSERT_EQ(delegate_output_data[i], default_output_data[i]);
+  }
+}
+
+std::vector<char> ReshapeTester::CreateTfLiteModel() const {
+  flatbuffers::FlatBufferBuilder builder;
+  flatbuffers::Offset<OperatorCode> operator_code =
+      CreateOperatorCode(builder, BuiltinOperator_RESHAPE, 0);
+
+  std::vector<flatbuffers::Offset<Buffer>> buffers{{
+      CreateBuffer(builder, builder.CreateVector({})),
+  }};
+  if (OutputShapeAsInput()) {
+    buffers.emplace_back(CreateBuffer(
+        builder, builder.CreateVector(
+                     reinterpret_cast<const uint8_t*>(OutputShape().data()),
+                     OutputShape().size() * sizeof(int32_t))));
+  }
+
+  std::vector<flatbuffers::Offset<Tensor>> tensors{{
+      CreateTensor(builder,
+                   builder.CreateVector<int32_t>(InputShape().data(),
+                                                 InputShape().size()),
+                   TensorType_FLOAT32),
+      CreateTensor(builder,
+                   builder.CreateVector<int32_t>(OutputShape().data(),
+                                                 OutputShape().size()),
+                   TensorType_FLOAT32),
+  }};
+
+  if (OutputShapeAsInput()) {
+    const std::array<int32_t, 1> reshape_shape{
+        {static_cast<int32_t>(InputShape().size())}};
+    tensors.insert(tensors.begin() + 1,
+                   CreateTensor(builder,
+                                builder.CreateVector<int32_t>(
+                                    reshape_shape.data(), reshape_shape.size()),
+                                TensorType_INT32, /*buffer=*/1));
+  }
+
+  std::vector<int32_t> op_inputs({0});
+  if (OutputShapeAsInput()) {
+    op_inputs.push_back(1);
+  }
+  const std::array<int32_t, 1> op_outputs{{OutputShapeAsInput() ? 2 : 1}};
+
+  BuiltinOptions builtin_options_type = tflite::BuiltinOptions_NONE;
+  flatbuffers::Offset<void> builtin_options = 0;
+  if (!OutputShapeAsInput()) {
+    builtin_options_type = tflite::BuiltinOptions_ReshapeOptions;
+    builtin_options =
+        CreateReshapeOptions(
+            builder, builder.CreateVector<int32_t>(OutputShape().data(),
+                                                   OutputShape().size()))
+            .Union();
+  }
+
+  const flatbuffers::Offset<Operator> op = CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()),
+      builtin_options_type, builtin_options);
+
+  const std::array<int32_t, 1> subgraph_inputs{{op_inputs.front()}};
+  const std::array<int32_t, 1> subgraph_outputs{{op_outputs.front()}};
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(&op, 1));
+
+  const flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION, builder.CreateVector(&operator_code, 1),
+      builder.CreateVector(&subgraph, 1), builder.CreateString("Reshape model"),
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+int32_t ReshapeTester::ComputeSize(const std::vector<int32_t>& shape) {
+  return std::accumulate(shape.cbegin(), shape.cend(), 1,
+                         std::multiplies<int32_t>());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/reshape_tester.h b/tensorflow/lite/delegates/webnn/reshape_tester.h
new file mode 100644
index 00000000000..778f47d7e0b
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/reshape_tester.h
@@ -0,0 +1,87 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_RESHAPE_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_RESHAPE_TESTER_H_
+
+#include <cstdint>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+
+namespace tflite {
+namespace webnn {
+
+class ReshapeTester {
+ public:
+  ReshapeTester() = default;
+  ReshapeTester(const ReshapeTester&) = delete;
+  ReshapeTester& operator=(const ReshapeTester&) = delete;
+
+  inline ReshapeTester& InputShape(const std::vector<int32_t>& input_shape) {
+    for (int32_t input_dim : input_shape) {
+      EXPECT_GT(input_dim, 0);
+    }
+    input_shape_ = std::vector<int32_t>(input_shape.begin(), input_shape.end());
+    input_size_ = ReshapeTester::ComputeSize(input_shape);
+    return *this;
+  }
+
+  inline const std::vector<int32_t>& InputShape() const { return input_shape_; }
+
+  inline ReshapeTester& OutputShape(const std::vector<int32_t>& output_shape) {
+    for (int32_t output_dim : output_shape) {
+      EXPECT_GT(output_dim, 0);
+    }
+    output_shape_ =
+        std::vector<int32_t>(output_shape.begin(), output_shape.end());
+    output_size_ = ReshapeTester::ComputeSize(output_shape);
+    return *this;
+  }
+
+  inline const std::vector<int32_t>& OutputShape() const {
+    return output_shape_;
+  }
+
+  inline int32_t InputSize() const { return input_size_; }
+
+  inline int32_t OutputSize() const { return output_size_; }
+
+  inline ReshapeTester& OutputShapeAsInput(bool shape_as_input) {
+    shape_as_input_ = shape_as_input;
+    return *this;
+  }
+
+  inline bool OutputShapeAsInput() const { return shape_as_input_; }
+
+  void Test(TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel() const;
+
+  static int32_t ComputeSize(const std::vector<int32_t>& shape);
+
+  std::vector<int32_t> input_shape_;
+  std::vector<int32_t> output_shape_;
+  int32_t input_size_ = 1;
+  int32_t output_size_ = 1;
+  bool shape_as_input_ = false;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_RESHAPE_TESTER_H_
diff --git a/tensorflow/lite/delegates/webnn/resize_bilinear_test.cc b/tensorflow/lite/delegates/webnn/resize_bilinear_test.cc
new file mode 100644
index 00000000000..d2e52849616
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/resize_bilinear_test.cc
@@ -0,0 +1,103 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <algorithm>
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/resize_bilinear_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(ResizeBilinear, AlignCenters) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto size_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 16), std::ref(rng));
+
+  ResizeBilinearTester()
+      .HalfPixelCenters(true)
+      .InputHeight(size_rng())
+      .InputWidth(size_rng())
+      .OutputHeight(size_rng())
+      .OutputWidth(size_rng())
+      .Channels(channel_rng())
+      .Test(webnn_delegate.get());
+}
+
+// Webnn does not support this option. Corresponding issue:https://github.com/webmachinelearning/webnn/issues/270
+TEST(ResizeBilinear, DISABLED_AlignCentersTF1X) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto size_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 16), std::ref(rng));
+
+  ResizeBilinearTester()
+      .InputHeight(size_rng())
+      .InputWidth(size_rng())
+      .OutputHeight(size_rng())
+      .OutputWidth(size_rng())
+      .Channels(channel_rng())
+      .Test(webnn_delegate.get());
+}
+
+// Webnn does not support this option. Corresponding issue:https://github.com/webmachinelearning/webnn/issues/270
+TEST(ResizeBilinear, DISABLED_AlignCorners) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto size_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 16), std::ref(rng));
+
+  ResizeBilinearTester()
+      .AlignCorners(true)
+      .InputHeight(size_rng())
+      .InputWidth(size_rng())
+      .OutputHeight(size_rng())
+      .OutputWidth(size_rng())
+      .Channels(channel_rng())
+      .Test(webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/resize_bilinear_tester.cc b/tensorflow/lite/delegates/webnn/resize_bilinear_tester.cc
new file mode 100644
index 00000000000..22d05983dbc
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/resize_bilinear_tester.cc
@@ -0,0 +1,187 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/resize_bilinear_tester.h"
+
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <numeric>
+#include <random>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+void ResizeBilinearTester::Test(TfLiteDelegate* delegate) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto input_rng =
+      std::bind(std::uniform_real_distribution<float>(), std::ref(rng));
+
+  std::vector<char> buffer = CreateTfLiteModel();
+  const Model* model = GetModel(buffer.data());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+
+  ASSERT_EQ(delegate_interpreter->inputs().size(), 1);
+  ASSERT_EQ(default_interpreter->inputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->outputs().size(), 1);
+  ASSERT_EQ(default_interpreter->outputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  float* default_input_data = default_interpreter->typed_input_tensor<float>(0);
+  std::generate(default_input_data,
+                default_input_data +
+                    BatchSize() * InputHeight() * InputWidth() * Channels(),
+                std::ref(input_rng));
+
+  float* delegate_input_data =
+      delegate_interpreter->typed_input_tensor<float>(0);
+  std::copy(default_input_data,
+            default_input_data +
+                BatchSize() * InputHeight() * InputWidth() * Channels(),
+            delegate_input_data);
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float* default_output_data =
+      default_interpreter->typed_output_tensor<float>(0);
+  float* delegate_output_data =
+      delegate_interpreter->typed_output_tensor<float>(0);
+
+  for (int i = 0; i < BatchSize(); i++) {
+    for (int y = 0; y < OutputHeight(); y++) {
+      for (int x = 0; x < OutputWidth(); x++) {
+        for (int c = 0; c < Channels(); c++) {
+          const int index =
+              ((i * OutputHeight() + y) * OutputWidth() + x) * Channels() + c;
+          ASSERT_NEAR(default_output_data[index], delegate_output_data[index],
+                      std::max(std::abs(default_output_data[index]) * 1.0e-4f,
+                               10.0f * std::numeric_limits<float>::epsilon()))
+              << "batch " << i << " / " << BatchSize() << ", y position " << y
+              << " / " << OutputHeight() << ", x position " << x << " / "
+              << OutputWidth() << ", channel " << c << " / " << Channels();
+        }
+      }
+    }
+  }
+}
+
+std::vector<char> ResizeBilinearTester::CreateTfLiteModel() const {
+  flatbuffers::FlatBufferBuilder builder;
+  flatbuffers::Offset<OperatorCode> operator_code =
+      CreateOperatorCode(builder, BuiltinOperator_RESIZE_BILINEAR);
+
+  flatbuffers::Offset<tflite::ResizeBilinearOptions> resize_bilinear_options =
+      CreateResizeBilinearOptions(builder, AlignCorners(), HalfPixelCenters());
+
+  const std::array<int32_t, 2> size_data{{OutputHeight(), OutputWidth()}};
+
+  const std::array<flatbuffers::Offset<Buffer>, 2> buffers{{
+      CreateBuffer(builder, builder.CreateVector({})),
+      CreateBuffer(builder,
+                   builder.CreateVector(
+                       reinterpret_cast<const uint8_t*>(size_data.data()),
+                       size_data.size() * sizeof(int32_t))),
+  }};
+
+  const std::array<int32_t, 4> input_shape{
+      {BatchSize(), InputHeight(), InputWidth(), Channels()}};
+  const std::array<int32_t, 4> output_shape{
+      {BatchSize(), OutputHeight(), OutputWidth(), Channels()}};
+  const std::array<int32_t, 1> size_shape{
+      {static_cast<int32_t>(size_data.size())}};
+
+  const std::array<flatbuffers::Offset<Tensor>, 3> tensors{{
+      CreateTensor(
+          builder,
+          builder.CreateVector<int32_t>(input_shape.data(), input_shape.size()),
+          TensorType_FLOAT32),
+      CreateTensor(
+          builder,
+          builder.CreateVector<int32_t>(size_shape.data(), size_shape.size()),
+          TensorType_INT32, /*buffer=*/1),
+      CreateTensor(builder,
+                   builder.CreateVector<int32_t>(output_shape.data(),
+                                                 output_shape.size()),
+                   TensorType_FLOAT32),
+  }};
+
+  const std::array<int32_t, 2> op_inputs{{0, 1}};
+  const std::array<int32_t, 1> op_outputs{{2}};
+  flatbuffers::Offset<Operator> op = CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()),
+      BuiltinOptions_ResizeBilinearOptions, resize_bilinear_options.Union());
+
+  const std::array<int32_t, 1> subgraph_inputs{{0}};
+  const std::array<int32_t, 1> subgraph_outputs{{2}};
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(&op, 1));
+
+  flatbuffers::Offset<flatbuffers::String> description =
+      builder.CreateString("Resize Bilinear model");
+
+  flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION, builder.CreateVector(&operator_code, 1),
+      builder.CreateVector(&subgraph, 1), description,
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/resize_bilinear_tester.h b/tensorflow/lite/delegates/webnn/resize_bilinear_tester.h
new file mode 100644
index 00000000000..bd37faae226
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/resize_bilinear_tester.h
@@ -0,0 +1,115 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_XNNPACK_RESIZE_BILINEAR_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_XNNPACK_RESIZE_BILINEAR_TESTER_H_
+
+#include <cstdint>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+
+namespace tflite {
+namespace webnn {
+
+class ResizeBilinearTester {
+ public:
+  ResizeBilinearTester() = default;
+  ResizeBilinearTester(const ResizeBilinearTester&) = delete;
+  ResizeBilinearTester& operator=(const ResizeBilinearTester&) = delete;
+
+  inline ResizeBilinearTester& BatchSize(int32_t batch_size) {
+    EXPECT_GT(batch_size, 0);
+    batch_size_ = batch_size;
+    return *this;
+  }
+
+  inline int32_t BatchSize() const { return batch_size_; }
+
+  inline ResizeBilinearTester& Channels(int32_t channels) {
+    EXPECT_GT(channels, 0);
+    channels_ = channels;
+    return *this;
+  }
+
+  inline int32_t Channels() const { return channels_; }
+
+  inline ResizeBilinearTester& InputHeight(int32_t input_height) {
+    EXPECT_GT(input_height, 0);
+    input_height_ = input_height;
+    return *this;
+  }
+
+  inline int32_t InputHeight() const { return input_height_; }
+
+  inline ResizeBilinearTester& InputWidth(int32_t input_width) {
+    EXPECT_GT(input_width, 0);
+    input_width_ = input_width;
+    return *this;
+  }
+
+  inline int32_t InputWidth() const { return input_width_; }
+
+  inline ResizeBilinearTester& OutputHeight(int32_t output_height) {
+    EXPECT_GT(output_height, 0);
+    output_height_ = output_height;
+    return *this;
+  }
+
+  inline int32_t OutputHeight() const { return output_height_; }
+
+  inline ResizeBilinearTester& OutputWidth(int32_t output_width) {
+    EXPECT_GT(output_width, 0);
+    output_width_ = output_width;
+    return *this;
+  }
+
+  inline int32_t OutputWidth() const { return output_width_; }
+
+  ResizeBilinearTester& AlignCorners(bool align_corners) {
+    align_corners_ = align_corners;
+    return *this;
+  }
+
+  bool AlignCorners() const { return align_corners_; }
+
+  ResizeBilinearTester& HalfPixelCenters(bool half_pixel_centers) {
+    half_pixel_centers_ = half_pixel_centers;
+    return *this;
+  }
+
+  bool HalfPixelCenters() const { return half_pixel_centers_; }
+
+  void Test(TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel() const;
+
+  int32_t batch_size_ = 1;
+  int32_t channels_ = 1;
+  int32_t input_height_ = 1;
+  int32_t input_width_ = 1;
+  int32_t output_height_ = 1;
+  int32_t output_width_ = 1;
+  bool align_corners_ = false;
+  bool half_pixel_centers_ = false;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_XNNPACK_RESIZE_BILINEAR_TESTER_H_
diff --git a/tensorflow/lite/delegates/webnn/softmax_test.cc b/tensorflow/lite/delegates/webnn/softmax_test.cc
new file mode 100644
index 00000000000..d469336be22
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/softmax_test.cc
@@ -0,0 +1,46 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/softmax_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Softmax, 2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  SoftmaxTester().Shape({batch, channels}).Test(webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/softmax_tester.cc b/tensorflow/lite/delegates/webnn/softmax_tester.cc
new file mode 100644
index 00000000000..c3088671c0e
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/softmax_tester.cc
@@ -0,0 +1,171 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/softmax_tester.h"
+
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <numeric>
+#include <random>
+#include <vector>
+#include <string>
+
+#include <gtest/gtest.h>
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+void SoftmaxTester::Test(TfLiteDelegate* delegate) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto input_rng = std::bind(
+      std::uniform_real_distribution<float>(-15.0f, 15.0f), std::ref(rng));
+
+  std::vector<char> buffer = CreateTfLiteModel();
+  const Model* model = GetModel(buffer.data());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+
+  ASSERT_EQ(delegate_interpreter->inputs().size(), 1);
+  ASSERT_EQ(default_interpreter->inputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->outputs().size(), 1);
+  ASSERT_EQ(default_interpreter->outputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  float* default_input_data = default_interpreter->typed_input_tensor<float>(0);
+  std::generate(default_input_data, default_input_data + Size(),
+                std::ref(input_rng));
+
+  float* delegate_input_data =
+      delegate_interpreter->typed_input_tensor<float>(0);
+  std::copy(default_input_data, default_input_data + Size(),
+            delegate_input_data);
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float* default_output_data =
+      default_interpreter->typed_output_tensor<float>(0);
+  float* delegate_output_data =
+      delegate_interpreter->typed_output_tensor<float>(0);
+
+  std::string input_shape_string = "Input shape:\n";
+  for(auto i : Shape()){
+      input_shape_string += std::to_string(i)+" ";
+  }
+  std::string input_data_string = "\nInput data:\n";
+  for (size_t i = 0; i < Size(); i++)
+  {
+      input_data_string+= std::to_string(default_input_data[i])+" ";
+  }
+  
+  for (size_t i = 0; i < Size(); i++) {
+    ASSERT_NEAR(default_output_data[i], delegate_output_data[i],
+                std::numeric_limits<float>::epsilon() *
+                    std::max(std::abs(default_output_data[i]) * 10.0f, 1.0f))<<input_shape_string<<input_data_string;
+  }
+}
+
+std::vector<char> SoftmaxTester::CreateTfLiteModel() const {
+  flatbuffers::FlatBufferBuilder builder;
+  flatbuffers::Offset<OperatorCode> operator_code =
+      CreateOperatorCode(builder, BuiltinOperator_SOFTMAX);
+
+  const std::array<flatbuffers::Offset<Buffer>, 1> buffers{{
+      CreateBuffer(builder, builder.CreateVector({})),
+  }};
+
+  const std::array<flatbuffers::Offset<Tensor>, 2> tensors{{
+      CreateTensor(
+          builder,
+          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),
+          TensorType_FLOAT32),
+      CreateTensor(
+          builder,
+          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),
+          TensorType_FLOAT32),
+  }};
+
+  flatbuffers::Offset<SoftmaxOptions> softmax_options =
+      CreateSoftmaxOptions(builder, Beta());
+
+  const std::array<int32_t, 1> op_inputs{{0}};
+  const std::array<int32_t, 1> op_outputs{{1}};
+  flatbuffers::Offset<Operator> op = CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()),
+      BuiltinOptions_SoftmaxOptions, softmax_options.Union());
+
+  const std::array<int32_t, 1> subgraph_inputs{{0}};
+  const std::array<int32_t, 1> subgraph_outputs{{1}};
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(&op, 1));
+
+  flatbuffers::Offset<flatbuffers::String> description =
+      builder.CreateString("Softmax model");
+
+  flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION, builder.CreateVector(&operator_code, 1),
+      builder.CreateVector(&subgraph, 1), description,
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+int32_t SoftmaxTester::ComputeSize(const std::vector<int32_t>& shape) {
+  return std::accumulate(shape.cbegin(), shape.cend(), 1,
+                         std::multiplies<int32_t>());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/softmax_tester.h b/tensorflow/lite/delegates/webnn/softmax_tester.h
new file mode 100644
index 00000000000..e2bbe17fd1e
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/softmax_tester.h
@@ -0,0 +1,71 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_XNNPACK_SOFTMAX_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_XNNPACK_SOFTMAX_TESTER_H_
+
+#include <cstdint>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+
+namespace tflite {
+namespace webnn {
+
+class SoftmaxTester {
+ public:
+  SoftmaxTester() = default;
+  SoftmaxTester(const SoftmaxTester&) = delete;
+  SoftmaxTester& operator=(const SoftmaxTester&) = delete;
+
+  inline SoftmaxTester& Shape(std::initializer_list<int32_t> shape) {
+    EXPECT_GT(shape.size(), 0);
+    for (auto it = shape.begin(); it != shape.end(); ++it) {
+      EXPECT_GT(*it, 0);
+    }
+    shape_ = std::vector<int32_t>(shape.begin(), shape.end());
+    size_ = SoftmaxTester::ComputeSize(shape_);
+    return *this;
+  }
+
+  const std::vector<int32_t>& Shape() const { return shape_; }
+
+  int32_t Size() const { return size_; }
+
+  inline SoftmaxTester& Beta(float beta) {
+    beta_ = beta;
+    return *this;
+  }
+
+  float Beta() const { return beta_; }
+
+  void Test(TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel() const;
+
+  static int32_t ComputeSize(const std::vector<int32_t>& shape);
+
+  std::vector<int32_t> shape_;
+  int32_t size_;
+  float beta_ = 1.0f;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_XNNPACK_SOFTMAX_TESTER_H_
diff --git a/tensorflow/lite/delegates/webnn/split_test.cc b/tensorflow/lite/delegates/webnn/split_test.cc
new file mode 100644
index 00000000000..38162f0bce2
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/split_test.cc
@@ -0,0 +1,354 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <algorithm>
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/split_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Split, 1D_to_2_outputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+  const std::vector<int32_t> shape({shape_rng() * 2});
+
+  for (int i = -1; i < 1; i++) {
+    // clang-format off
+    SplitTester()
+        .InputShape(shape)
+        .SplitDimension(i)
+        .NumSplits(2)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Split, 2D_to_2_outputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  auto split_dim_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+
+  for (int i = -2; i < 2; i++) {
+    std::vector<int32_t> shape({shape_rng(), shape_rng()});
+    shape[i < 0 ? i + shape.size() : i] = split_dim_rng() * 2;
+
+    // clang-format off
+    SplitTester()
+        .InputShape(shape)
+        .SplitDimension(i)
+        .NumSplits(2)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Split, 3D_to_2_outputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  auto split_dim_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+
+  for (int i = -3; i < 3; i++) {
+    std::vector<int32_t> shape({shape_rng(), shape_rng(), shape_rng()});
+    shape[i < 0 ? i + shape.size() : i] = split_dim_rng() * 2;
+
+    // clang-format off
+    SplitTester()
+        .InputShape(shape)
+        .SplitDimension(i)
+        .NumSplits(2)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Split, 4D_to_2_outputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  auto split_dim_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+
+  for (int i = -4; i < 4; i++) {
+    std::vector<int32_t> shape(
+        {shape_rng(), shape_rng(), shape_rng(), shape_rng()});
+    shape[i < 0 ? i + shape.size() : i] = split_dim_rng() * 2;
+
+    // clang-format off
+    SplitTester()
+        .InputShape(shape)
+        .SplitDimension(i)
+        .NumSplits(2)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Split, 1D_to_3_outputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+  const std::vector<int32_t> shape({shape_rng() * 3});
+
+  for (int i = -1; i < 1; i++) {
+    // clang-format off
+    SplitTester()
+        .InputShape(shape)
+        .SplitDimension(i)
+        .NumSplits(3)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Split, 2D_to_3_outputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  auto split_dim_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+
+  for (int i = -2; i < 2; i++) {
+    std::vector<int32_t> shape({shape_rng(), shape_rng()});
+    shape[i < 0 ? i + shape.size() : i] = split_dim_rng() * 3;
+
+    // clang-format off
+    SplitTester()
+        .InputShape(shape)
+        .SplitDimension(i)
+        .NumSplits(3)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Split, 3D_to_3_outputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  auto split_dim_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+
+  for (int i = -3; i < 3; i++) {
+    std::vector<int32_t> shape({shape_rng(), shape_rng(), shape_rng()});
+    shape[i < 0 ? i + shape.size() : i] = split_dim_rng() * 3;
+
+    // clang-format off
+    SplitTester()
+        .InputShape(shape)
+        .SplitDimension(i)
+        .NumSplits(3)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Split, 4D_to_3_outputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  auto split_dim_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+
+  for (int i = -4; i < 4; i++) {
+    std::vector<int32_t> shape(
+        {shape_rng(), shape_rng(), shape_rng(), shape_rng()});
+    shape[i < 0 ? i + shape.size() : i] = split_dim_rng() * 3;
+
+    // clang-format off
+    SplitTester()
+        .InputShape(shape)
+        .SplitDimension(i)
+        .NumSplits(3)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Split, 1D_to_4_outputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+  const std::vector<int32_t> shape({shape_rng() * 4});
+
+  for (int i = -1; i < 1; i++) {
+    // clang-format off
+    SplitTester()
+        .InputShape(shape)
+        .SplitDimension(i)
+        .NumSplits(4)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Split, 2D_to_4_outputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  auto split_dim_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+
+  for (int i = -2; i < 2; i++) {
+    std::vector<int32_t> shape({shape_rng(), shape_rng()});
+    shape[i < 0 ? i + shape.size() : i] = split_dim_rng() * 4;
+
+    // clang-format off
+    SplitTester()
+        .InputShape(shape)
+        .SplitDimension(i)
+        .NumSplits(4)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Split, 3D_to_4_outputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  auto split_dim_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+
+  for (int i = -3; i < 3; i++) {
+    std::vector<int32_t> shape({shape_rng(), shape_rng(), shape_rng()});
+    shape[i < 0 ? i + shape.size() : i] = split_dim_rng() * 4;
+
+    // clang-format off
+    SplitTester()
+        .InputShape(shape)
+        .SplitDimension(i)
+        .NumSplits(4)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+TEST(Split, 4D_to_4_outputs) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 10), std::ref(rng));
+  auto split_dim_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+
+  for (int i = -4; i < 4; i++) {
+    std::vector<int32_t> shape(
+        {shape_rng(), shape_rng(), shape_rng(), shape_rng()});
+    shape[i < 0 ? i + shape.size() : i] = split_dim_rng() * 4;
+
+    // clang-format off
+    SplitTester()
+        .InputShape(shape)
+        .SplitDimension(i)
+        .NumSplits(4)
+        .Test(TensorType_FLOAT32, webnn_delegate.get());
+    // clang-format on
+  }
+}
+
+}  // namespace webnn
+}  // namespace tflite
\ No newline at end of file
diff --git a/tensorflow/lite/delegates/webnn/split_tester.cc b/tensorflow/lite/delegates/webnn/split_tester.cc
new file mode 100644
index 00000000000..5fab10965f7
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/split_tester.cc
@@ -0,0 +1,239 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/split_tester.h"
+
+#include <algorithm>
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <numeric>
+#include <random>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+template <class T>
+void SplitTester::Test(Interpreter *delegate_interpreter,
+                       Interpreter *default_interpreter) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  std::uniform_int_distribution<int32_t> input_distribution(
+      std::numeric_limits<T>::min(), std::numeric_limits<T>::max());
+  auto input_rng = std::bind(input_distribution, std::ref(rng));
+
+  T *default_input_data = default_interpreter->typed_input_tensor<T>(1);
+  std::generate(default_input_data,
+                default_input_data + ComputeSize(InputShape()),
+                std::ref(input_rng));
+
+  T *webnn_input_data = delegate_interpreter->typed_input_tensor<T>(1);
+  std::copy(default_input_data, default_input_data + ComputeSize(InputShape()),
+            webnn_input_data);
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  T *default_output1_data = default_interpreter->typed_output_tensor<T>(0);
+  T *webnn_output1_data = delegate_interpreter->typed_output_tensor<T>(0);
+  T *default_output2_data = default_interpreter->typed_output_tensor<T>(1);
+  T *webnn_output2_data = delegate_interpreter->typed_output_tensor<T>(1);
+
+  for (size_t i = 0; i < ComputeSize(OutputShape()); i++) {
+    ASSERT_EQ(static_cast<int32_t>(default_output1_data[i]),
+              static_cast<int32_t>(webnn_output1_data[i]));
+    ASSERT_EQ(static_cast<int32_t>(default_output2_data[i]),
+              static_cast<int32_t>(webnn_output2_data[i]));
+  }
+}
+
+template <>
+void SplitTester::Test<float>(Interpreter *delegate_interpreter,
+                              Interpreter *default_interpreter) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  std::uniform_real_distribution<float> input_distribution(-25.0f, 25.0f);
+  auto input_rng = std::bind(input_distribution, std::ref(rng));
+
+  float *default_input_data = default_interpreter->typed_input_tensor<float>(1);
+  std::generate(default_input_data,
+                default_input_data + ComputeSize(InputShape()),
+                std::ref(input_rng));
+
+  float *webnn_input_data =
+      delegate_interpreter->typed_input_tensor<float>(1);
+  std::copy(default_input_data, default_input_data + ComputeSize(InputShape()),
+            webnn_input_data);
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float *default_output1_data =
+      default_interpreter->typed_output_tensor<float>(0);
+  float *webnn_output1_data =
+      delegate_interpreter->typed_output_tensor<float>(0);
+  float *default_output2_data =
+      default_interpreter->typed_output_tensor<float>(0);
+  float *webnn_output2_data =
+      delegate_interpreter->typed_output_tensor<float>(0);
+
+  for (size_t i = 0; i < ComputeSize(OutputShape()); i++) {
+    ASSERT_EQ(default_output1_data[i], webnn_output1_data[i]);
+    ASSERT_EQ(default_output2_data[i], webnn_output2_data[i]);
+  }
+}
+
+void SplitTester::Test(TensorType tensor_type, TfLiteDelegate *delegate) const {
+  std::vector<char> buffer = CreateTfLiteModel(tensor_type);
+  const Model *model = GetModel(buffer.data());
+
+  int32_t axis = SplitDimension();
+  axis += axis < 0 ? InputShape().size() : 0;
+  ASSERT_EQ(0, InputShape()[axis] % NumSplits());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+  ASSERT_EQ(delegate_interpreter->inputs().size(), 2);
+  ASSERT_EQ(default_interpreter->inputs().size(), 2);
+  ASSERT_EQ(delegate_interpreter->outputs().size(), NumSplits());
+  ASSERT_EQ(default_interpreter->outputs().size(), NumSplits());
+
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  switch (tensor_type) {
+    case TensorType_FLOAT32:
+      Test<float>(delegate_interpreter.get(), default_interpreter.get());
+      break;
+    case TensorType_INT8:
+      Test<int8_t>(delegate_interpreter.get(), default_interpreter.get());
+      break;
+    case TensorType_UINT8:
+      Test<uint8_t>(delegate_interpreter.get(), default_interpreter.get());
+      break;
+    default:
+      GTEST_FAIL();
+  }
+}
+
+std::vector<char> SplitTester::CreateTfLiteModel(TensorType tensor_type) const {
+  flatbuffers::FlatBufferBuilder builder;
+  flatbuffers::Offset<OperatorCode> operator_code =
+      CreateOperatorCode(builder, BuiltinOperator_SPLIT, 0);
+
+  std::array<int32_t, 1> split_dim = {SplitDimension()};
+  std::vector<flatbuffers::Offset<Buffer>> buffers{
+      {CreateBuffer(builder, builder.CreateVector({})),
+       CreateBuffer(builder,
+                    builder.CreateVector(
+                        reinterpret_cast<const uint8_t *>(split_dim.data()),
+                        split_dim.size() * sizeof(int32_t)))}};
+  std::array<int32_t, 0> split_dim_shape = {};
+
+  flatbuffers::Offset<QuantizationParameters> quantization_params =
+      CreateQuantizationParameters(
+          builder, /*min=*/0, /*max=*/0,
+          builder.CreateVector<float>({/*scale=*/1.0f}),
+          builder.CreateVector<int64_t>({/*zero_point=*/0}));
+
+  std::vector<flatbuffers::Offset<Tensor>> tensors{{
+      CreateTensor(builder,
+                   builder.CreateVector<int32_t>(split_dim_shape.data(),
+                                                 split_dim_shape.size()),
+                   TensorType_INT32, /*buffer=*/1, /*name=*/0,
+                   quantization_params),
+      CreateTensor(builder,
+                   builder.CreateVector<int32_t>(InputShape().data(),
+                                                 InputShape().size()),
+                   tensor_type,
+                   /*buffer=*/0, /*name=*/0, quantization_params),
+  }};
+
+  for (int i = 0; i < NumSplits(); i++) {
+    tensors.push_back(
+        CreateTensor(builder,
+                     builder.CreateVector<int32_t>(OutputShape().data(),
+                                                   OutputShape().size()),
+                     tensor_type,
+                     /*buffer=*/0, /*name=*/0, quantization_params));
+  }
+
+  const std::array<int32_t, 2> op_inputs{0, 1};
+  std::vector<int32_t> op_outputs;
+  op_outputs.reserve(NumSplits());
+  for (int i = 0; i < NumSplits(); i++) {
+    op_outputs.push_back(op_inputs.size() + i);
+  }
+  EXPECT_EQ(op_outputs.size(), NumSplits());
+
+  const flatbuffers::Offset<Operator> op = CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()),
+      tflite::BuiltinOptions_SplitOptions,
+      CreateSplitOptions(builder, NumSplits()).Union());
+
+  const std::array<int32_t, 2> subgraph_inputs = op_inputs;
+  const std::vector<int32_t> subgraph_outputs = op_outputs;
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(&op, 1));
+
+  const flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION, builder.CreateVector(&operator_code, 1),
+      builder.CreateVector(&subgraph, 1), builder.CreateString("Split model"),
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+int32_t SplitTester::ComputeSize(const std::vector<int32_t> &shape) {
+  return std::accumulate(shape.cbegin(), shape.cend(), 1,
+                         std::multiplies<int32_t>());
+}
+
+}  // namespace webnn
+}  // namespace tflite
\ No newline at end of file
diff --git a/tensorflow/lite/delegates/webnn/split_tester.h b/tensorflow/lite/delegates/webnn/split_tester.h
new file mode 100644
index 00000000000..1816009d79f
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/split_tester.h
@@ -0,0 +1,85 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_SPLIT_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_SPLIT_TESTER_H_
+
+#include <cstdint>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+
+namespace tflite {
+namespace webnn {
+
+class SplitTester {
+ public:
+  SplitTester() = default;
+  SplitTester(const SplitTester&) = delete;
+  SplitTester& operator=(const SplitTester&) = delete;
+
+  inline SplitTester& SplitDimension(int32_t split_dim) {
+    split_dim_ = split_dim;
+    return *this;
+  }
+
+  inline SplitTester& InputShape(const std::vector<int32_t>& shape) {
+    for (auto it = shape.begin(); it != shape.end(); ++it) {
+      EXPECT_GT(*it, 0);
+    }
+    input_shape_ = std::vector<int32_t>(shape.begin(), shape.end());
+    return *this;
+  }
+
+  int32_t SplitDimension() const { return split_dim_; }
+
+  inline SplitTester& NumSplits(int num_splits) {
+    num_splits_ = num_splits;
+    return *this;
+  }
+
+  inline const int NumSplits() const { return num_splits_; }
+
+  inline const std::vector<int32_t>& InputShape() const { return input_shape_; }
+
+  std::vector<int32_t> OutputShape() const {
+    std::vector<int32_t> output_shape = InputShape();
+    int32_t split_dim = SplitDimension();
+    split_dim += split_dim < 0 ? InputShape().size() : 0;
+    EXPECT_LE(0, split_dim);
+    EXPECT_EQ(0, output_shape[split_dim] % NumSplits());
+    output_shape[split_dim] /= NumSplits();
+    return output_shape;
+  }
+
+  template <typename T>
+  void Test(Interpreter* delegate_interpreter,
+            Interpreter* default_interpreter) const;
+  void Test(TensorType tensor_type, TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel(TensorType tensor_type) const;
+
+  static int32_t ComputeSize(const std::vector<int32_t>& shape);
+
+  std::vector<int32_t> input_shape_;
+  int32_t split_dim_;
+  int num_splits_;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_SPLIT_TESTER_H_
\ No newline at end of file
diff --git a/tensorflow/lite/delegates/webnn/tanh_test.cc b/tensorflow/lite/delegates/webnn/tanh_test.cc
new file mode 100644
index 00000000000..1a6d30b72f7
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/tanh_test.cc
@@ -0,0 +1,109 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/unary_elementwise_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Logistic, 4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  UnaryElementwiseTester()
+      .Shape({batch, height, width, channels})
+      .RelativeTolerance(1.0e+4f)
+      .Test(BuiltinOperator_TANH, webnn_delegate.get());
+}
+
+TEST(Logistic, 3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  UnaryElementwiseTester()
+      .Shape({batch, width, channels})
+      .RelativeTolerance(1.0e+4f)
+      .Test(BuiltinOperator_TANH, webnn_delegate.get());
+}
+
+TEST(Logistic, 2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  UnaryElementwiseTester()
+      .Shape({batch, channels})
+      .RelativeTolerance(1.0e+4f)
+      .Test(BuiltinOperator_TANH, webnn_delegate.get());
+}
+
+TEST(Logistic, 1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+
+  UnaryElementwiseTester().Shape({batch}).RelativeTolerance(1.0e+4f).Test(
+      BuiltinOperator_TANH, webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/transpose_conv_test.cc b/tensorflow/lite/delegates/webnn/transpose_conv_test.cc
new file mode 100644
index 00000000000..112558341aa
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/transpose_conv_test.cc
@@ -0,0 +1,651 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/delegates/webnn/transpose_conv_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(TransposeConvTest, 2x2Stride2) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(2)
+      .KernelWidth(2)
+      .StrideHeight(2)
+      .StrideWidth(2)
+      .ValidPadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, 2x2Stride2NoBias) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(2)
+      .KernelWidth(2)
+      .StrideHeight(2)
+      .StrideWidth(2)
+      .ValidPadding()
+      .NoBias()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, 3x3Stride2) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(3)
+      .KernelWidth(3)
+      .StrideHeight(2)
+      .StrideWidth(2)
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, 3x3Stride2NoBias) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(3)
+      .KernelWidth(3)
+      .StrideHeight(2)
+      .StrideWidth(2)
+      .SamePadding()
+      .NoBias()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, 4x4Stride2) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(4)
+      .KernelWidth(4)
+      .StrideHeight(2)
+      .StrideWidth(2)
+      .ValidPadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, 4x4Stride2NoBias) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(4)
+      .KernelWidth(4)
+      .StrideHeight(2)
+      .StrideWidth(2)
+      .ValidPadding()
+      .NoBias()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, 4x4Stride4) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(4)
+      .KernelWidth(4)
+      .StrideHeight(4)
+      .StrideWidth(4)
+      .ValidPadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, 4x4Stride4NoBias) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(5, 25), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(4)
+      .KernelWidth(4)
+      .StrideHeight(4)
+      .StrideWidth(4)
+      .ValidPadding()
+      .NoBias()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, SmallKernelWithSamePadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 7), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .BatchSize(batch_rng())
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, SmallKernelWithSamePaddingNoBias) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 7), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .BatchSize(batch_rng())
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .SamePadding()
+      .NoBias()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, SmallKernelWithValidPadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 7), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .BatchSize(batch_rng())
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .ValidPadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, SmallKernelWithValidPaddingNoBias) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 7), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .BatchSize(batch_rng())
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .ValidPadding()
+      .NoBias()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, StrideWithSamePadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .BatchSize(batch_rng())
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SamePadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, StrideWithSamePaddingNoBias) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .BatchSize(batch_rng())
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SamePadding()
+      .NoBias()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, StrideWithValidPadding) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .BatchSize(batch_rng())
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .ValidPadding()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, StrideWithValidPaddingNoBias) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .BatchSize(batch_rng())
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .ValidPadding()
+      .NoBias()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, FP16Weights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .BatchSize(batch_rng())
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SamePadding()
+      .FP16Weights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, FP16WeightsNoBias) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .BatchSize(batch_rng())
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SamePadding()
+      .NoBias()
+      .FP16Weights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, SparseWeights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .BatchSize(batch_rng())
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SamePadding()
+      .SparseWeights()
+      .Test(webnn_delegate.get());
+}
+
+TEST(TransposeConvTest, SparseWeightsNoBias) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto batch_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 4), std::ref(rng));
+  auto output_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(10, 25), std::ref(rng));
+  auto kernel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(3, 5), std::ref(rng));
+  auto stride_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 3), std::ref(rng));
+  auto channel_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+
+  TransposeConvTester()
+      .BatchSize(batch_rng())
+      .OutputHeight(output_rng())
+      .OutputWidth(output_rng())
+      .InputChannels(channel_rng())
+      .OutputChannels(channel_rng())
+      .KernelHeight(kernel_rng())
+      .KernelWidth(kernel_rng())
+      .StrideHeight(stride_rng())
+      .StrideWidth(stride_rng())
+      .SamePadding()
+      .NoBias()
+      .SparseWeights()
+      .Test(webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/transpose_conv_tester.cc b/tensorflow/lite/delegates/webnn/transpose_conv_tester.cc
new file mode 100644
index 00000000000..cc3da91afcc
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/transpose_conv_tester.cc
@@ -0,0 +1,333 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/transpose_conv_tester.h"
+
+#include <cassert>
+#include <cstdint>
+#include <functional>
+#include <numeric>
+#include <random>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "fp16.h"  // from @FP16
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/c/builtin_op_data.h"
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+void TransposeConvTester::Test(TfLiteDelegate* delegate) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto f32rng = std::bind(std::uniform_real_distribution<float>(), rng);
+
+  std::vector<char> buffer = CreateTfLiteModel();
+  const Model* model = GetModel(buffer.data());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+
+  ASSERT_EQ(delegate_interpreter->inputs().size(), 1);
+  ASSERT_EQ(default_interpreter->inputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->outputs().size(), 1);
+  ASSERT_EQ(default_interpreter->outputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  const int input_data_size =
+      BatchSize() * InputHeight() * InputWidth() * InputChannels();
+  float* default_input_data = default_interpreter->typed_input_tensor<float>(0);
+  std::generate(default_input_data, default_input_data + input_data_size,
+                std::ref(f32rng));
+
+  float* webnn_input_data =
+      delegate_interpreter->typed_input_tensor<float>(0);
+  std::copy(default_input_data, default_input_data + input_data_size,
+            webnn_input_data);
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float* default_output_data = default_interpreter->typed_tensor<float>(
+      default_interpreter->outputs()[0]);
+  float* webnn_output_data = delegate_interpreter->typed_tensor<float>(
+      delegate_interpreter->outputs()[0]);
+
+  const int output_data_size =
+      BatchSize() * OutputHeight() * OutputWidth() * OutputChannels();
+  for (size_t i = 0; i < output_data_size; i++) {
+    ASSERT_NEAR(default_output_data[i], webnn_output_data[i],
+                std::numeric_limits<float>::epsilon() *
+                    std::max(std::abs(default_output_data[i]) * 25.0f, 1.0f));
+  }
+}
+
+std::vector<char> TransposeConvTester::CreateTfLiteModel() const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto f32rng = std::bind(std::uniform_real_distribution<float>(), rng);
+
+  const std::vector<int32_t> input_shape = {BatchSize(), InputHeight(),
+                                            InputWidth(), InputChannels()};
+  const std::vector<int32_t> output_shape = {BatchSize(), OutputHeight(),
+                                             OutputWidth(), OutputChannels()};
+  const std::vector<int32_t> filter_shape = {OutputChannels(), KernelHeight(),
+                                             KernelWidth(), InputChannels()};
+  const std::vector<int32_t> bias_shape = {OutputChannels()};
+
+  flatbuffers::FlatBufferBuilder builder;
+
+  std::vector<flatbuffers::Offset<OperatorCode>> operator_codes;
+
+  std::vector<flatbuffers::Offset<tflite::Operator>> operators;
+  std::vector<flatbuffers::Offset<Tensor>> tensors;
+
+  // Buffer 0 is a sentinel as required by the schema, means "no buffer".
+  std::vector<flatbuffers::Offset<tflite::Buffer>> buffers = {
+      CreateBuffer(builder, builder.CreateVector({}))};
+  const int kNoBuffer = 0;
+
+  // Create a tensor containing the expected output shape.
+  const int buffer_index_output_shape = buffers.size();
+  buffers.emplace_back(CreateBuffer(
+      builder, builder.CreateVector(
+                   reinterpret_cast<const uint8_t*>(output_shape.data()),
+                   sizeof(int32_t) * output_shape.size())));
+
+  std::vector<int32_t> output_shape_tensor_shape = {4};
+  const int tensor_index_output_shape = tensors.size();
+  tensors.emplace_back(
+      CreateTensorDirect(builder, &output_shape_tensor_shape, TensorType_INT32,
+                         /*buffer=*/buffer_index_output_shape));
+
+  // The last one (two) tensor(s) will be the float32 kernel (and bias if used).
+  if (FP16Weights()) {
+    const int kOpCodeIndexDequantize = operator_codes.size();
+    operator_codes.emplace_back(
+        CreateOperatorCode(builder, BuiltinOperator_DEQUANTIZE));
+
+    auto f16rng = std::bind(fp16_ieee_from_fp32_value, f32rng);
+
+    std::vector<uint16_t> filter_data(OutputChannels() * KernelHeight() *
+                                      KernelWidth() * InputChannels());
+
+    std::generate(filter_data.begin(), filter_data.end(), f16rng);
+
+    const int buffer_index_filter = buffers.size();
+    buffers.emplace_back(CreateBuffer(
+        builder, builder.CreateVector(
+                     reinterpret_cast<const uint8_t*>(filter_data.data()),
+                     sizeof(uint16_t) * filter_data.size())));
+
+    const int tensor_index_float16_filter = tensors.size();
+    tensors.emplace_back(CreateTensorDirect(builder, &filter_shape,
+                                            TensorType_FLOAT16,
+                                            /*buffer=*/buffer_index_filter));
+
+    const int kInvalidIndex = -1;
+    int tensor_index_float16_bias = kInvalidIndex;
+    if (UseBias()) {
+      std::vector<uint16_t> bias_data(OutputChannels());
+      std::generate(bias_data.begin(), bias_data.end(), f16rng);
+
+      const int buffer_index_bias = buffers.size();
+      buffers.emplace_back(CreateBuffer(
+          builder, builder.CreateVector(
+                       reinterpret_cast<const uint8_t*>(bias_data.data()),
+                       sizeof(uint16_t) * bias_data.size())));
+
+      tensor_index_float16_bias = tensors.size();
+      tensors.emplace_back(CreateTensorDirect(builder, &bias_shape,
+                                              TensorType_FLOAT16,
+                                              /*buffer=*/buffer_index_bias));
+    }
+
+    const int tensor_index_filter = tensors.size();
+    tensors.emplace_back(CreateTensorDirect(
+        builder, &filter_shape, TensorType_FLOAT32, /*buffer=*/kNoBuffer));
+
+    const std::vector<int32_t> dequantize_filter_inputs = {
+        tensor_index_float16_filter};
+    const std::vector<int32_t> dequantize_filter_outputs{tensor_index_filter};
+    operators.emplace_back(CreateOperatorDirect(
+        builder, /*opcode_index=*/kOpCodeIndexDequantize,
+        &dequantize_filter_inputs, &dequantize_filter_outputs));
+
+    assert(tensor_index_filter + 1 == tensors.size());
+
+    if (UseBias()) {
+      const int tensor_index_bias = tensors.size();
+      tensors.emplace_back(CreateTensorDirect(
+          builder, &bias_shape, TensorType_FLOAT32, /*buffer=*/kNoBuffer));
+
+      const std::vector<int32_t> dequantize_bias_inputs = {
+          tensor_index_float16_bias};
+      const std::vector<int32_t> dequantize_bias_outputs = {tensor_index_bias};
+      operators.emplace_back(CreateOperatorDirect(
+          builder, /*opcode_index=*/kOpCodeIndexDequantize,
+          &dequantize_bias_inputs, &dequantize_bias_outputs));
+
+      assert(tensor_index_bias + 1 == tensors.size());
+    }
+  } else {
+    std::vector<float> filter_data(OutputChannels() * KernelHeight() *
+                                   KernelWidth() * InputChannels());
+
+    std::generate(filter_data.begin(), filter_data.end(), f32rng);
+
+    const int buffer_index_filter = buffers.size();
+    buffers.emplace_back(CreateBuffer(
+        builder, builder.CreateVector(
+                     reinterpret_cast<const uint8_t*>(filter_data.data()),
+                     sizeof(float) * filter_data.size())));
+
+    if (SparseWeights()) {
+      const int dims_count = filter_shape.size();
+      std::vector<flatbuffers::Offset<DimensionMetadata>> dim_metadata(
+          dims_count);
+      std::vector<int> traversal_order(dims_count);
+      for (int dim = 0; dim < dims_count; dim++) {
+        traversal_order[dim] = dim;
+        dim_metadata[dim] = CreateDimensionMetadata(
+            builder, DimensionType_DENSE, filter_shape[dim]);
+      }
+      flatbuffers::Offset<SparsityParameters> sparsity_parameters =
+          CreateSparsityParameters(
+              builder, builder.CreateVector(traversal_order),
+              /*block_map=*/0, builder.CreateVector(dim_metadata));
+      const int tensor_index_filter_sparse = tensors.size();
+      tensors.emplace_back(CreateTensorDirect(
+          builder, &filter_shape, TensorType_FLOAT32,
+          /*buffer=*/buffer_index_filter, /*name=*/nullptr, /*quantization=*/0,
+          /*is_variable=*/false, /*sparsity=*/sparsity_parameters));
+
+      const int opcode_index_densify = operator_codes.size();
+      operator_codes.emplace_back(
+          CreateOperatorCode(builder, BuiltinOperator_DENSIFY));
+
+      const int future_tensor_index_filter = tensors.size();
+      const std::vector<int32_t> densify_filter_inputs = {
+          tensor_index_filter_sparse};
+      const std::vector<int32_t> densify_filter_outputs = {
+          future_tensor_index_filter};
+      operators.emplace_back(CreateOperatorDirect(
+          builder, /*opcode_index=*/opcode_index_densify,
+          &densify_filter_inputs, &densify_filter_outputs));
+
+      // The dense filter tensor is just about to be added.
+      assert(future_tensor_index_filter == tensors.size());
+    }
+
+    tensors.emplace_back(CreateTensorDirect(
+        builder, &filter_shape, TensorType_FLOAT32,
+        /*buffer=*/SparseWeights() ? kNoBuffer : buffer_index_filter));
+
+    if (UseBias()) {
+      std::vector<float> bias_data(OutputChannels());
+      std::generate(bias_data.begin(), bias_data.end(), f32rng);
+
+      const int buffer_index_bias = buffers.size();
+      buffers.emplace_back(CreateBuffer(
+          builder, builder.CreateVector(
+                       reinterpret_cast<const uint8_t*>(bias_data.data()),
+                       sizeof(float) * bias_data.size())));
+
+      tensors.emplace_back(CreateTensorDirect(builder, &bias_shape,
+                                              TensorType_FLOAT32,
+                                              /*buffer=*/buffer_index_bias));
+    }
+  }
+
+  const int top_tensor = tensors.size() - 1;
+  const int tensor_index_filter = UseBias() ? top_tensor - 1 : top_tensor;
+
+  const int tensor_index_input = tensors.size();
+  tensors.emplace_back(
+      CreateTensorDirect(builder, &input_shape, TensorType_FLOAT32));
+
+  std::vector<int32_t> op_inputs = {tensor_index_output_shape,
+                                    tensor_index_filter, tensor_index_input};
+  if (UseBias()) {
+    const int tensor_index_bias = top_tensor;
+    op_inputs.push_back(tensor_index_bias);
+  }
+
+  const int tensor_index_output = tensors.size();
+  tensors.emplace_back(
+      CreateTensorDirect(builder, &output_shape, TensorType_FLOAT32));
+
+  const std::vector<int32_t> op_outputs = {tensor_index_output};
+
+  const int opcode_index_transpose_conv = operator_codes.size();
+  operator_codes.emplace_back(
+      CreateOperatorCode(builder, BuiltinOperator_TRANSPOSE_CONV));
+
+  flatbuffers::Offset<TransposeConvOptions> transpose_conv_options =
+      CreateTransposeConvOptions(builder, Padding(), StrideWidth(),
+                                 StrideHeight());
+  operators.emplace_back(CreateOperatorDirect(
+      builder, /*opcode_index=*/opcode_index_transpose_conv, &op_inputs,
+      &op_outputs, BuiltinOptions_TransposeConvOptions,
+      transpose_conv_options.Union()));
+
+  const std::vector<int32_t> subgraph_inputs = {tensor_index_input};
+  const std::vector<int32_t> subgraph_outputs = {tensor_index_output};
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraphDirect(
+      builder, &tensors, &subgraph_inputs, &subgraph_outputs, &operators);
+
+  flatbuffers::Offset<flatbuffers::String> description =
+      builder.CreateString("TransposeConv model");
+
+  flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION,
+      builder.CreateVector(operator_codes.data(), operator_codes.size()),
+      builder.CreateVector(&subgraph, 1), description,
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/transpose_conv_tester.h b/tensorflow/lite/delegates/webnn/transpose_conv_tester.h
new file mode 100644
index 00000000000..a7be695d9dc
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/transpose_conv_tester.h
@@ -0,0 +1,229 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_TRANSPOSE_CONV_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_TRANSPOSE_CONV_TESTER_H_
+
+#include <cstdint>
+#include <functional>
+#include <random>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/builtin_op_data.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+
+namespace tflite {
+namespace webnn {
+
+class TransposeConvTester {
+ public:
+  TransposeConvTester() = default;
+  TransposeConvTester(const TransposeConvTester&) = delete;
+  TransposeConvTester& operator=(const TransposeConvTester&) = delete;
+
+  inline TransposeConvTester& BatchSize(int32_t batch_size) {
+    EXPECT_GT(batch_size, 0);
+    batch_size_ = batch_size;
+    return *this;
+  }
+
+  inline int32_t BatchSize() const { return batch_size_; }
+
+  inline TransposeConvTester& InputChannels(int32_t input_channels) {
+    EXPECT_GT(input_channels, 0);
+    input_channels_ = input_channels;
+    return *this;
+  }
+
+  inline int32_t InputChannels() const { return input_channels_; }
+
+  inline TransposeConvTester& OutputChannels(int32_t output_channels) {
+    EXPECT_GT(output_channels, 0);
+    output_channels_ = output_channels;
+    return *this;
+  }
+
+  inline int32_t OutputChannels() const { return output_channels_; }
+
+  inline TransposeConvTester& OutputHeight(int32_t output_height) {
+    EXPECT_GT(output_height, 0);
+    output_height_ = output_height;
+    return *this;
+  }
+
+  inline int32_t OutputHeight() const { return output_height_; }
+
+  inline TransposeConvTester& OutputWidth(int32_t output_width) {
+    EXPECT_GT(output_width, 0);
+    output_width_ = output_width;
+    return *this;
+  }
+
+  inline int32_t OutputWidth() const { return output_width_; }
+
+  inline TransposeConvTester& KernelHeight(int32_t kernel_height) {
+    EXPECT_GT(kernel_height, 0);
+    kernel_height_ = kernel_height;
+    return *this;
+  }
+
+  inline int32_t KernelHeight() const { return kernel_height_; }
+
+  inline TransposeConvTester& KernelWidth(int32_t kernel_width) {
+    EXPECT_GT(kernel_width, 0);
+    kernel_width_ = kernel_width;
+    return *this;
+  }
+
+  inline int32_t KernelWidth() const { return kernel_width_; }
+
+  inline TransposeConvTester& StrideHeight(int32_t stride_height) {
+    EXPECT_GT(stride_height, 0);
+    stride_height_ = stride_height;
+    return *this;
+  }
+
+  inline int32_t StrideHeight() const { return stride_height_; }
+
+  inline TransposeConvTester& StrideWidth(int32_t stride_width) {
+    EXPECT_GT(stride_width, 0);
+    stride_width_ = stride_width;
+    return *this;
+  }
+
+  inline int32_t StrideWidth() const { return stride_width_; }
+
+  inline TransposeConvTester& FP16Weights() {
+    fp16_weights_ = true;
+    return *this;
+  }
+
+  inline bool FP16Weights() const { return fp16_weights_; }
+
+  inline TransposeConvTester& INT8Weights() {
+    int8_weights_ = true;
+    return *this;
+  }
+
+  inline bool INT8Weights() const { return int8_weights_; }
+
+  inline TransposeConvTester& INT8ChannelWiseWeights() {
+    int8_channel_wise_weights_ = true;
+    return *this;
+  }
+
+  inline bool INT8ChannelWiseWeights() const {
+    return int8_channel_wise_weights_;
+  }
+
+  inline TransposeConvTester& SparseWeights() {
+    sparse_weights_ = true;
+    return *this;
+  }
+
+  inline bool SparseWeights() const { return sparse_weights_; }
+
+  inline TransposeConvTester& SamePadding() {
+    padding_ = ::tflite::Padding_SAME;
+    return *this;
+  }
+
+  inline TransposeConvTester& ValidPadding() {
+    padding_ = ::tflite::Padding_VALID;
+    return *this;
+  }
+
+  inline ::tflite::Padding Padding() const { return padding_; }
+
+  inline int32_t InputWidth() const {
+    return ComputeInputSize(OutputWidth(), KernelWidth(), StrideWidth());
+  }
+
+  inline int32_t InputHeight() const {
+    return ComputeInputSize(OutputHeight(), KernelHeight(), StrideHeight());
+  }
+
+  inline int32_t PaddingWidth() const {
+    return ComputePadding(OutputWidth(), KernelWidth(), StrideWidth());
+  }
+
+  inline int32_t PaddingHeight() const {
+    return ComputePadding(OutputHeight(), KernelHeight(), StrideHeight());
+  }
+
+  inline bool UseBias() const { return use_bias_; }
+
+  inline TransposeConvTester& WithBias(bool use_bias = true) {
+    use_bias_ = use_bias;
+    return *this;
+  }
+
+  inline TransposeConvTester& NoBias() { return WithBias(false); }
+
+  void Test(TfLiteDelegate* delegate) const;
+
+ private:
+  int32_t ComputeInputSize(int32_t output_size, int32_t kernel_size,
+                           int32_t stride) const {
+    // Roughly follows TFLite's `ComputeOutSize`.
+    switch (padding_) {
+      case ::tflite::Padding_VALID:
+        return (output_size + stride - kernel_size) / stride;
+        break;
+      case ::tflite::Padding_SAME:
+        return (output_size + stride - 1) / stride;
+        break;
+      default:
+        assert(false);
+    }
+  }
+
+  int32_t ComputePadding(int32_t output_size, int32_t kernel_size,
+                         int32_t stride) const {
+    // Roughly follows TFLite's `ComputePaddingWithOffset`.
+    if (padding_ == ::tflite::Padding_VALID) {
+      return 0;
+    }
+    assert(padding_ == ::tflite::Padding_SAME);
+    const int32_t input_size =
+        ComputeInputSize(output_size, kernel_size, stride);
+    return (output_size - 1) * stride + kernel_size - input_size;
+  }
+
+ private:
+  std::vector<char> CreateTfLiteModel() const;
+
+  int32_t batch_size_ = 1;
+  int32_t input_channels_ = 1;
+  int32_t output_channels_ = 1;
+  int32_t output_height_ = 1;
+  int32_t output_width_ = 1;
+  int32_t kernel_height_ = 1;
+  int32_t kernel_width_ = 1;
+  int32_t stride_height_ = 1;
+  int32_t stride_width_ = 1;
+  ::tflite::Padding padding_ = ::tflite::Padding_VALID;
+  bool use_bias_ = true;
+  bool fp16_weights_ = false;
+  bool int8_weights_ = false;
+  bool int8_channel_wise_weights_ = false;
+  bool sparse_weights_ = false;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_TRANSPOSE_CONV_TESTER_H_
diff --git a/tensorflow/lite/delegates/webnn/unary_elementwise_tester.cc b/tensorflow/lite/delegates/webnn/unary_elementwise_tester.cc
new file mode 100644
index 00000000000..f30e42b0c85
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/unary_elementwise_tester.cc
@@ -0,0 +1,185 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/unary_elementwise_tester.h"
+
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <numeric>
+#include <random>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+void UnaryElementwiseTester::Test(tflite::BuiltinOperator unary_op,
+                                  TfLiteDelegate* delegate) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  std::uniform_real_distribution<float> input_distribution(-15.0f, 15.0f);
+  switch (unary_op) {
+    case BuiltinOperator_SQRT:
+      input_distribution = std::uniform_real_distribution<float>(0.0f, 10.0f);
+      break;
+    default:
+      break;
+  }
+  auto input_rng = std::bind(input_distribution, std::ref(rng));
+
+  std::vector<char> buffer = CreateTfLiteModel(unary_op);
+  const Model* model = GetModel(buffer.data());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+
+  ASSERT_EQ(delegate_interpreter->inputs().size(), 1);
+  ASSERT_EQ(default_interpreter->inputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->outputs().size(), 1);
+  ASSERT_EQ(default_interpreter->outputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  float* default_input_data = default_interpreter->typed_input_tensor<float>(0);
+  std::generate(default_input_data, default_input_data + Size(),
+                std::ref(input_rng));
+
+  float* delegate_input_data =
+      delegate_interpreter->typed_input_tensor<float>(0);
+  std::copy(default_input_data, default_input_data + Size(),
+            delegate_input_data);
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float* default_output_data =
+      default_interpreter->typed_output_tensor<float>(0);
+  float* delegate_output_data =
+      delegate_interpreter->typed_output_tensor<float>(0);
+
+  switch (unary_op) {
+    case BuiltinOperator_ABS:
+    case BuiltinOperator_CEIL:
+    case BuiltinOperator_FLOOR:
+    case BuiltinOperator_NEG:
+    case BuiltinOperator_RELU:
+    case BuiltinOperator_RELU_N1_TO_1:
+    case BuiltinOperator_RELU6:
+    case BuiltinOperator_ROUND:
+    case BuiltinOperator_SQUARE:
+    case BuiltinOperator_SQRT:
+      for (size_t i = 0; i < Size(); i++) {
+        ASSERT_EQ(default_output_data[i], delegate_output_data[i]);
+      }
+      break;
+    default:
+      for (size_t i = 0; i < Size(); i++) {
+        ASSERT_NEAR(
+            default_output_data[i], delegate_output_data[i],
+            std::numeric_limits<float>::epsilon() *
+                std::max(std::abs(default_output_data[i]) * RelativeTolerance(),
+                         1.0f));
+      }
+      break;
+  }
+}
+
+std::vector<char> UnaryElementwiseTester::CreateTfLiteModel(
+    tflite::BuiltinOperator unary_op) const {
+  flatbuffers::FlatBufferBuilder builder;
+  flatbuffers::Offset<OperatorCode> operator_code =
+      CreateOperatorCode(builder, unary_op);
+
+  const std::array<flatbuffers::Offset<Buffer>, 1> buffers{{
+      CreateBuffer(builder, builder.CreateVector({})),
+  }};
+
+  const std::array<flatbuffers::Offset<Tensor>, 2> tensors{{
+      CreateTensor(
+          builder,
+          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),
+          TensorType_FLOAT32),
+      CreateTensor(
+          builder,
+          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),
+          TensorType_FLOAT32),
+  }};
+
+  const std::array<int32_t, 1> op_inputs{{0}};
+  const std::array<int32_t, 1> op_outputs{{1}};
+  flatbuffers::Offset<Operator> op = CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()));
+
+  const std::array<int32_t, 1> subgraph_inputs{{0}};
+  const std::array<int32_t, 1> subgraph_outputs{{1}};
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(&op, 1));
+
+  flatbuffers::Offset<flatbuffers::String> description =
+      builder.CreateString("Unary operator model");
+
+  flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION, builder.CreateVector(&operator_code, 1),
+      builder.CreateVector(&subgraph, 1), description,
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+int32_t UnaryElementwiseTester::ComputeSize(const std::vector<int32_t>& shape) {
+  return std::accumulate(shape.cbegin(), shape.cend(), 1,
+                         std::multiplies<int32_t>());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/unary_elementwise_tester.h b/tensorflow/lite/delegates/webnn/unary_elementwise_tester.h
new file mode 100644
index 00000000000..12c6a0aac40
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/unary_elementwise_tester.h
@@ -0,0 +1,70 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_UNARY_ELEMENTWISE_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_UNARY_ELEMENTWISE_TESTER_H_
+
+#include <cstdint>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+
+namespace tflite {
+namespace webnn {
+
+class UnaryElementwiseTester {
+ public:
+  UnaryElementwiseTester() = default;
+  UnaryElementwiseTester(const UnaryElementwiseTester&) = delete;
+  UnaryElementwiseTester& operator=(const UnaryElementwiseTester&) = delete;
+
+  inline UnaryElementwiseTester& Shape(std::initializer_list<int32_t> shape) {
+    for (auto it = shape.begin(); it != shape.end(); ++it) {
+      EXPECT_GT(*it, 0);
+    }
+    shape_ = std::vector<int32_t>(shape.begin(), shape.end());
+    size_ = UnaryElementwiseTester::ComputeSize(shape_);
+    return *this;
+  }
+
+  const std::vector<int32_t>& Shape() const { return shape_; }
+
+  int32_t Size() const { return size_; }
+
+  inline UnaryElementwiseTester& RelativeTolerance(float relative_tolerance) {
+    relative_tolerance_ = relative_tolerance;
+    return *this;
+  }
+
+  float RelativeTolerance() const { return relative_tolerance_; }
+
+  void Test(tflite::BuiltinOperator unary_op, TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel(tflite::BuiltinOperator unary_op) const;
+
+  static int32_t ComputeSize(const std::vector<int32_t>& shape);
+
+  std::vector<int32_t> shape_;
+  int32_t size_;
+  float relative_tolerance_{10.0f};
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_UNARY_ELEMENTWISE_TESTER_H_
diff --git a/tensorflow/lite/delegates/webnn/unpack_test.cc b/tensorflow/lite/delegates/webnn/unpack_test.cc
new file mode 100644
index 00000000000..d675f2d4fb2
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/unpack_test.cc
@@ -0,0 +1,105 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <gtest/gtest.h>
+
+#include <algorithm>
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include "tensorflow/lite/delegates/webnn/unpack_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Split, 4Dto3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+  const std::vector<int32_t> shape(
+      {shape_rng() * 2, shape_rng() * 2, shape_rng() * 2, shape_rng() * 2});
+
+  for (int i = -4; i < 4; i++) {
+    UnpackTester().InputShape(shape).UnpackAxis(i).NumSplits(2).Test(
+        TensorType_FLOAT32, webnn_delegate.get());
+  }
+}
+
+TEST(Split, 3Dto2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+  const std::vector<int32_t> shape(
+      {shape_rng() * 2, shape_rng() * 2, shape_rng() * 2});
+
+  for (int i = -3; i < 3; i++) {
+    UnpackTester().InputShape(shape).UnpackAxis(i).NumSplits(2).Test(
+        TensorType_FLOAT32, webnn_delegate.get());
+  }
+}
+
+TEST(Split, 2Dto1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+  const std::vector<int32_t> shape({shape_rng() * 2, shape_rng() * 2});
+
+  for (int i = -2; i < 2; i++) {
+    UnpackTester().InputShape(shape).UnpackAxis(i).NumSplits(2).Test(
+        TensorType_FLOAT32, webnn_delegate.get());
+  }
+}
+
+TEST(Split, 1Dto0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(1, 5), std::ref(rng));
+  const std::vector<int32_t> shape({shape_rng() * 2});
+
+  for (int i = -1; i < 1; i++) {
+    UnpackTester().InputShape(shape).UnpackAxis(i).NumSplits(2).Test(
+        TensorType_FLOAT32, webnn_delegate.get());
+  }
+}
+}  // namespace webnn
+}  // namespace tflite
\ No newline at end of file
diff --git a/tensorflow/lite/delegates/webnn/unpack_tester.cc b/tensorflow/lite/delegates/webnn/unpack_tester.cc
new file mode 100644
index 00000000000..f2987aafcf8
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/unpack_tester.cc
@@ -0,0 +1,223 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/unpack_tester.h"
+
+#include <gtest/gtest.h>
+
+#include <algorithm>
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <numeric>
+#include <random>
+#include <string>
+#include <vector>
+
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+template <class T>
+void UnpackTester::Test(Interpreter* delegate_interpreter,
+                        Interpreter* default_interpreter) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  std::uniform_int_distribution<int32_t> input_distribution(
+      std::numeric_limits<T>::min(), std::numeric_limits<T>::max());
+  auto input_rng = std::bind(input_distribution, std::ref(rng));
+
+  T* default_input_data = default_interpreter->typed_input_tensor<T>(0);
+  std::generate(default_input_data,
+                default_input_data + ComputeSize(InputShape()),
+                std::ref(input_rng));
+
+  T* webnn_input_data = delegate_interpreter->typed_input_tensor<T>(0);
+  std::copy(default_input_data, default_input_data + ComputeSize(InputShape()),
+            webnn_input_data);
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  int output_size = NumSplits();
+  std::vector<T*> default_output(output_size);
+  std::vector<T*> delegate_output(output_size);
+  for (size_t i = 0; i < output_size; i++) {
+    default_output[i] = default_interpreter->typed_output_tensor<T>(i);
+    delegate_output[i] = delegate_interpreter->typed_output_tensor<T>(i);
+  }
+
+  for (size_t i = 0; i < output_size; i++) {
+    for (size_t j = 0; j < ComputeSize(OutputShape()); j++) {
+      ASSERT_EQ(default_output[i][j], delegate_output[i][j]);
+    }
+  }
+}
+
+template <>
+void UnpackTester::Test<float>(Interpreter* delegate_interpreter,
+                               Interpreter* default_interpreter) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  std::uniform_real_distribution<float> input_distribution(-1.0f, 1.0f);
+  auto input_rng = std::bind(input_distribution, std::ref(rng));
+
+  float* default_input_data = default_interpreter->typed_input_tensor<float>(0);
+  std::generate(default_input_data,
+                default_input_data + ComputeSize(InputShape()),
+                std::ref(input_rng));
+  float* webnn_input_data = delegate_interpreter->typed_input_tensor<float>(0);
+
+  std::copy(default_input_data, default_input_data + ComputeSize(InputShape()),
+            webnn_input_data);
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  int output_size = NumSplits();
+  std::vector<float*> default_output(output_size);
+  std::vector<float*> delegate_output(output_size);
+  for (size_t i = 0; i < output_size; i++) {
+    default_output[i] = default_interpreter->typed_output_tensor<float>(i);
+    delegate_output[i] = delegate_interpreter->typed_output_tensor<float>(i);
+  }
+
+  for (size_t i = 0; i < output_size; i++) {
+    for (size_t j = 0; j < ComputeSize(OutputShape()); j++) {
+      ASSERT_EQ(default_output[i][j], delegate_output[i][j]);
+    }
+  }
+}
+
+void UnpackTester::Test(TensorType tensor_type,
+                        TfLiteDelegate* delegate) const {
+  std::vector<char> buffer = CreateTfLiteModel(tensor_type);
+  const Model* model = GetModel(buffer.data());
+
+  int32_t axis = UnpackAxis();
+  axis += axis < 0 ? InputShape().size() : 0;
+  ASSERT_EQ(0, InputShape()[axis] % NumSplits());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+  ASSERT_EQ(delegate_interpreter->inputs().size(), 1);
+  ASSERT_EQ(default_interpreter->inputs().size(), 1);
+  ASSERT_EQ(delegate_interpreter->outputs().size(), NumSplits());
+  ASSERT_EQ(default_interpreter->outputs().size(), NumSplits());
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  switch (tensor_type) {
+    case TensorType_FLOAT32:
+      Test<float>(delegate_interpreter.get(), default_interpreter.get());
+      break;
+    case TensorType_INT8:
+      Test<int8_t>(delegate_interpreter.get(), default_interpreter.get());
+      break;
+    case TensorType_UINT8:
+      Test<uint8_t>(delegate_interpreter.get(), default_interpreter.get());
+      break;
+    default:
+      GTEST_FAIL();
+  }
+}
+
+std::vector<char> UnpackTester::CreateTfLiteModel(
+    TensorType tensor_type) const {
+  flatbuffers::FlatBufferBuilder builder;
+  flatbuffers::Offset<OperatorCode> operator_code =
+      CreateOperatorCode(builder, BuiltinOperator_UNPACK);
+
+  std::vector<flatbuffers::Offset<Buffer>> buffers{{
+      CreateBuffer(builder, builder.CreateVector({})),
+  }};
+
+  std::vector<flatbuffers::Offset<Tensor>> tensors{{CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(InputShape().data(), InputShape().size()),
+      tensor_type,
+      /*buffer=*/0, /*name=*/0)}};
+
+  for (int i = 0; i < NumSplits(); i++) {
+    tensors.push_back(
+        CreateTensor(builder,
+                     builder.CreateVector<int32_t>(OutputShape().data(),
+                                                   OutputShape().size()),
+                     tensor_type,
+                     /*buffer=*/0, /*name=*/0));
+  }
+
+  const std::array<int32_t, 1> op_inputs{{0}};
+  std::vector<int32_t> op_outputs;
+  op_outputs.reserve(NumSplits());
+  for (int i = 0; i < NumSplits(); i++) {
+    op_outputs.push_back(op_inputs.size() + i);
+  }
+  EXPECT_EQ(op_outputs.size(), NumSplits());
+  const flatbuffers::Offset<Operator> op = CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()),
+      tflite::BuiltinOptions_UnpackOptions,
+      CreateUnpackOptions(builder, NumSplits(), UnpackAxis()).Union());
+
+  const std::array<int32_t, 1> subgraph_inputs = op_inputs;
+  const std::vector<int32_t> subgraph_outputs = op_outputs;
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(&op, 1));
+
+  const flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION, builder.CreateVector(&operator_code, 1),
+      builder.CreateVector(&subgraph, 1), builder.CreateString("Unpack model"),
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+int32_t UnpackTester::ComputeSize(const std::vector<int32_t>& shape) {
+  return std::accumulate(shape.cbegin(), shape.cend(), 1,
+                         std::multiplies<int32_t>());
+}
+
+}  // namespace webnn
+}  // namespace tflite
\ No newline at end of file
diff --git a/tensorflow/lite/delegates/webnn/unpack_tester.h b/tensorflow/lite/delegates/webnn/unpack_tester.h
new file mode 100644
index 00000000000..ef2dd7c66c3
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/unpack_tester.h
@@ -0,0 +1,88 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_SPLIT_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_SPLIT_TESTER_H_
+
+#include <cstdint>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/minimal_logging.h"
+namespace tflite {
+namespace webnn {
+
+class UnpackTester {
+ public:
+  UnpackTester() = default;
+  UnpackTester(const UnpackTester&) = delete;
+  UnpackTester& operator=(const UnpackTester&) = delete;
+
+  inline UnpackTester& UnpackAxis(int32_t unpack_axis) {
+    unpack_axis_ = unpack_axis;
+    return *this;
+  }
+
+  inline UnpackTester& InputShape(const std::vector<int32_t>& shape) {
+    for (auto it = shape.begin(); it != shape.end(); ++it) {
+      EXPECT_GT(*it, 0);
+    }
+    input_shape_ = std::vector<int32_t>(shape.begin(), shape.end());
+    return *this;
+  }
+
+  int32_t UnpackAxis() const { return unpack_axis_; }
+
+  inline UnpackTester& NumSplits(int num_splits) {
+    int32_t split_dim = UnpackAxis();
+    split_dim += split_dim < 0 ? InputShape().size() : 0;
+    TFLITE_LOG(tflite::TFLITE_LOG_INFO,"%d\n",split_dim);
+    num_splits_ = InputShape()[split_dim];
+    return *this;
+  }
+
+  inline const int NumSplits() const { return num_splits_; }
+
+  inline const std::vector<int32_t>& InputShape() const { return input_shape_; }
+
+  std::vector<int32_t> OutputShape() const {
+    std::vector<int32_t> output_shape = InputShape();
+    int32_t split_dim = UnpackAxis();
+    split_dim += split_dim < 0 ? InputShape().size() : 0;
+    EXPECT_LE(0, split_dim);
+    EXPECT_EQ(0, output_shape[split_dim] % NumSplits());
+    output_shape.erase(output_shape.begin()+split_dim);
+    return output_shape;
+  }
+
+  template <typename T>
+  void Test(Interpreter* delegate_interpreter,
+            Interpreter* default_interpreter) const;
+  void Test(TensorType tensor_type, TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel(TensorType tensor_type) const;
+
+  static int32_t ComputeSize(const std::vector<int32_t>& shape);
+
+  std::vector<int32_t> input_shape_;
+  int32_t unpack_axis_;
+  int32_t num_splits_;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_SPLIT_TESTER_H_
\ No newline at end of file
diff --git a/tensorflow/lite/delegates/webnn/webnn_delegate.cc b/tensorflow/lite/delegates/webnn/webnn_delegate.cc
new file mode 100644
index 00000000000..dab893c7d58
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/webnn_delegate.cc
@@ -0,0 +1,2810 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+#include <algorithm>
+#include <array>
+#include <cstdint>
+#include <cstring>
+#include <limits>
+#include <memory>
+#include <string>
+#include <unordered_map>
+#include <unordered_set>
+#include <utility>
+#include <vector>
+
+#ifdef __EMSCRIPTEN__
+#include <emscripten.h>
+#include <emscripten/html5.h>
+#include <emscripten/val.h>
+#endif
+
+#include <fp16/fp16.h>
+#include "tensorflow/lite/builtin_ops.h"
+#include "tensorflow/lite/c/builtin_op_data.h"
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/minimal_logging.h"
+#include "tensorflow/lite/kernels/internal/utils/sparsity_format_converter.h"
+
+namespace tflite {
+namespace webnn {
+namespace {
+
+// Forward declaration.
+TfLiteStatus DelegatePrepare(TfLiteContext* context, TfLiteDelegate* delegate);
+
+class Delegate {
+  friend class Subgraph;
+
+ public:
+  explicit Delegate(const TfLiteWebNNDelegateOptions* options) {
+    std::unordered_map<uint32_t, std::string> device_type_name_s = {
+        {0, "auto"}, {1, "gpu"}, {2, "cpu"}};
+    std::unordered_map<uint32_t, std::string> power_preference_name_s = {
+        {0, "auto"}, {1, "high-performance"}, {2, "low-power"}};
+    device_type_name_ = device_type_name_s[options->deviceType];
+    power_preference_name_ = power_preference_name_s[options->powerPreference];
+    TFLITE_LOG_PROD_ONCE(tflite::TFLITE_LOG_INFO,
+                         "Created TensorFlow Lite WebNN delegate for device"
+                         " %s and power %s.",
+                         device_type_name_.c_str(),
+                         power_preference_name_.c_str());
+  }
+
+  TfLiteIntArray* PrepareOpsToDelegate(TfLiteContext* context);
+  TfLiteDelegate* tflite_delegate() { return &delegate_; }
+
+ private:
+  TfLiteDelegate delegate_ = {
+      reinterpret_cast<void*>(this),  // .data_
+      DelegatePrepare,                // .Prepare
+      nullptr,                        // .CopyFromBufferHandle
+      nullptr,                        // .CopyToBufferHandle
+      nullptr,                        // .FreeBufferHandle
+      kTfLiteDelegateFlagsNone,       // .flags
+  };
+
+  // Unpacked data for quasi-static tensors, i.e. tensors produced by
+  // dequantizing or unpacking static buffers.
+  std::vector<char> static_unpacked_data_;
+  // Mapping from a tensor index for a quasi-static tensor to the offset to
+  // its unpacked data within static_unpacked_data_.
+  std::unordered_map<int, size_t> static_unpacked_data_map_;
+  // Set of indices of nodes which unpack static data, e.g. Dequantize
+  // operators which convert FP16 static weights to FP32. These nodes are simply
+  // ignored in the delegate implementation, because their outputs are
+  // pre-unpacked in DelegatePrepare.
+  std::unordered_set<int> static_unpack_nodes_;
+  // Set of indices of tensors with unpacked static sparse weights.
+  std::unordered_set<int> static_sparse_weights_;
+  std::string device_type_name_;
+  std::string power_preference_name_;
+};
+
+class Subgraph {
+ public:
+  static Subgraph* Create(TfLiteContext* context,
+                          const TfLiteDelegateParams* params,
+                          const Delegate* delegate) {
+    // Convert subgraph inputs and outputs to hash sets for faster lookup.
+    const std::unordered_set<int> inputs(
+        &params->input_tensors->data[0],
+        &params->input_tensors->data[params->input_tensors->size]);
+    std::unordered_set<int> outputs;
+    for (int o = 0; o < params->output_tensors->size; o++) {
+      const int output_tensor_idx = params->output_tensors->data[o];
+      // Exclude quasi-static tensors which may have become subgraph outputs
+      // after partitioning.
+      if (delegate->static_unpacked_data_map_.count(output_tensor_idx) == 0) {
+        outputs.insert(output_tensor_idx);
+      }
+    }
+
+    TfLiteIntArray* execution_plan;
+    if (context->GetExecutionPlan(context, &execution_plan) != kTfLiteOk) {
+      return nullptr;
+    }
+
+    // Create WebNN context and graph builder
+    thread_local const emscripten::val ml = emscripten::val::global("navigator")["ml"];
+    emscripten::val context_options = emscripten::val::object();
+    context_options.set("deviceType", emscripten::val(delegate->device_type_name_));
+    context_options.set("powerPreference", emscripten::val(delegate->power_preference_name_));
+    emscripten::val wnn_context = ml.call<emscripten::val>("createContextSync", context_options);
+
+    if (!wnn_context.as<bool>()) {
+      TF_LITE_KERNEL_LOG(context, "Failed to create WebNN context.");
+      return nullptr;
+    }
+    emscripten::val wnn_builder = emscripten::val::global("MLGraphBuilder").new_(wnn_context);
+    if (!wnn_builder.as<bool>()) {
+      TF_LITE_KERNEL_LOG(context, "Failed to create WebNN graph builder.");
+      return nullptr;
+    }
+
+    bool has_sparse_weights = false;
+    // Detect which tensors are used as inputs or outputs of any subgraph nodes.
+    // -1 denotes tensor not used in the subgraph. These indexes will be
+    // filtered out and removed later.
+    std::vector<int> tensors(context->tensors_size, -1);
+    for (int i = 0; i < params->nodes_to_replace->size; i++) {
+      const int node_index = params->nodes_to_replace->data[i];
+
+      TfLiteNode* node = nullptr;
+      TfLiteRegistration* registration = nullptr;
+      if (context->GetNodeAndRegistration(context, node_index, &node,
+                                          &registration) != kTfLiteOk) {
+        return nullptr;
+      }
+
+      // Detect if any of the node's inputs are sparse weights.
+      if (!has_sparse_weights) {
+        for (int i = 0; i < node->inputs->size; i++) {
+          if (delegate->static_sparse_weights_.count(node->inputs->data[i]) !=
+              0) {
+            has_sparse_weights = true;
+          }
+        }
+      }
+
+      if (delegate->static_unpack_nodes_.count(node_index) != 0) {
+        // The node unpacks static input and can be skipped because its input
+        // was pre-unpacked in DelegatePrepare.
+        continue;
+      }
+
+      switch (registration->builtin_code) {
+        case kTfLiteBuiltinMean:
+        case kTfLiteBuiltinPad:
+        case kTfLiteBuiltinReshape:
+        case kTfLiteBuiltinResizeBilinear:
+          // Ignore the second input (new shape),
+          // because it is represented as parameters of the WebNN operator
+          // rather than extra input.
+          {
+            const int t = node->inputs->data[0];
+            tensors[t] = t;
+          }
+          break;
+        case kTfLiteBuiltinSplit:
+          // Ignore the first input (axis),
+          // because it is represented as parameters of the WebNN operator
+          // rather than extra input.
+          {
+            const int t = node->inputs->data[1];
+            tensors[t] = t;
+          }
+          break;
+        default:
+          // All other operators: process all inputs
+          for (int k = 0; k < node->inputs->size; k++) {
+            const int t = node->inputs->data[k];
+            if (t >= 0) {
+              tensors[t] = t;
+            }
+          }
+      }
+      for (int k = 0; k < node->outputs->size; k++) {
+        const int t = node->outputs->data[k];
+        if (t >= 0) {
+          tensors[t] = t;
+        }
+      }
+    }
+    // Filter out and remove -1 (unused) indexes.
+    tensors.erase(std::remove_if(tensors.begin(), tensors.end(),
+                                 [](int i) { return i < 0; }),
+                  tensors.end());
+    std::sort(tensors.begin(), tensors.end());
+
+    // Create a set of quasi-static tensors for VisitNode function
+    std::unordered_set<int> quasi_static_tensors;
+    for (const std::pair<const int, size_t>& entry :
+         delegate->static_unpacked_data_map_) {
+      quasi_static_tensors.insert(entry.first);
+    }
+
+    // WebNN operands for TFLite tensors
+    std::unordered_map<int, emscripten::val> webnn_operands;
+    std::unordered_set<int> compute_inputs;
+    for (int t : tensors) {
+      std::string datatype;
+      switch (context->tensors[t].type) {
+        case kTfLiteFloat32:
+          datatype = "float32";
+          break;
+        default:
+          TF_LITE_KERNEL_LOG(
+              context,
+              "unsupported datatype (%s) of tensor %d in WebNN delegate",
+              TfLiteTypeGetName(context->tensors[t].type), t);
+          return nullptr;
+      }
+
+      const void* data = nullptr;
+      if (context->tensors[t].allocation_type == kTfLiteMmapRo) {
+        data = context->tensors[t].data.raw_const;
+      } else {
+        // Check for quasi-static data.
+        const auto it = delegate->static_unpacked_data_map_.find(t);
+        if (it != delegate->static_unpacked_data_map_.end()) {
+          data = delegate->static_unpacked_data_.data() + it->second;
+        }
+      }
+
+      std::vector<int32_t> dims(
+          &context->tensors[t].dims->data[0],
+          &context->tensors[t].dims->data[context->tensors[t].dims->size]);
+
+      if (inputs.count(t) != 0 || quasi_static_tensors.count(t) != 0) {
+        emscripten::val desc = emscripten::val::object();
+        desc.set("type", emscripten::val(datatype));
+        desc.set("dimensions", emscripten::val::array(dims));
+
+        emscripten::val operand = emscripten::val::object();
+        if (data == nullptr) {
+          compute_inputs.insert(t);
+          std::string name = std::to_string(t);
+          operand = wnn_builder.call<emscripten::val>("input", name, desc);
+        } else {
+          auto data_size = context->tensors[t].bytes / 4;
+          emscripten::val view{ emscripten::typed_memory_view(data_size, static_cast<const float*>(data)) };
+          operand = wnn_builder.call<emscripten::val>("constant", desc, view);
+        }
+        webnn_operands.insert(std::make_pair(t, operand));
+      }
+    }
+
+    // Create WebNN nodes for TFLite delegate nodes
+    // keep the buffers of constants created during graph building.
+    std::vector<std::unique_ptr<char>> constant_buffers;
+    for (int i = 0; i < params->nodes_to_replace->size; i++) {
+      const int node_index = params->nodes_to_replace->data[i];
+      if (delegate->static_unpack_nodes_.count(node_index)) {
+        // The node unpacks static input and can be skipped because its input
+        // was pre-unpacked in DelegatePrepare.
+        continue;
+      }
+
+      TfLiteNode* node = nullptr;
+      TfLiteRegistration* registration = nullptr;
+      if (context->GetNodeAndRegistration(context, node_index, &node,
+                                          &registration) != kTfLiteOk) {
+        return nullptr;
+      }
+
+      if (VisitNode(wnn_builder, context, registration, node, node_index,
+                    quasi_static_tensors, webnn_operands, constant_buffers) != kTfLiteOk) {
+        return nullptr;
+      }
+    }
+
+    emscripten::val named_operands = emscripten::val::object();
+    for (auto o : outputs) {
+      std::string name = std::to_string(o);
+      if (!webnn_operands.at(o).as<bool>()) {
+        TF_LITE_KERNEL_LOG(context, "Invalid operand");
+        return nullptr;
+      }
+      named_operands.set(name, webnn_operands.at(o));
+    }
+
+    emscripten::val wnn_graph = wnn_builder.call<emscripten::val>("buildSync", named_operands);
+    if (!wnn_graph.as<bool>()) {
+      TF_LITE_KERNEL_LOG(context, "failed to build WebNN graph");
+      return nullptr;
+    }
+    return new Subgraph(wnn_context, wnn_graph, std::move(compute_inputs), std::move(outputs));
+  }
+
+  TfLiteStatus Prepare(TfLiteContext* context) { return kTfLiteOk; }
+
+  TfLiteStatus Invoke(TfLiteContext* context) {
+    bool any_pointers_changed = false;
+    for (std::pair<int, void*> io_info : externals_) {
+      const TfLiteTensor& tensor = context->tensors[io_info.first];
+      void* data_pointer = &dummy_data_;
+      if (tensor.data.raw != nullptr) {
+        data_pointer = tensor.data.raw;
+      } else {
+        if (tensor.bytes != 0) {
+          TF_LITE_KERNEL_LOG(
+              context, "unexpected null data pointer in external tensor %d",
+              io_info.first);
+          return kTfLiteError;
+        }
+      }
+      if (data_pointer != io_info.second) {
+        any_pointers_changed = true;
+        externals_[io_info.first] = data_pointer;
+      }
+    }
+
+    if (any_pointers_changed) {
+      graph_inputs_ = emscripten::val::object();
+      for (int t : inputs_) {
+        std::string name = std::to_string(t);
+        auto input_size = context->tensors[t].bytes / 4;
+        auto input_data = context->tensors[t].data.f;
+        emscripten::val view{ emscripten::typed_memory_view(input_size, input_data) };
+        graph_inputs_.set(name, view);
+      }
+
+      graph_outputs_ = emscripten::val::object();
+      for (int t : outputs_) {
+        std::string name = std::to_string(t);
+        auto output_size = context->tensors[t].bytes / 4;
+        auto output_data = context->tensors[t].data.f;
+        emscripten::val view{emscripten::typed_memory_view(output_size, output_data)};
+        graph_outputs_.set(name, view);
+      }
+    }
+
+    wnn_context_.call<void>("computeSync", wnn_graph_, graph_inputs_, graph_outputs_);
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CalculatePadding(TfLiteContext* context,
+                                       TfLitePadding padding, std::string& auto_pad,
+                                       int node_index) {
+    switch (padding) {
+      case kTfLitePaddingSame: {
+        auto_pad = "same-upper";
+        return kTfLiteOk;
+      }
+      case kTfLitePaddingValid:
+        auto_pad = "explicit";
+        return kTfLiteOk;
+      default:
+        TF_LITE_MAYBE_KERNEL_LOG(context,
+                                 "invalid padding mode (%d) in node #%d",
+                                 static_cast<int>(padding), node_index);
+        return kTfLiteError;
+    }
+  }
+
+  static TfLiteStatus CheckConvolutionParams(TfLiteContext* context,
+                                             const TfLiteConvParams* params,
+                                             int node_index) {
+    if (params->stride_width <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride width %d in node #%d",
+                               params->stride_width, node_index);
+      return kTfLiteError;
+    }
+    if (params->stride_height <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride height %d in node #%d",
+                               params->stride_height, node_index);
+      return kTfLiteError;
+    }
+
+    if (params->dilation_width_factor <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "invalid dilation width factor %d in node #%d",
+                               params->dilation_width_factor, node_index);
+      return kTfLiteError;
+    }
+    if (params->dilation_height_factor <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "invalid dilation height factor %d in node #%d",
+                               params->dilation_height_factor, node_index);
+      return kTfLiteError;
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckDepthwiseConvolutionParams(
+      TfLiteContext* context, const TfLiteDepthwiseConvParams* params,
+      int output_channels, int node_index) {
+    if (params->stride_width <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride width %d in node #%d",
+                               params->stride_width, node_index);
+      return kTfLiteError;
+    }
+    if (params->stride_height <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride height %d in node #%d",
+                               params->stride_height, node_index);
+      return kTfLiteError;
+    }
+
+    if (params->depth_multiplier <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "invalid depth multiplier %d in node #%d",
+                               params->depth_multiplier, node_index);
+      return kTfLiteError;
+    }
+    if (output_channels % params->depth_multiplier != 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "depth multiplier %d is incompatible with "
+                               "number of output channels %d in node #%d",
+                               params->depth_multiplier, output_channels,
+                               node_index);
+      return kTfLiteError;
+    }
+
+    if (params->dilation_width_factor <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "invalid dilation width factor %d in node #%d",
+                               params->dilation_width_factor, node_index);
+      return kTfLiteError;
+    }
+    if (params->dilation_height_factor <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "invalid dilation height factor %d in node #%d",
+                               params->dilation_height_factor, node_index);
+      return kTfLiteError;
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckMediaPipeTransposedConvolutionParams(
+      TfLiteContext* context, const TfLiteTransposeConvParams* params,
+      int node_index) {
+    if (params->stride_width <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride width %d in node #%d",
+                               params->stride_width, node_index);
+      return kTfLiteError;
+    }
+    if (params->stride_height <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride height %d in node #%d",
+                               params->stride_height, node_index);
+      return kTfLiteError;
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckFullyConnectedParams(
+      TfLiteContext* context, const TfLiteFullyConnectedParams* params,
+      int node_index) {
+    if (params->weights_format != kTfLiteFullyConnectedWeightsFormatDefault) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context, "unsupported non-default weights format in node #%d",
+          node_index);
+      return kTfLiteError;
+    }
+
+    if (params->asymmetric_quantize_inputs) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context, "unsupported asymmetric quantize inputs in node #%d",
+          node_index);
+      return kTfLiteError;
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckPoolingParams(TfLiteContext* context,
+                                         const TfLitePoolParams* params,
+                                         int node_index) {
+    if (params->stride_width <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride width %d in node #%d",
+                               params->stride_width, node_index);
+      return kTfLiteError;
+    }
+    if (params->stride_height <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride height %d in node #%d",
+                               params->stride_height, node_index);
+      return kTfLiteError;
+    }
+
+    if (params->filter_width <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid filter width %d in node #%d",
+                               params->filter_width, node_index);
+      return kTfLiteError;
+    }
+    if (params->filter_height <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid filter height %d in node #%d",
+                               params->filter_height, node_index);
+      return kTfLiteError;
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckNumInputsAndOutputs(
+      TfLiteContext* context, TfLiteNode* node, int min_num_inputs,
+      int max_num_inputs, int expected_num_outputs, int node_index) {
+    if (node->inputs->size < min_num_inputs ||
+        node->inputs->size > max_num_inputs) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "unexpected number of inputs (%d) in node #%d",
+                               node->inputs->size, node_index);
+      return kTfLiteError;
+    }
+    if (node->outputs->size != expected_num_outputs) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context, "unexpected number of outputs (%d != %d) in node #%d",
+          node->outputs->size, expected_num_outputs, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckNumInputsAndOutputs(TfLiteContext* context,
+                                               TfLiteNode* node,
+                                               int expected_num_inputs,
+                                               int expected_num_outputs,
+                                               int node_index) {
+    if (node->inputs->size != expected_num_inputs) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context, "unexpected number of inputs (%d != %d) in node #%d",
+          node->inputs->size, expected_num_inputs, node_index);
+      return kTfLiteError;
+    }
+    if (node->outputs->size != expected_num_outputs) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context, "unexpected number of outputs (%d != %d) in node #%d",
+          node->outputs->size, expected_num_outputs, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckTensorType(TfLiteContext* context,
+                                      const TfLiteTensor& tensor,
+                                      TfLiteType expected_type,
+                                      int tensor_index, int node_index) {
+    if (tensor.type != expected_type) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context, "unsupported type %s in tensor #%d in node #%d",
+          TfLiteTypeGetName(tensor.type), tensor_index, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckTensorFloat32Type(TfLiteContext* context,
+                                             const TfLiteTensor& tensor,
+                                             int tensor_index, int node_index) {
+    return CheckTensorType(context, tensor, kTfLiteFloat32, tensor_index,
+                           node_index);
+  }
+
+  static TfLiteStatus CheckTensorFloat32OrQInt8Type(TfLiteContext* context,
+                                                    const TfLiteTensor& tensor,
+                                                    int tensor_index,
+                                                    int node_index) {
+    switch (tensor.type) {
+      case kTfLiteFloat32:
+        break;
+      default:
+        TF_LITE_MAYBE_KERNEL_LOG(
+            context, "unsupported type %s in tensor #%d in node #%d",
+            TfLiteTypeGetName(tensor.type), tensor_index, node_index);
+        return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckTensorFloat32OrQInt32Type(TfLiteContext* context,
+                                                     const TfLiteTensor& tensor,
+                                                     int tensor_index,
+                                                     int node_index) {
+    switch (tensor.type) {
+      case kTfLiteFloat32:
+        break;
+      default:
+        TF_LITE_MAYBE_KERNEL_LOG(
+            context, "unsupported type %s in tensor #%d in node #%d",
+            TfLiteTypeGetName(tensor.type), tensor_index, node_index);
+        return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckTensorShape(TfLiteContext* context,
+                                       const TfLiteTensor& tensor,
+                                       int min_num_dims, int max_num_dims,
+                                       int tensor_index) {
+    if (min_num_dims == max_num_dims) {
+      if (tensor.dims->size != min_num_dims) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            context,
+            "unsupported number of shape dimensions (%d) in tensor #%d: "
+            "%d dimensions expected",
+            tensor.dims->size, tensor_index, min_num_dims);
+        return kTfLiteError;
+      }
+    } else {
+      if (tensor.dims->size < min_num_dims) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            context,
+            "unsupported number of shape dimensions (%d) in tensor #%d: "
+            "at least %d dimensions expected",
+            tensor.dims->size, tensor_index, min_num_dims);
+        return kTfLiteError;
+      }
+      if (tensor.dims->size > max_num_dims) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            context,
+            "unsupported number of shape dimensions (%d) in tensor #%d: "
+            "at most %d dimensions expected",
+            tensor.dims->size, tensor_index, max_num_dims);
+        return kTfLiteError;
+      }
+    }
+    for (int i = 0; i < tensor.dims->size; i++) {
+      if (tensor.dims->data[i] <= 0) {
+        TF_LITE_MAYBE_KERNEL_LOG(context,
+                                 "invalid num of elements (%d) in "
+                                 "dimension #%d in tensor #%d",
+                                 tensor.dims->data[i], i, tensor_index);
+        return kTfLiteError;
+      }
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckTensorShape(TfLiteContext* context,
+                                       const TfLiteTensor& tensor,
+                                       int expected_num_dims,
+                                       int tensor_index) {
+    return CheckTensorShape(context, tensor, expected_num_dims,
+                            expected_num_dims, tensor_index);
+  }
+
+  static TfLiteStatus CheckPaddingsTensorShape(TfLiteContext* context,
+                                               const TfLiteTensor& tensor,
+                                               int expected_rows,
+                                               int tensor_index,
+                                               int node_index) {
+    if (tensor.dims->size != 2) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "unexpected number of shape dimensions (%d) in "
+                               "padding tensor #%d in node #%d: "
+                               "expected a 2D tensor",
+                               tensor.dims->size, tensor_index, node_index);
+      return kTfLiteError;
+    }
+    if (tensor.dims->data[0] != expected_rows) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "unexpected number of rows (%d) in "
+                               "padding tensor #%d in node #%d: "
+                               "%d rows expected",
+                               tensor.dims->size, tensor_index, node_index,
+                               expected_rows);
+      return kTfLiteError;
+    }
+    if (tensor.dims->data[1] != 2) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "unexpected number of columns (%d) in "
+                               "padding tensor #%d in node #%d: "
+                               "2 columns expected",
+                               tensor.dims->size, tensor_index, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckAxesTensorShape(TfLiteContext* context,
+                                           const TfLiteTensor& tensor,
+                                           int tensor_index, int node_index) {
+    if (tensor.dims->size != 1) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "unexpected number of shape dimensions (%d) in "
+                               "axes tensor #%d in node #%d: "
+                               "expected a 1D tensor",
+                               tensor.dims->size, tensor_index, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckShapeTensorShape(TfLiteContext* context,
+                                            const TfLiteTensor& tensor,
+                                            int tensor_index, int node_index) {
+    if (tensor.dims->size != 1) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "unexpected number of shape dimensions (%d) in "
+                               "shape tensor #%d in node #%d: "
+                               "expected a 1D tensor",
+                               tensor.dims->size, tensor_index, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckTensorNonDynamicAllocation(
+      TfLiteContext* context, const TfLiteTensor& tensor, int tensor_index,
+      int node_index) {
+    // TODO: remove checks once dynamic tensors are supported
+    if (tensor.allocation_type == kTfLiteDynamic) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context,
+          "invalid allocation type in tensor #%d in node #%d: "
+          "expected non-dynamic tensor",
+          tensor_index, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckTensorStaticAllocation(TfLiteContext* context,
+                                                  const TfLiteTensor& tensor,
+                                                  int tensor_index,
+                                                  int node_index) {
+    if (tensor.allocation_type != kTfLiteMmapRo ||
+        tensor.data.raw_const == nullptr) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context,
+          "invalid allocation type in tensor #%d in node #%d: "
+          "expected static read-only tensor",
+          tensor_index, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static emscripten::val BuildClamp(
+      const emscripten::val& builder, const emscripten::val& input,
+      float min_value, float max_value, std::vector<std::unique_ptr<char>>& constant_buffers) {
+    emscripten::val options = emscripten::val::object();
+    options.set("minValue", min_value);
+    options.set("maxValue", max_value);
+    return builder.call<emscripten::val>("clamp", input, options);
+  }
+
+  static emscripten::val GetClampOperator(
+      const emscripten::val& builder, float min_value, float max_value) {
+    emscripten::val options = emscripten::val::object();
+    options.set("minValue", min_value);
+    options.set("maxValue", max_value);
+    return builder.call<emscripten::val>("clamp", options);
+  }
+
+  static TfLiteStatus GetActivation(
+      const emscripten::val& builder, TfLiteContext* context, int node_index,
+      TfLiteFusedActivation activation, emscripten::val& activation_operator) {
+    switch (activation) {
+      case kTfLiteActRelu:
+        activation_operator = builder.call<emscripten::val>("relu");
+        return kTfLiteOk;
+      case kTfLiteActReluN1To1:
+        activation_operator = GetClampOperator(builder, -1.0f, +1.0f);
+        return kTfLiteOk;
+      case kTfLiteActRelu6:
+        activation_operator = GetClampOperator(builder, 0.0f, 6.0f);
+        return kTfLiteOk;
+      case kTfLiteActTanh:
+        activation_operator = builder.call<emscripten::val>("tanh");
+        return kTfLiteOk;
+      case kTfLiteActSignBit:
+        TF_LITE_MAYBE_KERNEL_LOG(
+            context, "unsupported fused activation (Sign) in node #%d",
+            node_index);
+        return kTfLiteError;
+      case kTfLiteActSigmoid:
+          activation_operator = builder.call<emscripten::val>("sigmoid");
+      default:
+        TF_LITE_MAYBE_KERNEL_LOG(context,
+                                 "invalid fused activation (%d) in node #%d",
+                                 static_cast<int>(activation), node_index);
+        return kTfLiteError;
+    }
+  }
+
+  static TfLiteStatus VisitActivation(
+      const emscripten::val& builder, TfLiteContext* context, int node_index,
+      int input_tensor_id, int output_tensor_id, TfLiteFusedActivation activation,
+      std::unordered_map<int, emscripten::val>& webnn_operands, std::vector<std::unique_ptr<char>>& constant_buffers) {
+    switch (activation) {
+      case kTfLiteActNone:
+        return kTfLiteOk;
+      case kTfLiteActRelu:
+        if (!builder.isNull()) {
+          webnn_operands.at(output_tensor_id) = builder.call<emscripten::val>("relu", webnn_operands.at(input_tensor_id));
+        }
+        return kTfLiteOk;
+      case kTfLiteActReluN1To1:
+        if (!builder.isNull()) {
+          webnn_operands.at(output_tensor_id) = BuildClamp(
+              builder, webnn_operands.at(input_tensor_id), -1.0f, +1.0f, constant_buffers);
+        }
+        return kTfLiteOk;
+      case kTfLiteActRelu6:
+        if (!builder.isNull()) {
+          webnn_operands.at(output_tensor_id) = BuildClamp(
+              builder, webnn_operands.at(input_tensor_id), 0.0f, 6.0f, constant_buffers);
+        }
+        return kTfLiteOk;
+      case kTfLiteActTanh:
+        if (!builder.isNull()) {
+          webnn_operands.at(output_tensor_id) = builder.call<emscripten::val>("tanh", webnn_operands.at(input_tensor_id));
+        }
+        return kTfLiteOk;
+      case kTfLiteActSignBit:
+        TF_LITE_MAYBE_KERNEL_LOG(
+            context, "unsupported fused activation (Sign) in node #%d",
+            node_index);
+        return kTfLiteError;
+      case kTfLiteActSigmoid:
+        if (!builder.isNull()) {
+          webnn_operands.at(output_tensor_id) = builder.call<emscripten::val>("sigmoid", webnn_operands.at(input_tensor_id));
+        }
+        return kTfLiteOk;
+      default:
+        TF_LITE_MAYBE_KERNEL_LOG(context,
+                                 "invalid fused activation (%d) in node #%d",
+                                 static_cast<int>(activation), node_index);
+        return kTfLiteError;
+    }
+  }
+
+  static TfLiteStatus VisitNode(
+      const emscripten::val& builder, TfLiteContext* context,
+      TfLiteRegistration* registration, TfLiteNode* node, int node_index,
+      const std::unordered_set<int>& quasi_static_tensors,
+      std::unordered_map<int, emscripten::val>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    // TFLite context used for logging purposes. When we create a new node
+    // (subgraph is non-null), logging context is the same as context, and error
+    // messages are passed to TFLite. When we detect supported operations
+    // (subgraph is null), logging context is null, and error messages are
+    // supressed.
+    TfLiteContext* logging_context = builder.isNull() ? nullptr : context;
+    switch (registration->builtin_code) {
+      case kTfLiteBuiltinAdd: {
+        const TfLiteAddParams* add_params =
+            static_cast<const TfLiteAddParams*>(node->builtin_data);
+
+        return VisitAddNode(builder, logging_context, node_index, node,
+                            context->tensors, add_params, webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinSub: {
+        const TfLiteSubParams* sub_params =
+            static_cast<const TfLiteSubParams*>(node->builtin_data);
+
+        return VisitSubNode(builder, logging_context, node_index, node,
+                            context->tensors, sub_params, webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinMul: {
+        const TfLiteMulParams* mul_params =
+            static_cast<const TfLiteMulParams*>(node->builtin_data);
+
+        return VisitMulNode(builder, logging_context, node_index, node,
+                            context->tensors, mul_params, webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinPad:
+        return VisitPadNode(builder, logging_context, node_index, node,
+                            context->tensors, webnn_operands, constant_buffers);
+      case kTfLiteBuiltinAveragePool2d: {
+        const TfLitePoolParams* pool_params =
+            static_cast<const TfLitePoolParams*>(node->builtin_data);
+
+        return VisitAveragePool2DNode(builder, logging_context, node_index,
+                                      node, context->tensors, pool_params,
+                                      webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinMaxPool2d: {
+        const TfLitePoolParams* pool_params =
+            static_cast<const TfLitePoolParams*>(node->builtin_data);
+
+        return VisitMaxPool2DNode(builder, logging_context, node_index,
+                                  node, context->tensors, pool_params,
+                                  webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinMean: {
+        const TfLiteReducerParams* reducer_params =
+            static_cast<const TfLiteReducerParams*>(node->builtin_data);
+
+        return VisitMeanNode(builder, logging_context, node_index,
+                             node, context->tensors, reducer_params,
+                             webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinConcatenation: {
+        const TfLiteConcatenationParams* concat_params =
+            static_cast<const TfLiteConcatenationParams*>(node->builtin_data);
+
+        return VisitConcatenationNode(builder, logging_context, node_index, node,
+                                      context->tensors, concat_params,
+                                      webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinConv2d: {
+        const TfLiteConvParams* conv_params =
+            static_cast<const TfLiteConvParams*>(node->builtin_data);
+
+        return VisitConv2DNode(builder, logging_context, node_index, node,
+                               context->tensors, conv_params,
+                               quasi_static_tensors, webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinDepthwiseConv2d: {
+        const TfLiteDepthwiseConvParams* dwconv_params =
+            static_cast<const TfLiteDepthwiseConvParams*>(node->builtin_data);
+
+        return VisitDepthwiseConv2DNode(builder, logging_context, node_index,
+                                        node, context->tensors, dwconv_params,
+                                        quasi_static_tensors, webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinFullyConnected: {
+        const TfLiteFullyConnectedParams* fc_params =
+            static_cast<const TfLiteFullyConnectedParams*>(node->builtin_data);
+
+        return VisitFullyConnectedNode(builder, logging_context, node_index, node,
+                                       context->tensors, fc_params, quasi_static_tensors,
+                                       webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinHardSwish:
+        return VisitHardSwishNode(builder, logging_context, node_index, node,
+                                  context->tensors, webnn_operands);
+      case kTfLiteBuiltinLogistic:
+        return VisitLogisticNode(builder, logging_context, node_index, node,
+                                 context->tensors, webnn_operands);
+      case kTfLiteBuiltinRelu:
+        return VisitReluNode(builder, logging_context, node_index, node,
+                             context->tensors, webnn_operands);
+      case kTfLiteBuiltinReshape: {
+        const TfLiteReshapeParams* reshape_params =
+            static_cast<const TfLiteReshapeParams*>(node->builtin_data);
+
+        return VisitReshapeNode(builder, logging_context, node_index, node,
+                                context->tensors, reshape_params, webnn_operands);
+      }
+      case kTfLiteBuiltinResizeBilinear: {
+        const TfLiteResizeBilinearParams* resize_params =
+            static_cast<const TfLiteResizeBilinearParams*>(node->builtin_data);
+
+        return VisitResizeBilinearNode(builder, logging_context, node_index,
+                                       node, context->tensors, resize_params,
+                                       webnn_operands);
+      }
+      case kTfLiteBuiltinSoftmax: {
+        const TfLiteSoftmaxParams* softmax_params =
+            static_cast<const TfLiteSoftmaxParams*>(node->builtin_data);
+
+        return VisitSoftmaxNode(builder, logging_context, node_index, node,
+                                context->tensors, softmax_params, webnn_operands);
+      }
+      case kTfLiteBuiltinSplit: {
+        const TfLiteSplitParams* split_params =
+            static_cast<const TfLiteSplitParams*>(node->builtin_data);
+
+        return VisitSplitNode(builder, logging_context, node_index, node,
+                                context->tensors, split_params, webnn_operands);
+      }
+      case kTfLiteBuiltinTanh:
+        return VisitTanhNode(builder, logging_context, node_index, node,
+                             context->tensors, webnn_operands);
+      case kTfLiteBuiltinUnpack: {
+        const TfLiteUnpackParams* unpack_params =
+            static_cast<const TfLiteUnpackParams*>(node->builtin_data);
+
+        return VisitUnpackNode(builder, logging_context, node_index, node,
+                               context->tensors, unpack_params, webnn_operands);
+      }
+      case kTfLiteBuiltinCustom: {
+        if (strcmp(registration->custom_name, "Convolution2DTransposeBias") ==
+            0) {
+          TfLiteTransposeConvParams deconv_params = {kTfLitePaddingUnknown};
+          std::memcpy(&deconv_params, node->custom_initial_data,
+                      node->custom_initial_data_size);
+
+          return VisitMediaPipeDeconvolutionNode(
+              builder, context, node_index, node, context->tensors,
+              &deconv_params, quasi_static_tensors, webnn_operands);
+        }
+        return kTfLiteError;
+      }
+      default:
+        return kTfLiteError;
+    }
+  }
+
+  static TfLiteStatus VisitAddNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteAddParams* add_params,
+      std::unordered_map<int, emscripten::val>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, 1, node_index));
+
+    const int input1_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input1_tensor = tensors[input1_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input1_tensor, input1_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input1_tensor, input1_tensor_id, node_index));
+
+    const int input2_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& input2_tensor = tensors[input2_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input2_tensor, input2_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input2_tensor, input2_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    if (!builder.isNull()) {
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input1_tensor_id).as<bool>());
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input2_tensor_id).as<bool>());
+      webnn_operands.insert(std::make_pair(output_tensor_id,
+          builder.call<emscripten::val>("add", webnn_operands.at(input1_tensor_id), webnn_operands.at(input2_tensor_id))));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(output_tensor_id).as<bool>());
+    }
+
+    if (add_params != nullptr) {
+      TF_LITE_ENSURE_STATUS(VisitActivation(
+          builder, logging_context, node_index, output_tensor_id, output_tensor_id,
+          add_params->activation, webnn_operands, constant_buffers));
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitSubNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteSubParams* sub_params,
+      std::unordered_map<int, emscripten::val>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, 1, node_index));
+
+    const int input1_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input1_tensor = tensors[input1_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input1_tensor, input1_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input1_tensor, input1_tensor_id, node_index));
+
+    const int input2_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& input2_tensor = tensors[input2_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input2_tensor, input2_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input2_tensor, input2_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    if (!builder.isNull()) {
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input1_tensor_id).as<bool>());
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input2_tensor_id).as<bool>());
+      webnn_operands.insert(std::make_pair(output_tensor_id,
+          builder.call<emscripten::val>("sub", webnn_operands.at(input1_tensor_id), webnn_operands.at(input2_tensor_id))));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(output_tensor_id).as<bool>());
+    }
+
+    if (sub_params != nullptr) {
+      TF_LITE_ENSURE_STATUS(VisitActivation(
+          builder, logging_context, node_index, output_tensor_id, output_tensor_id,
+          sub_params->activation, webnn_operands, constant_buffers));
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitMulNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteMulParams* mul_params,
+      std::unordered_map<int, emscripten::val>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, 1, node_index));
+
+    const int input1_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input1_tensor = tensors[input1_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input1_tensor, input1_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input1_tensor, input1_tensor_id, node_index));
+
+    const int input2_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& input2_tensor = tensors[input2_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input2_tensor, input2_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input2_tensor, input2_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    if (!builder.isNull()) {
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input1_tensor_id).as<bool>());
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input2_tensor_id).as<bool>());
+      webnn_operands.insert(std::make_pair(output_tensor_id,
+          builder.call<emscripten::val>("mul", webnn_operands.at(input1_tensor_id), webnn_operands.at(input2_tensor_id))));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(output_tensor_id).as<bool>());
+    }
+
+    if (mul_params != nullptr) {
+      TF_LITE_ENSURE_STATUS(VisitActivation(
+          builder, logging_context, node_index, output_tensor_id, output_tensor_id,
+          mul_params->activation, webnn_operands, constant_buffers));
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitPadNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      std::unordered_map<int, emscripten::val>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int padding_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& paddings_tensor = tensors[padding_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorType(logging_context, paddings_tensor,
+                                          kTfLiteInt32, padding_tensor_id,
+                                          node_index));
+    TF_LITE_ENSURE_STATUS(CheckPaddingsTensorShape(
+        logging_context, paddings_tensor, input_tensor.dims->size,
+        padding_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+        logging_context, paddings_tensor, padding_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    const int32_t* paddings_data =
+        reinterpret_cast<const int32_t*>(paddings_tensor.data.data);
+    for (int i = 0; i < paddings_tensor.dims->data[0]; i++) {
+      const int32_t pre_padding = paddings_data[i * 2 + 0];
+      if (pre_padding < 0) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            logging_context,
+            "invalid pre-padding %d for dimension #%d in node %d", pre_padding,
+            i, node_index);
+        return kTfLiteError;
+      }
+
+      const int32_t post_padding = paddings_data[i * 2 + 1];
+      if (post_padding < 0) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            logging_context,
+            "invalid post-padding %d for dimension #%d in node %d", pre_padding,
+            i, node_index);
+        return kTfLiteError;
+      }
+    }
+
+    if (!builder.isNull()) {
+      size_t rank = paddings_tensor.dims->data[0];
+      std::vector<int32_t> padding(rank * 2);
+      for (int i = 0; i < rank; i++) {
+        padding[i * 2 + 0] = static_cast<int32_t>(paddings_data[i * 2 + 0]);
+        padding[i * 2 + 1] = static_cast<int32_t>(paddings_data[i * 2 + 1]);
+      }
+      const size_t padding_buffer_length = sizeof(int32_t) * padding.size();
+      std::unique_ptr<char> padding_buffer(new char[padding_buffer_length]);
+      std::memcpy(padding_buffer.get(), padding.data(), padding_buffer_length);
+      std::vector<int32_t> dims = {static_cast<int32_t>(rank), 2};
+      emscripten::val desc = emscripten::val::object();
+      desc.set("type", emscripten::val("int32"));
+      desc.set("dimensions", emscripten::val::array(dims));
+
+      emscripten::val view{ emscripten::typed_memory_view(padding.size(), padding.data()) };
+      emscripten::val padding_operand = builder.call<emscripten::val>("constant", desc, view);
+      constant_buffers.push_back(std::move(padding_buffer));
+
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input_tensor_id).as<bool>());
+      webnn_operands.insert(std::make_pair(output_tensor_id,
+          builder.call<emscripten::val>("pad", webnn_operands.at(input_tensor_id), padding_operand)));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(output_tensor_id).as<bool>());
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitAveragePool2DNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLitePoolParams* pool_params,
+      std::unordered_map<int, emscripten::val>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    TF_LITE_ENSURE_STATUS(
+        CheckPoolingParams(logging_context, pool_params, node_index));
+
+    std::string auto_pad;
+    TF_LITE_ENSURE_STATUS(CalculatePadding(
+        logging_context, pool_params->padding, auto_pad, node_index));
+
+    if (!builder.isNull()) {
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input_tensor_id).as<bool>());
+      if (pool_params->filter_height == 1 && pool_params->filter_width == 1) {
+        // Only do activation.
+        webnn_operands.insert(std::make_pair(output_tensor_id, webnn_operands.at(input_tensor_id)));
+      } else {
+        std::vector<int32_t> strides = {
+            pool_params->stride_height, pool_params->stride_width};
+        std::vector<int32_t> windowDimensions = {
+            pool_params->filter_height, pool_params->filter_width};
+
+        emscripten::val options = emscripten::val::object();
+        options.set("autoPad", emscripten::val(auto_pad));
+        options.set("strides", emscripten::val::array(strides));
+        options.set("windowDimensions", emscripten::val::array(windowDimensions));
+        options.set("layout", emscripten::val("nhwc"));
+        webnn_operands.insert(std::make_pair(
+            output_tensor_id,
+            builder.call<emscripten::val>("averagePool2d",
+                                           webnn_operands.at(input_tensor_id),
+                                           options)));
+      }
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(output_tensor_id).as<bool>());
+    }
+
+    TF_LITE_ENSURE_STATUS(VisitActivation(
+          builder, logging_context, node_index, output_tensor_id, output_tensor_id,
+          pool_params->activation, webnn_operands, constant_buffers));
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitMaxPool2DNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLitePoolParams* pool_params,
+      std::unordered_map<int, emscripten::val>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    TF_LITE_ENSURE_STATUS(
+        CheckPoolingParams(logging_context, pool_params, node_index));
+
+    std::string auto_pad;
+    TF_LITE_ENSURE_STATUS(CalculatePadding(
+        logging_context, pool_params->padding, auto_pad, node_index));
+
+    if (!builder.isNull()) {
+      std::vector<int32_t> strides = {
+          pool_params->stride_height, pool_params->stride_width};
+      std::vector<int32_t> windowDimensions = {
+          pool_params->filter_height, pool_params->filter_width};
+
+      emscripten::val options = emscripten::val::object();
+      options.set("autoPad", emscripten::val(auto_pad));
+      options.set("strides", emscripten::val::array(strides));
+      options.set("windowDimensions", emscripten::val::array(windowDimensions));
+      options.set("layout", emscripten::val("nhwc"));
+
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input_tensor_id).as<bool>());
+      webnn_operands.insert(std::make_pair(
+            output_tensor_id,
+            builder.call<emscripten::val>("maxPool2d",
+                                           webnn_operands.at(input_tensor_id),
+                                           options)));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(output_tensor_id).as<bool>());
+    }
+
+    TF_LITE_ENSURE_STATUS(VisitActivation(
+          builder, logging_context, node_index, output_tensor_id, output_tensor_id,
+          pool_params->activation, webnn_operands, constant_buffers));
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitMeanNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteReducerParams* reducer_params,
+      std::unordered_map<int, emscripten::val>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, input_tensor, 4,
+                                           input_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int axes_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& axes_tensor = tensors[axes_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorType(logging_context, axes_tensor,
+                                          kTfLiteInt32, axes_tensor_id,
+                                          node_index));
+    TF_LITE_ENSURE_STATUS(CheckAxesTensorShape(
+        logging_context, axes_tensor, axes_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+        logging_context, axes_tensor, axes_tensor_id, node_index));
+
+    if (axes_tensor.dims->data[0] != 2) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unsupported MEAN reduction along %d axes in node %d",
+          axes_tensor.dims->data[0], node_index);
+      return kTfLiteError;
+    }
+
+    const int32_t* axes_data =
+        reinterpret_cast<const int32_t*>(axes_tensor.data.data);
+    if (std::min(axes_data[0], axes_data[1]) != 1 ||
+        std::max(axes_data[0], axes_data[1]) != 2) {
+      TF_LITE_MAYBE_KERNEL_LOG(logging_context,
+                               "unsupported MEAN reduction along non-spatial "
+                               "axes %d and %d in node %d",
+                               std::min(axes_data[0], axes_data[1]),
+                               std::max(axes_data[0], axes_data[1]),
+                               node_index);
+      return kTfLiteError;
+    }
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    const int expected_output_dims = reducer_params->keep_dims ? 4 : 2;
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor,
+                                           expected_output_dims,
+                                           output_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    if (!builder.isNull()) {
+      emscripten::val reduceOptions = emscripten::val::object();
+      std::vector<int32_t> axes;
+      axes.assign(&axes_data[0], &axes_data[0] + axes_tensor.dims->data[0]);
+      reduceOptions.set("axes", emscripten::val::array(axes));
+      reduceOptions.set("keepDimensions", emscripten::val(reducer_params->keep_dims));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input_tensor_id).as<bool>());
+
+      webnn_operands.insert(std::make_pair(
+          output_tensor_id,
+          builder.call<emscripten::val>("reduceMean",
+                                        webnn_operands.at(input_tensor_id),
+                                        reduceOptions)));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(output_tensor_id).as<bool>());
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitConcatenationNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteConcatenationParams* concat_params,
+      std::unordered_map<int, emscripten::val>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    size_t input_size = node->inputs->size;
+    const TfLiteTensor& first_input_tensor = tensors[node->inputs->data[0]];
+    uint32_t axis = concat_params->axis < 0
+                     ? first_input_tensor.dims->size + concat_params->axis
+                     : concat_params->axis;
+    if (!builder.isNull()) {
+      emscripten::val input_operands = emscripten::val::array();
+      for (size_t i = 0; i < input_size; ++i) {
+        TF_LITE_ENSURE(logging_context, webnn_operands.at(node->inputs->data[i]).as<bool>());
+        input_operands.call<void>("push", webnn_operands.at(node->inputs->data[i]));
+      }
+      webnn_operands.insert(
+          std::make_pair(node->outputs->data[0],
+                         builder.call<emscripten::val>("concat", input_operands,
+                                                       emscripten::val(axis))));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(node->outputs->data[0]).as<bool>());
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitConv2DNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteConvParams* conv_params,
+      const std::unordered_set<int>& quasi_static_tensors,
+      std::unordered_map<int, emscripten::val>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckConvolutionParams(logging_context, conv_params, node_index));
+
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 3, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, input_tensor, 4,
+                                           input_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int filter_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& filter_tensor = tensors[filter_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, filter_tensor, filter_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, filter_tensor, 4,
+                                           filter_tensor_id));
+    if (quasi_static_tensors.count(filter_tensor_id) == 0) {
+      TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+          logging_context, filter_tensor, filter_tensor_id, node_index));
+    }
+
+    const int bias_tensor_id = node->inputs->data[2];
+    // bias_tensor_id < 0 means without bias.
+    if (bias_tensor_id >= 0) {
+      const TfLiteTensor& bias_tensor = tensors[bias_tensor_id];
+      TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt32Type(
+          logging_context, bias_tensor, node->inputs->data[2], node_index));
+      TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, bias_tensor, 1,
+                                            node->inputs->data[2]));
+      if (quasi_static_tensors.count(node->inputs->data[2]) == 0) {
+        TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+            logging_context, bias_tensor, node->inputs->data[2], node_index));
+      }
+    }
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor, 4,
+                                           output_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    std::string auto_pad;
+    TF_LITE_ENSURE_STATUS(CalculatePadding(
+        logging_context, conv_params->padding, auto_pad, node_index));
+
+    if (!builder.isNull()) {
+      std::vector<int32_t> strides = {
+          conv_params->stride_height, conv_params->stride_width};
+      std::vector<int32_t> dilations = {
+          conv_params->dilation_height_factor, conv_params->dilation_width_factor};
+
+      emscripten::val options = emscripten::val::object();
+      options.set("autoPad", emscripten::val(auto_pad));
+      options.set("strides", emscripten::val::array(strides));
+      options.set("dilations", emscripten::val::array(dilations));
+      options.set("inputLayout", emscripten::val("nhwc"));
+      options.set("filterLayout", emscripten::val("ohwi"));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input_tensor_id).as<bool>());
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(filter_tensor_id).as<bool>());
+      if (bias_tensor_id >= 0) {
+        TF_LITE_ENSURE(logging_context, webnn_operands.at(bias_tensor_id).as<bool>());
+        options.set("bias", webnn_operands.at(bias_tensor_id));
+      }
+
+      emscripten::val activation_operator = emscripten::val::object();
+      if (conv_params->activation != kTfLiteActNone) {
+        TF_LITE_ENSURE_STATUS(GetActivation(builder, logging_context, node_index,
+            conv_params->activation, activation_operator));
+        options.set("activation", activation_operator);
+      }
+      emscripten::val output = builder.call<emscripten::val>("conv2d",
+          webnn_operands.at(input_tensor_id), webnn_operands.at(filter_tensor_id), options);
+      webnn_operands.insert(std::make_pair(output_tensor_id, output));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(output_tensor_id).as<bool>());
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitMediaPipeDeconvolutionNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteTransposeConvParams* deconv_params,
+      const std::unordered_set<int>& quasi_static_tensors,
+      std::unordered_map<int, emscripten::val>& webnn_operands) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 3, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, input_tensor, 4,
+                                           input_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int filter_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& filter_tensor = tensors[filter_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, filter_tensor, filter_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, filter_tensor, 4,
+                                           filter_tensor_id));
+    if (quasi_static_tensors.count(filter_tensor_id) == 0) {
+      TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+          logging_context, filter_tensor, filter_tensor_id, node_index));
+    }
+
+    const int bias_tensor_id = node->inputs->data[2];
+    const TfLiteTensor& bias_tensor = tensors[bias_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, bias_tensor, bias_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, bias_tensor, 1,
+                                           bias_tensor_id));
+    if (quasi_static_tensors.count(bias_tensor_id) == 0) {
+      TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+          logging_context, bias_tensor, bias_tensor_id, node_index));
+    }
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor, 4,
+                                           output_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    const int output_channels = filter_tensor.dims->data[0];
+    const int kernel_height = filter_tensor.dims->data[1];
+    const int kernel_width = filter_tensor.dims->data[2];
+    const int input_channels = filter_tensor.dims->data[3];
+
+    TF_LITE_ENSURE_STATUS(CheckMediaPipeTransposedConvolutionParams(
+        logging_context, deconv_params, node_index));
+
+    std::string auto_pad;
+    TF_LITE_ENSURE_STATUS(CalculatePadding(
+        logging_context, deconv_params->padding, auto_pad, node_index));
+
+    if (!builder.isNull()) {
+      emscripten::val options = emscripten::val::object();
+      options.set("autoPad", emscripten::val(auto_pad));
+      std::vector<int32_t> strides = {
+          deconv_params->stride_height, deconv_params->stride_width};
+      options.set("strides", emscripten::val::array(strides));
+      options.set("inputLayout", emscripten::val("nhwc"));
+      options.set("filterLayout", emscripten::val("ohwi"));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input_tensor_id).as<bool>());
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(filter_tensor_id).as<bool>());
+      if (bias_tensor_id >= 0) {
+        TF_LITE_ENSURE(logging_context, webnn_operands.at(bias_tensor_id).as<bool>());
+        options.set("bias", webnn_operands.at(bias_tensor_id));
+      }
+      emscripten::val output = builder.call<emscripten::val>("convTranspose2d",
+          webnn_operands.at(input_tensor_id), webnn_operands.at(filter_tensor_id), options);
+      webnn_operands.insert(std::make_pair(output_tensor_id, output));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(output_tensor_id).as<bool>());
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitDepthwiseConv2DNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteDepthwiseConvParams* dwconv_params,
+      const std::unordered_set<int>& quasi_static_tensors,
+      std::unordered_map<int, emscripten::val>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 3, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, input_tensor, 4,
+                                           input_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int filter_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& filter_tensor = tensors[filter_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, filter_tensor, filter_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, filter_tensor, 4,
+                                           filter_tensor_id));
+    if (quasi_static_tensors.count(filter_tensor_id) == 0) {
+      TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+          logging_context, filter_tensor, filter_tensor_id, node_index));
+    }
+
+    const int bias_tensor_id = node->inputs->data[2];
+    // bias_tensor_id < 0 means without bias.
+    if (bias_tensor_id >= 0) {
+      const TfLiteTensor& bias_tensor = tensors[bias_tensor_id];
+      TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt32Type(
+          logging_context, bias_tensor, node->inputs->data[2], node_index));
+      TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, bias_tensor, 1,
+                                            node->inputs->data[2]));
+      if (quasi_static_tensors.count(node->inputs->data[2]) == 0) {
+        TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+            logging_context, bias_tensor, node->inputs->data[2], node_index));
+      }
+    }
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor, 4,
+                                           output_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    const int output_channels = filter_tensor.dims->data[3];
+    TF_LITE_ENSURE_STATUS(CheckDepthwiseConvolutionParams(
+        logging_context, dwconv_params, output_channels, node_index));
+
+    std::string auto_pad;
+    TF_LITE_ENSURE_STATUS(CalculatePadding(
+        logging_context, dwconv_params->padding, auto_pad, node_index));
+
+    if (!builder.isNull()) {
+      std::vector<int32_t> strides = {
+          dwconv_params->stride_height, dwconv_params->stride_width};
+      std::vector<int32_t> dilations = {
+          dwconv_params->dilation_height_factor, dwconv_params->dilation_width_factor};
+
+      emscripten::val options = emscripten::val::object();
+      options.set("autoPad", emscripten::val(auto_pad));
+      options.set("strides", emscripten::val::array(strides));
+      options.set("dilations", emscripten::val::array(dilations));
+      options.set("inputLayout", emscripten::val("nhwc"));
+      options.set("filterLayout", emscripten::val("ihwo"));
+      options.set("groups", emscripten::val(output_channels / dwconv_params->depth_multiplier));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input_tensor_id).as<bool>());
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(filter_tensor_id).as<bool>());
+      if (bias_tensor_id >= 0) {
+        TF_LITE_ENSURE(logging_context, webnn_operands.at(bias_tensor_id).as<bool>());
+        options.set("bias", webnn_operands.at(bias_tensor_id));
+      }
+
+      emscripten::val activation_operator = emscripten::val::object();
+      if (dwconv_params->activation != kTfLiteActNone) {
+        TF_LITE_ENSURE_STATUS(GetActivation(builder, logging_context, node_index,
+            dwconv_params->activation, activation_operator));
+        options.set("activation", activation_operator);
+      }
+      emscripten::val output = builder.call<emscripten::val>("conv2d",
+          webnn_operands.at(input_tensor_id), webnn_operands.at(filter_tensor_id), options);
+      webnn_operands.insert(std::make_pair(output_tensor_id, output));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(output_tensor_id).as<bool>());
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitFullyConnectedNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteFullyConnectedParams* fc_params,
+      const std::unordered_set<int>& quasi_static_tensors,
+      std::unordered_map<int, emscripten::val>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckFullyConnectedParams(logging_context, fc_params, node_index));
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, 3, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int filter_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& filter_tensor = tensors[filter_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, filter_tensor, filter_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, filter_tensor, 2,
+                                           filter_tensor_id));
+    if (quasi_static_tensors.count(filter_tensor_id) == 0) {
+      TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+          logging_context, filter_tensor, filter_tensor_id, node_index));
+    }
+
+    int bias_tensor_id = -1;
+    if (node->inputs->size >= 3) {
+      bias_tensor_id = node->inputs->data[2];
+      if (bias_tensor_id >= 0) {
+        const TfLiteTensor& bias_tensor = tensors[bias_tensor_id];
+        TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt32Type(
+            logging_context, bias_tensor, bias_tensor_id, node_index));
+        TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, bias_tensor, 1,
+                                               bias_tensor_id));
+        if (quasi_static_tensors.count(bias_tensor_id) == 0) {
+          TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+              logging_context, bias_tensor, bias_tensor_id, node_index));
+        }
+      }
+    }
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    const int32_t output_channels = filter_tensor.dims->data[0];
+    const int32_t input_channels = filter_tensor.dims->data[1];
+
+    if (input_tensor.dims->size == 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unexpected number of shape dimensions %d in tensor #%d",
+          input_tensor.dims->size, input_tensor_id);
+      return kTfLiteError;
+    }
+    const int32_t * input_dims_data = input_tensor.dims->data;
+    int32_t num_input_elements = 1;
+    for (int i = 0; i < input_tensor.dims->size; i++) {
+      if (input_dims_data[i] <= 0) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            logging_context, "invalid dimension #%d (%d) in tensor #%d", i,
+            input_dims_data[i], input_tensor_id);
+        return kTfLiteError;
+      }
+      num_input_elements *= input_dims_data[i];
+    }
+
+    if (fc_params->keep_num_dims) {
+      TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor,
+                                             input_tensor.dims->size,
+                                             output_tensor_id));
+
+      for (int i = 0; i < input_tensor.dims->size - 1; i++) {
+        if (input_dims_data[i] != output_tensor.dims->data[i]) {
+          TF_LITE_MAYBE_KERNEL_LOG(
+              logging_context,
+              "mismatch in shape dimension %d (%d != %d) in input and output "
+              "tensors of FULLY_CONNECTED operator #%d",
+              i, input_dims_data[i], output_tensor.dims->data[i],
+              node_index);
+          return kTfLiteError;
+        }
+      }
+    } else {
+      if (num_input_elements % input_channels != 0) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            logging_context,
+            "number of elements in input tensor #%d in FULLY_CONNECTED "
+            "operator is not divisible by input channels (%d)",
+            input_tensor_id, input_channels);
+        return kTfLiteError;
+      }
+
+      TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor, 2,
+                                             output_tensor_id));
+
+      if (output_tensor.dims->data[0] != num_input_elements / input_channels) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            logging_context,
+            "batch size %d in output tensor #%d in FULLY_CONNECTED operator "
+            "does not match batch size %d in reshaped input tensor #%d",
+            output_tensor.dims->data[0], output_tensor_id,
+            num_input_elements / input_channels, input_tensor_id);
+        return kTfLiteError;
+      }
+    }
+
+    if (output_tensor.dims->data[output_tensor.dims->size - 1] !=
+        output_channels) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "number of channels %d in output tensor #%d does not match output "
+          "channels %d in filter tensor #%d",
+          output_tensor.dims->data[output_tensor.dims->size - 1],
+          output_tensor_id, output_channels, filter_tensor_id);
+      return kTfLiteError;
+    }
+
+    if (!builder.isNull()) {
+      emscripten::val options = emscripten::val::object();
+      options.set("aTranspose", emscripten::val(false));
+      options.set("bTranspose",  emscripten::val(true));
+      if (bias_tensor_id >= 0) {
+        TF_LITE_ENSURE(logging_context, webnn_operands.at(bias_tensor_id).as<bool>());
+        options.set("c", webnn_operands.at(bias_tensor_id));
+      }
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input_tensor_id).as<bool>());
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(filter_tensor_id).as<bool>());
+      if (fc_params->keep_num_dims || input_tensor.dims->size != 2) {
+        // Reshape input to 2D tensor
+        const int32_t n_inputs = input_channels;
+        std::vector<int32_t> new_input_shape = {-1, n_inputs};
+        emscripten::val reshaped_input = builder.call<emscripten::val>(
+            "reshape", webnn_operands.at(input_tensor_id),
+            emscripten::val::array(new_input_shape));
+        emscripten::val gemm = builder.call<emscripten::val>(
+            "gemm", reshaped_input, webnn_operands.at(filter_tensor_id),
+            options);
+
+        std::vector<int> newShape;
+        newShape.assign(
+            &output_tensor.dims->data[0],
+            &output_tensor.dims->data[0] + output_tensor.dims->size);
+        webnn_operands.insert(std::make_pair(
+            output_tensor_id,
+            builder.call<emscripten::val>("reshape", gemm,
+                                          emscripten::val::array(newShape))));
+      } else {
+        webnn_operands.insert(
+            std::make_pair(output_tensor_id,
+                           builder.call<emscripten::val>(
+                               "gemm", webnn_operands.at(input_tensor_id),
+                               webnn_operands.at(filter_tensor_id), options)));
+      }
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(output_tensor_id).as<bool>());
+    }
+
+    TF_LITE_ENSURE_STATUS(VisitActivation(
+        builder, logging_context, node_index, output_tensor_id, output_tensor_id,
+        fc_params->activation, webnn_operands, constant_buffers));
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitHardSwishNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      std::unordered_map<int, emscripten::val>& webnn_operands) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, 1, node_index));
+
+    const TfLiteTensor& input_tensor = tensors[node->inputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+
+    const TfLiteTensor& output_tensor = tensors[node->outputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+
+    if (!builder.isNull()) {
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(node->inputs->data[0]).as<bool>());
+      webnn_operands.insert(std::make_pair(
+          node->outputs->data[0], builder.call<emscripten::val>(
+                                "hardSwish", webnn_operands.at(node->inputs->data[0]))));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(node->outputs->data[0]).as<bool>());
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitLogisticNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      std::unordered_map<int, emscripten::val>& webnn_operands) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, 1, node_index));
+
+    const TfLiteTensor& input_tensor = tensors[node->inputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+
+    const TfLiteTensor& output_tensor = tensors[node->outputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+
+    if (!builder.isNull()) {
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(node->inputs->data[0]).as<bool>());
+      webnn_operands.insert(std::make_pair(
+          node->outputs->data[0], builder.call<emscripten::val>(
+                                "sigmoid", webnn_operands.at(node->inputs->data[0]))));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(node->outputs->data[0]).as<bool>());
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitReluNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      std::unordered_map<int, emscripten::val>& webnn_operands) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, 1, node_index));
+
+    const TfLiteTensor& input_tensor = tensors[node->inputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+
+    const TfLiteTensor& output_tensor = tensors[node->outputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+
+    if (!builder.isNull()) {
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(node->inputs->data[0]).as<bool>());
+      webnn_operands.insert(std::make_pair(
+          node->outputs->data[0], builder.call<emscripten::val>(
+                                "relu", webnn_operands.at(node->inputs->data[0]))));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(node->outputs->data[0]).as<bool>());
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitReshapeNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteReshapeParams* reshape_params,
+      std::unordered_map<int, emscripten::val>& webnn_operands) {
+    switch (node->inputs->size) {
+      case 1:
+      case 2:
+        break;
+      default:
+        TF_LITE_MAYBE_KERNEL_LOG(
+            logging_context,
+            "unexpected number of inputs (%d) in node #%d: "
+            "either one or two inputs expected",
+            node->inputs->size, node_index);
+        return kTfLiteError;
+    }
+    if (node->outputs->size != 1) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unexpected number of outputs (%d) in node #%d: one output expected",
+          node->outputs->size, node_index);
+      return kTfLiteError;
+    }
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    if (node->inputs->size == 2) {
+      const int shape_tensor_id = node->inputs->data[1];
+      const TfLiteTensor& shape_tensor = tensors[shape_tensor_id];
+      TF_LITE_ENSURE_STATUS(CheckTensorType(logging_context, shape_tensor,
+                                            kTfLiteInt32, shape_tensor_id,
+                                            node_index));
+      TF_LITE_ENSURE_STATUS(CheckShapeTensorShape(
+          logging_context, shape_tensor, shape_tensor_id, node_index));
+      TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+          logging_context, shape_tensor,shape_tensor_id, node_index));
+    }
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    if (!builder.isNull()) {
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input_tensor_id).as<bool>());
+      std::vector<int> newShape;
+      newShape.assign(&output_tensor.dims->data[0],
+                      &output_tensor.dims->data[0] + output_tensor.dims->size);
+      webnn_operands.insert(std::make_pair(
+          output_tensor_id, builder.call<emscripten::val>(
+                                "reshape", webnn_operands.at(input_tensor_id),
+                                emscripten::val::array(newShape))));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(output_tensor_id).as<bool>());
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitResizeBilinearNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteResizeBilinearParams* resize_params,
+      std::unordered_map<int, emscripten::val>& webnn_operands) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, input_tensor, 4,
+                                           input_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int shape_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& shape_tensor = tensors[shape_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorType(logging_context, shape_tensor,
+                                          kTfLiteInt32, shape_tensor_id,
+                                          node_index));
+    TF_LITE_ENSURE_STATUS(CheckShapeTensorShape(
+        logging_context, shape_tensor, shape_tensor_id, node_index));
+    if (shape_tensor.dims->data[0] != 2) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unexpected number of dimensions %d in the output shape in node %d",
+          shape_tensor.dims->data[0], node_index);
+    }
+    TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+        logging_context, shape_tensor, shape_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor, 4,
+                                           output_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    const int32_t* shape_data =
+        reinterpret_cast<const int32_t*>(shape_tensor.data.data);
+    for (int i = 0; i < shape_tensor.dims->data[0]; i++) {
+      const int32_t dim = shape_data[i];
+      if (dim <= 0) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            logging_context, "invalid output dimension #%d value %d in node %d",
+            i, dim, node_index);
+        return kTfLiteError;
+      }
+    }
+
+    if (!builder.isNull()) {
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input_tensor_id).as<bool>());
+      std::vector<int32_t> sizes = {shape_data[0], shape_data[1]};
+      std::vector<int32_t> axes = {1, 2};
+      emscripten::val options = emscripten::val::object();
+      options.set("mode", emscripten::val("linear"));
+      options.set("sizes", emscripten::val::array(sizes));
+      options.set("axes", emscripten::val::array(axes));
+
+      webnn_operands.insert(std::make_pair(
+          output_tensor_id,
+          builder.call<emscripten::val>(
+              "resample2d", webnn_operands.at(input_tensor_id), options)));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(output_tensor_id).as<bool>());
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitSoftmaxNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteSoftmaxParams* params,
+      std::unordered_map<int, emscripten::val>& webnn_operands) {
+    if (params->beta != 1.0f) {
+      if (logging_context != nullptr) {
+        TF_LITE_KERNEL_LOG(logging_context,
+                           "unsupported beta value %.7f in SOFTMAX node #%d",
+                           params->beta, node_index);
+      }
+      return kTfLiteError;
+    }
+
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    if (!builder.isNull()) {
+      TF_LITE_ENSURE(logging_context,
+                     webnn_operands.at(input_tensor_id).as<bool>());
+      webnn_operands.insert(
+          std::make_pair(output_tensor_id,
+                         builder.call<emscripten::val>(
+                             "softmax", webnn_operands.at(input_tensor_id))));
+      TF_LITE_ENSURE(logging_context,
+                     webnn_operands.at(output_tensor_id).as<bool>());
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitSplitNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteSplitParams* params,
+      std::unordered_map<int, emscripten::val>& webnn_operands) {
+    const int num_splits = params->num_splits;
+    if (num_splits == 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unexpected value of num_splits %d in the split params in node %d",
+          num_splits, node_index);
+      return kTfLiteError;
+    }
+
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, num_splits, node_index));
+
+    const int input_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int axis_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& axis_tensor = tensors[axis_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorType(logging_context, axis_tensor,
+                                          kTfLiteInt32, axis_tensor_id,
+                                          node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, axis_tensor, 0,
+                                           axis_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+        logging_context, axis_tensor, axis_tensor_id, node_index));
+
+    const int* axis_data =
+        reinterpret_cast<const int*>(axis_tensor.data.data);
+    int axis_value = axis_data[0];
+
+    const int num_dims = input_tensor.dims->size;
+    if (axis_value < 0) {
+      axis_value += num_dims;
+    }
+    if (axis_value < 0 || axis_value > num_dims) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unexpected data of axis %d in the axis tensor in node %d",
+          axis_data[0], node_index);
+      return kTfLiteError;
+    }
+    const int input_size = input_tensor.dims->data[axis_value];
+    if (input_size % num_splits != 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "Not an even split");
+      return kTfLiteError;
+    }
+
+    const int output_size = node->outputs->size;
+
+    if (!builder.isNull()) {
+      std::vector<uint32_t> splits = {static_cast<const uint32_t>(num_splits)};
+      emscripten::val options = emscripten::val::object();
+      options.set("axis", axis_value);
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input_tensor_id).as<bool>());
+      emscripten::val split_operand_array = builder.call<emscripten::val>(
+          "split", webnn_operands.at(input_tensor_id),
+          emscripten::val::array(splits), options);
+      TF_LITE_ENSURE(
+          logging_context,
+          split_operand_array["length"].as<int32_t>() == output_size);
+
+      for (int i = 0; i < output_size; i++) {
+        int output_tensor_id = node->outputs->data[i];
+        const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+        TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+            logging_context, output_tensor, output_tensor_id, node_index));
+        TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+            logging_context, output_tensor, output_tensor_id, node_index));
+        webnn_operands.insert(
+            std::make_pair(output_tensor_id, split_operand_array[i]));
+        TF_LITE_ENSURE(logging_context, webnn_operands.at(output_tensor_id).as<bool>());
+      }
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitTanhNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      std::unordered_map<int, emscripten::val>& webnn_operands) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, 1, node_index));
+
+    const TfLiteTensor& input_tensor = tensors[node->inputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+
+    const TfLiteTensor& output_tensor = tensors[node->outputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+
+    if (!builder.isNull()) {
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(node->inputs->data[0]).as<bool>());
+      webnn_operands.insert(std::make_pair(
+          node->outputs->data[0], builder.call<emscripten::val>(
+                                "tanh", webnn_operands.at(node->inputs->data[0]))));
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(node->outputs->data[0]).as<bool>());
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitUnpackNode(
+      const emscripten::val& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteUnpackParams* params,
+      std::unordered_map<int, emscripten::val>& webnn_operands) {
+    const int num = params->num;
+    int axis = params->axis;
+
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, num, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int num_dims = input_tensor.dims->size;
+    if (axis < 0) {
+      axis += num_dims;
+    }
+    if (axis < 0 || axis >= num_dims) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unexpected value of axis %d in the unpack params in node %d",
+          axis, node_index);
+      return kTfLiteError;
+    }
+
+    const int output_size = node->outputs->size;
+
+    if (num != output_size) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unexpected value of num %d in the unpack params in node %d",
+          num, node_index);
+      return kTfLiteError;
+    }
+
+    if (!builder.isNull()) {
+      TF_LITE_ENSURE(logging_context, webnn_operands.at(input_tensor_id).as<bool>());
+      emscripten::val squeeze_options = emscripten::val::object();
+      std::vector<int32_t> axes = {static_cast<int32_t>(axis)};
+      squeeze_options.set("axes", emscripten::val::array(axes));
+      // Unpack = split + squeeze in WebNN
+      // No need split if Unpack's num == 1
+      if (num == 1) {
+        int output_tensor_id = node->outputs->data[0];
+        const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+        TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+            logging_context, output_tensor, output_tensor_id, node_index));
+        TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+            logging_context, output_tensor, output_tensor_id, node_index));
+
+        webnn_operands.insert(std::make_pair(
+            output_tensor_id, builder.call<emscripten::val>(
+                                  "squeeze", webnn_operands.at(input_tensor_id),
+                                  squeeze_options)));
+        TF_LITE_ENSURE(logging_context,
+                       webnn_operands.at(output_tensor_id).as<bool>());
+      } else {
+        std::vector<uint32_t> splits = {static_cast<const uint32_t>(num)};
+        emscripten::val options = emscripten::val::object();
+        options.set("axis", axis);
+        emscripten::val split_operand_array = builder.call<emscripten::val>(
+            "split", webnn_operands.at(input_tensor_id),
+            emscripten::val::array(splits), options);
+        TF_LITE_ENSURE(
+            logging_context,
+            split_operand_array["length"].as<int32_t>() == output_size);
+
+        for (int i = 0; i < output_size; i++) {
+          int output_tensor_id = node->outputs->data[i];
+          const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+          TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+              logging_context, output_tensor, output_tensor_id, node_index));
+          TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+              logging_context, output_tensor, output_tensor_id, node_index));
+
+          webnn_operands.insert(std::make_pair(
+              output_tensor_id,
+              builder.call<emscripten::val>("squeeze", split_operand_array[i],
+                                            squeeze_options)));
+          TF_LITE_ENSURE(logging_context,
+                         webnn_operands.at(output_tensor_id).as<bool>());
+        }
+      }
+    }
+    return kTfLiteOk;
+  }
+
+ private:
+  Subgraph(emscripten::val context, emscripten::val graph, std::unordered_set<int>&& inputs, std::unordered_set<int>&& outputs)
+      : wnn_context_(context), wnn_graph_(graph), inputs_(inputs), outputs_(outputs) {
+    for (auto& i : inputs_) {
+      externals_[i] = nullptr;
+    }
+    for (auto& o : outputs_) {
+      externals_[o] = nullptr;
+    }
+    graph_inputs_ = emscripten::val::object();
+    graph_outputs_ = emscripten::val::object();
+  }
+
+  emscripten::val wnn_context_ = emscripten::val::object();
+  emscripten::val wnn_graph_ = emscripten::val::object();
+  // TFLite Tensor IDs == name of input/output tensors for the
+  // delegated subgraph.
+  std::unordered_set<int> inputs_;
+  std::unordered_set<int> outputs_;
+  emscripten::val graph_inputs_ = emscripten::val::object();
+  emscripten::val graph_outputs_ = emscripten::val::object();
+  std::unordered_map<int, void*> externals_;
+  char dummy_data_{0};
+};
+
+TfLiteIntArray* Delegate::PrepareOpsToDelegate(TfLiteContext* context) {
+  // Clear previous data, in case the delegate is reused without re-creation.
+  static_unpacked_data_map_.clear();
+  static_unpacked_data_.clear();
+  static_unpack_nodes_.clear();
+  static_sparse_weights_.clear();
+
+  TfLiteIntArray* execution_plan = nullptr;
+  if (context->GetExecutionPlan(context, &execution_plan) != kTfLiteOk) {
+    TF_LITE_KERNEL_LOG(context, "Unable to get graph execution plan.");
+    return nullptr;
+  }
+
+  // Mapping for quasi-static (unpacked from static) tensor index to the node
+  // index that produced it.
+  std::unordered_map<int, int> quasi_static_tensors_producers;
+  // Set of all quasi-static tensors in the execution plan.
+  std::unordered_set<int> quasi_static_tensors;
+  // Set of quasi-static tensors consumed by the delegated nodes.
+  std::unordered_set<int> quasi_static_tensors_to_unpack;
+
+  TfLiteIntArray* nodes_to_delegate =
+      TfLiteIntArrayCreate(execution_plan->size);
+  nodes_to_delegate->size = 0;
+  for (int i = 0; i < execution_plan->size; ++i) {
+    const int node_index = execution_plan->data[i];
+
+    // Check if TFLite nodes can be delegated to WebNN
+    TfLiteNode* node = nullptr;
+    TfLiteRegistration* registration = nullptr;
+    if (context->GetNodeAndRegistration(context, node_index, &node,
+                                        &registration) != kTfLiteOk) {
+      TF_LITE_KERNEL_LOG(context,
+                         "Unable to get node and registration for node %d.",
+                         node_index);
+      continue;  // Soft error (skip this node).
+    }
+
+    // Prepare to unpack FP16 tensors.
+    if (registration->builtin_code == kTfLiteBuiltinDequantize &&
+        node->inputs->size == 1 && node->outputs->size == 1) {
+      const TfLiteTensor& input_tensor =
+          context->tensors[node->inputs->data[0]];
+      const TfLiteTensor& output_tensor =
+          context->tensors[node->outputs->data[0]];
+      if ((input_tensor.allocation_type == kTfLiteMmapRo ||
+           quasi_static_tensors.count(node->inputs->data[0]) != 0) &&
+          input_tensor.type == kTfLiteFloat16 &&
+          output_tensor.type == kTfLiteFloat32) {
+        static_unpack_nodes_.insert(node_index);
+        quasi_static_tensors_producers[node->outputs->data[0]] = node_index;
+        quasi_static_tensors.insert(node->outputs->data[0]);
+
+        if (input_tensor.allocation_type != kTfLiteMmapRo) {
+          quasi_static_tensors_to_unpack.insert(node->inputs->data[0]);
+        }
+
+        // If dequantized input is sparse, so is its output
+        if (static_sparse_weights_.count(node->inputs->data[0]) != 0) {
+          static_sparse_weights_.insert(node->outputs->data[0]);
+        }
+
+        // Skip this node for now. If output of the node is consumed only by
+        // delegated nodes, it will be added to nodes_to_delegate in the end.
+        continue;
+      }
+    }
+
+    // Prepare to unpack sparse tensors.
+    // TODO(b/157729695): In the future, we also need to handle the case where a
+    // sparse tensor is fed to a TFLite op directly, and no Densify() op is
+    // inserted. For now this is not a problem because the Conv() op in tflite
+    // can only consume dense tensors.
+    if (registration->builtin_code == kTfLiteBuiltinDensify &&
+        node->inputs->size == 1 && node->outputs->size == 1) {
+      const TfLiteTensor& input_tensor =
+          context->tensors[node->inputs->data[0]];
+      const TfLiteTensor& output_tensor =
+          context->tensors[node->outputs->data[0]];
+      if (input_tensor.allocation_type == kTfLiteMmapRo &&
+          input_tensor.sparsity != nullptr &&
+          (input_tensor.type == kTfLiteFloat16 ||
+           input_tensor.type == kTfLiteFloat32) &&
+          output_tensor.type == input_tensor.type) {
+        static_unpack_nodes_.insert(node_index);
+        quasi_static_tensors_producers[node->outputs->data[0]] = node_index;
+        quasi_static_tensors.insert(node->outputs->data[0]);
+        static_sparse_weights_.insert(node->outputs->data[0]);
+
+        // Skip this node for now. If output of the node is consumed only by
+        // delegated nodes, it will be added to nodes_to_delegate in the end.
+        continue;
+      }
+    }
+
+    emscripten::val null_builder = emscripten::val::null();
+    std::unordered_map<int, emscripten::val> empty_webnn_operands;
+    std::vector<std::unique_ptr<char>> empty_buffers;
+    if (Subgraph::VisitNode(null_builder, context, registration, node,
+                            node_index, quasi_static_tensors,
+                            empty_webnn_operands, empty_buffers) != kTfLiteOk) {
+      // If a non-delegated node consumes output of a node that unpacks static
+      // data, that node shouldn't be delegated.
+      for (int j = 0; j < node->inputs->size; j++) {
+        const auto it =
+            quasi_static_tensors_producers.find(node->inputs->data[j]);
+        if (it != quasi_static_tensors_producers.end()) {
+          static_unpack_nodes_.erase(it->second);
+        }
+      }
+
+      // Non-delegatable node is not an error.
+      continue;
+    }
+
+    for (int j = 0; j < node->inputs->size; j++) {
+      if (quasi_static_tensors.count(node->inputs->data[j]) != 0) {
+        quasi_static_tensors_to_unpack.insert(node->inputs->data[j]);
+      }
+    }
+
+    nodes_to_delegate->data[nodes_to_delegate->size++] = node_index;
+  }
+
+  // Sort quasi-static tensors to be unpacked by the node index the produced
+  // them. This ensures that in situations where quasi-static tensor is
+  // produced from another quasi-static tensor, the tensors are unpacked in
+  // the original execution plan order.
+  std::vector<int> sorted_quasi_static_tensors_to_unpack(
+      quasi_static_tensors_to_unpack.cbegin(),
+      quasi_static_tensors_to_unpack.cend());
+  std::sort(sorted_quasi_static_tensors_to_unpack.begin(),
+            sorted_quasi_static_tensors_to_unpack.end(),
+            [&quasi_static_tensors_producers](int t1, int t2) {
+              return quasi_static_tensors_producers[t1] <
+                     quasi_static_tensors_producers[t2];
+            });
+
+  // Unpack static data of all tensors
+  for (int t : sorted_quasi_static_tensors_to_unpack) {
+    const int producer_index = quasi_static_tensors_producers[t];
+    // Check if TFLite nodes can be delegated to WebNN
+    TfLiteNode* node = nullptr;
+    TfLiteRegistration* registration = nullptr;
+    if (context->GetNodeAndRegistration(context, producer_index, &node,
+                                        &registration) != kTfLiteOk) {
+      TF_LITE_KERNEL_LOG(context,
+                         "Unable to get node and registration for node %d.",
+                         producer_index);
+      TfLiteIntArrayFree(nodes_to_delegate);
+      return nullptr;  // Hard error.
+    }
+
+    if (node->inputs->size != 1) {
+      TF_LITE_KERNEL_LOG(context, "unexpected number of inputs (%d) in node %d",
+                         node->inputs->size, producer_index);
+      TfLiteIntArrayFree(nodes_to_delegate);
+      return nullptr;  // Hard error.
+    }
+
+    if (node->outputs->size != 1) {
+      TF_LITE_KERNEL_LOG(context,
+                         "unexpected number of outputs (%d) in node %d",
+                         node->outputs->size, producer_index);
+      TfLiteIntArrayFree(nodes_to_delegate);
+      return nullptr;  // Hard error.
+    }
+
+    const TfLiteTensor& input_tensor = context->tensors[node->inputs->data[0]];
+
+    // Consider the case when the input to unpacking node is quasi-static.
+    const auto static_unpacked_input_it_ =
+        static_unpacked_data_map_.find(node->inputs->data[0]);
+    if (static_unpacked_input_it_ == static_unpacked_data_map_.end()) {
+      if (input_tensor.allocation_type != kTfLiteMmapRo) {
+        TF_LITE_KERNEL_LOG(
+            context,
+            "unexpected allocation type (%d) in tensor %d in node %d (%d)",
+            input_tensor.allocation_type, node->inputs->data[0], producer_index,
+            registration->builtin_code);
+        TfLiteIntArrayFree(nodes_to_delegate);
+        return nullptr;  // Hard error.
+      }
+    }
+
+    const TfLiteTensor& output_tensor = context->tensors[t];
+    size_t tensor_elements = output_tensor.bytes;
+    switch (output_tensor.type) {
+      case kTfLiteFloat32:
+        tensor_elements /= sizeof(float);
+        break;
+      case kTfLiteFloat16:
+        tensor_elements /= sizeof(uint16_t);
+        break;
+      default: {
+        TF_LITE_KERNEL_LOG(context,
+                           "unexpected datatype (%s) in tensor %d in node %d",
+                           TfLiteTypeGetName(output_tensor.type),
+                           node->outputs->data[0], producer_index);
+        TfLiteIntArrayFree(nodes_to_delegate);
+        return nullptr;  // Hard error.
+      }
+    }
+
+    const size_t tensor_offset = static_unpacked_data_.size();
+    static_unpacked_data_.resize(tensor_offset + context->tensors[t].bytes);
+
+    char* unpacked_data = static_unpacked_data_.data() + tensor_offset;
+    const char* packed_data =
+        static_unpacked_input_it_ != static_unpacked_data_map_.end()
+            ? static_unpacked_data_.data() + static_unpacked_input_it_->second
+            : static_cast<const char*>(input_tensor.data.data);
+    switch (registration->builtin_code) {
+      case kTfLiteBuiltinDequantize: {
+        if (input_tensor.type != kTfLiteFloat16) {
+          TF_LITE_KERNEL_LOG(
+              context, "unexpected tensor %d data type (%s) in node %d",
+              node->inputs->data[0], TfLiteTypeGetName(input_tensor.type),
+              producer_index);
+          TfLiteIntArrayFree(nodes_to_delegate);
+          return nullptr;  // Hard error.
+        }
+
+        if (input_tensor.sparsity != nullptr) {
+          TF_LITE_KERNEL_LOG(context,
+                             "unexpected FP16 sparse tensor %d in node %d",
+                             node->inputs->data[0], producer_index);
+          TfLiteIntArrayFree(nodes_to_delegate);
+          return nullptr;  // Hard error.
+        }
+
+        // Actual data unpacking
+        float* unpacked_fp32_data = reinterpret_cast<float*>(unpacked_data);
+        const uint16_t* packed_fp16_data =
+            reinterpret_cast<const uint16_t*>(packed_data);
+        for (size_t i = 0; i < tensor_elements; i++) {
+          unpacked_fp32_data[i] = fp16_ieee_to_fp32_value(packed_fp16_data[i]);
+        }
+        break;
+      }
+      case kTfLiteBuiltinDensify: {
+        if (input_tensor.sparsity == nullptr) {
+          TF_LITE_KERNEL_LOG(context, "unexpected dense tensor %d in node %d",
+                             node->inputs->data[0], producer_index);
+          TfLiteIntArrayFree(nodes_to_delegate);
+          return nullptr;  // Hard error.
+        }
+
+        const int dims_count = output_tensor.dims->size;
+        std::vector<int> vector_shape(dims_count);
+        for (int i = 0; i < dims_count; i++) {
+          vector_shape[i] = output_tensor.dims->data[i];
+        }
+
+        switch (input_tensor.type) {
+          case kTfLiteFloat32: {
+            const size_t dense_size = context->tensors[t].bytes / sizeof(float);
+            float* unpacked_fp32_data = reinterpret_cast<float*>(unpacked_data);
+            tflite::internal::sparsity::FormatConverter<float> converter(
+                vector_shape, *input_tensor.sparsity);
+            converter.SparseToDense(
+                static_cast<const float*>(input_tensor.data.data), dense_size,
+                unpacked_fp32_data, context);
+            break;
+          }
+          case kTfLiteFloat16: {
+            const size_t dense_size =
+                context->tensors[t].bytes / sizeof(Eigen::half);
+            Eigen::half* unpacked_fp16_data =
+                reinterpret_cast<Eigen::half*>(unpacked_data);
+            tflite::internal::sparsity::FormatConverter<Eigen::half> converter(
+                vector_shape, *input_tensor.sparsity);
+            converter.SparseToDense(
+                static_cast<const Eigen::half*>(input_tensor.data.data),
+                dense_size, unpacked_fp16_data, context);
+            break;
+          }
+          default: {
+            TF_LITE_KERNEL_LOG(
+                context, "unexpected tensor %d data type (%s) in node %d",
+                node->inputs->data[0], TfLiteTypeGetName(input_tensor.type),
+                producer_index);
+            TfLiteIntArrayFree(nodes_to_delegate);
+            return nullptr;  // Hard error.
+          }
+        }
+        break;
+      }
+      default:
+        TF_LITE_KERNEL_LOG(context, "unexpected op registration %d at node %d",
+                           registration->builtin_code, producer_index);
+        TfLiteIntArrayFree(nodes_to_delegate);
+        return nullptr;  // Hard error.
+    }
+
+    static_unpacked_data_map_[t] = tensor_offset;
+  }
+
+  // Add nodes that unpack static data consumed by delegated nodes.
+  // Note: this is done purely to avoid the overhead of running these nodes
+  // again in TFLite interpreter which would allocate memory for their outputs.
+  // We mark them as delegated, but the delegate would simply ignore these nodes
+  // as the static weights are already unpacked.
+  for (int node_index : static_unpack_nodes_) {
+    nodes_to_delegate->data[nodes_to_delegate->size++] = node_index;
+  }
+  std::sort(&nodes_to_delegate->data[0],
+            &nodes_to_delegate->data[nodes_to_delegate->size]);
+
+#ifdef WEBNN_DELEGATE_TEST_MODE
+  // In the test mode build (used by unit tests), WebNN delegate claims to
+  // support all operators in the execution plan to disable fallback to the
+  // default TensorFlow Lite kernels. Thus, if any of the ops in the model are
+  // not supported by the delegate, they will cause a failure in
+  // ::tflite::Interpreter::ModifyGraphWithDelegate, to be caught in the unit
+  // tests.
+  nodes_to_delegate->size = execution_plan->size;
+  std::copy(&execution_plan->data[0],
+            &execution_plan->data[execution_plan->size],
+            &nodes_to_delegate->data[0]);
+#endif
+
+  return nodes_to_delegate;
+}
+
+void* SubgraphInit(TfLiteContext* context, const char* buffer, size_t length) {
+  const TfLiteDelegateParams* params =
+      reinterpret_cast<const TfLiteDelegateParams*>(buffer);
+
+  return static_cast<void*>(Subgraph::Create(
+      context, params,
+      static_cast<::tflite::webnn::Delegate*>(params->delegate->data_)));
+}
+
+TfLiteStatus SubgraphPrepare(TfLiteContext* context, TfLiteNode* node) {
+  if (node->user_data == nullptr) {
+    return kTfLiteError;
+  }
+
+  return static_cast<Subgraph*>(node->user_data)->Prepare(context);
+}
+
+TfLiteStatus SubgraphInvoke(TfLiteContext* context, TfLiteNode* node) {
+  if (node->user_data == nullptr) {
+    return kTfLiteError;
+  }
+
+  return static_cast<Subgraph*>(node->user_data)->Invoke(context);
+}
+
+void SubgraphFree(TfLiteContext* context, void* buffer) {
+  if (buffer != nullptr) {
+    delete static_cast<Subgraph*>(buffer);
+  }
+}
+
+const TfLiteRegistration kSubgraphRegistration = {
+    /*.init=*/SubgraphInit,
+    /*.free=*/SubgraphFree,
+    /*.prepare=*/SubgraphPrepare,
+    /*.invoke=*/SubgraphInvoke,
+    /*.profiling_string=*/nullptr,
+    /*.builtin_code=*/0,
+    /*.custom_name=*/"TfLiteWebNNDelegate",
+    /*.version=*/2,
+};
+
+TfLiteStatus DelegatePrepare(TfLiteContext* context, TfLiteDelegate* delegate) {
+  TfLiteIntArray* ops_to_replace =
+      static_cast<::tflite::webnn::Delegate*>(delegate->data_)
+          ->PrepareOpsToDelegate(context);
+  if (ops_to_replace == nullptr) {
+    return kTfLiteError;
+  }
+
+  const TfLiteStatus status = context->ReplaceNodeSubsetsWithDelegateKernels(
+      context, kSubgraphRegistration, ops_to_replace, delegate);
+  TfLiteIntArrayFree(ops_to_replace);
+  return status;
+}
+
+}  // namespace
+}  // namespace webnn
+}  // namespace tflite
+
+TfLiteWebNNDelegateOptions TfLiteWebNNDelegateOptionsDefault() {
+  TfLiteWebNNDelegateOptions options = {2, 2};
+  return options;
+}
+
+TfLiteDelegate* TfLiteWebNNDelegateCreate(
+    const TfLiteWebNNDelegateOptions* options) {
+  auto* webnn_delegate = new ::tflite::webnn::Delegate(options);
+  return webnn_delegate ? webnn_delegate->tflite_delegate() : nullptr;
+}
+
+void TfLiteWebNNDelegateDelete(TfLiteDelegate* delegate) {
+  if (delegate != nullptr) {
+    delete static_cast<::tflite::webnn::Delegate*>(delegate->data_);
+  }
+}
diff --git a/tensorflow/lite/delegates/webnn/webnn_delegate.h b/tensorflow/lite/delegates/webnn/webnn_delegate.h
new file mode 100644
index 00000000000..b99f4119e69
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/webnn_delegate.h
@@ -0,0 +1,56 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_WEBNN_DELEGATE_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_WEBNN_DELEGATE_H_
+
+#include "tensorflow/lite/c/common.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif  // __cplusplus
+
+typedef struct {
+  // enum class DeviceType : uint32_t {
+  //     Auto = 0x00000000,
+  //     Gpu = 0x00000001,
+  //     Cpu = 0x00000002,
+  // };
+  uint32_t deviceType;
+  // enum class PowerPreference : uint32_t {
+  //     Auto = 0x00000000,
+  //     High_performance = 0x00000001,
+  //     Low_power = 0x00000002,
+  // };
+  uint32_t powerPreference;
+} TfLiteWebNNDelegateOptions;
+
+// Returns a structure with the default WebNN delegate options.
+TfLiteWebNNDelegateOptions TfLiteWebNNDelegateOptionsDefault();
+
+// Creates a new delegate instance that need to be destroyed with
+// `TfLiteWebNNDelegateDelete` when delegate is no longer used by TFLite.
+// When `options` is set to `nullptr`, the following default values are used:
+TfLiteDelegate* TfLiteWebNNDelegateCreate(
+    const TfLiteWebNNDelegateOptions* options);
+
+// Destroys a delegate created with `TfLiteWebNNDelegateCreate` call.
+void TfLiteWebNNDelegateDelete(TfLiteDelegate* delegate);
+
+#ifdef __cplusplus
+}
+#endif  // __cplusplus
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_WEBNN_DELEGATE_H_
diff --git a/tensorflow/lite/delegates/webnn/webnn_delegate_adaptor.cc b/tensorflow/lite/delegates/webnn/webnn_delegate_adaptor.cc
new file mode 100644
index 00000000000..e28532c9bae
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/webnn_delegate_adaptor.cc
@@ -0,0 +1,85 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#include <string>
+#include <vector>
+
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/c/c_api_types.h"  // IWYU pragma: export
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+#include "tensorflow/lite/tools/command_line_flags.h"
+#include "tensorflow/lite/tools/logging.h"
+
+namespace tflite {
+namespace tools {
+
+TfLiteDelegate* CreateTfLiteWebNNDelegateFromOptions(char** options_keys,
+                                                     char** options_values,
+                                                     size_t num_options) {
+  TfLiteWebNNDelegateOptions options = TfLiteWebNNDelegateOptionsDefault();
+  // Parse key-values options to TfLiteWebNNDelegateOptions by mimicking them as
+  // command-line flags.
+  std::vector<const char*> argv;
+  argv.reserve(num_options + 1);
+  constexpr char kWebNNDelegateParsing[] = "webnn_delegate_parsing";
+  argv.push_back(kWebNNDelegateParsing);
+
+  std::vector<std::string> option_args;
+  option_args.reserve(num_options);
+  for (int i = 0; i < num_options; ++i) {
+    option_args.emplace_back("--");
+    option_args.rbegin()->append(options_keys[i]);
+    option_args.rbegin()->push_back('=');
+    option_args.rbegin()->append(options_values[i]);
+    argv.push_back(option_args.rbegin()->c_str());
+  }
+
+  constexpr char kWebNNDeviceType[] = "webnn_device";
+
+  std::vector<tflite::Flag> flag_list = {
+      tflite::Flag::CreateFlag(kWebNNDeviceType,
+                               reinterpret_cast<int32_t*>(&options.deviceType),
+                               "WebNN device (0:auto, 1:gpu, 2:cpu)."),
+  };
+
+  int argc = num_options + 1;
+  if (!tflite::Flags::Parse(&argc, argv.data(), flag_list)) {
+    return nullptr;
+  }
+
+  TFLITE_LOG(INFO) << "WebNN delegate: WebNN device set to "
+                   << options.deviceType << ".";
+
+  return TfLiteWebNNDelegateCreate(&options);
+}
+
+}  // namespace tools
+}  // namespace tflite
+
+extern "C" {
+
+// Defines two symbols that need to be exported to use the TFLite external
+// delegate. See tensorflow/lite/delegates/external for details.
+TFL_CAPI_EXPORT TfLiteDelegate* tflite_plugin_create_delegate(
+    char** options_keys, char** options_values, size_t num_options,
+    void (*report_error)(const char*)) {
+  return tflite::tools::CreateTfLiteWebNNDelegateFromOptions(
+      options_keys, options_values, num_options);
+}
+
+TFL_CAPI_EXPORT void tflite_plugin_destroy_delegate(TfLiteDelegate* delegate) {
+  TfLiteWebNNDelegateDelete(delegate);
+}
+
+}  // extern "C"
diff --git a/tensorflow/lite/examples/label_image/CMakeLists.txt b/tensorflow/lite/examples/label_image/CMakeLists.txt
index 1bf259aad10..9301e54824e 100644
--- a/tensorflow/lite/examples/label_image/CMakeLists.txt
+++ b/tensorflow/lite/examples/label_image/CMakeLists.txt
@@ -41,6 +41,18 @@ else()
   set(TFLITE_LABEL_IMAGE_CC_OPTIONS "-DTFLITE_WITHOUT_XNNPACK")
 endif()  # TFLITE_ENABLE_XNNPACK
 
+if(TFLITE_ENABLE_WEBNN)
+  list(APPEND TFLITE_LABEL_IMAGE_SRCS
+    ${TFLITE_SOURCE_DIR}/tools/delegates/webnn_delegate_provider.cc
+  )
+  list(APPEND TFLITE_LABEL_IMAGE_SRCS "$ENV{WEBNN_NATIVE_DIR}/out/Release/gen/src/webnn/webnn_cpp.cpp")
+  link_directories(
+    "$ENV{WEBNN_NATIVE_DIR}"
+  )
+else()
+  set(TFLITE_LABEL_IMAGE_CC_OPTIONS "-DTFLITE_WITHOUT_WEBNN")
+endif()  # TFLITE_ENABLE_WEBNN
+
 if(CMAKE_SYSTEM_NAME MATCHES "Android")
   if(_TFLITE_ENABLE_NNAPI)
     list(APPEND TFLITE_LABEL_IMAGE_SRCS
@@ -66,3 +78,10 @@ target_compile_options(label_image
 target_link_libraries(label_image
   tensorflow-lite
 )
+
+if(TFLITE_ENABLE_WEBNN)
+  target_link_libraries(label_image
+    $ENV{WEBNN_NATIVE_DIR}/out/Release/libwebnn_native.so
+    $ENV{WEBNN_NATIVE_DIR}/out/Release/libwebnn_proc.so
+  )
+endif()
\ No newline at end of file
diff --git a/tensorflow/lite/examples/label_image/label_image.cc b/tensorflow/lite/examples/label_image/label_image.cc
index b3943380761..61f839fa3b8 100644
--- a/tensorflow/lite/examples/label_image/label_image.cc
+++ b/tensorflow/lite/examples/label_image/label_image.cc
@@ -129,6 +129,17 @@ class DelegateProviders {
         params_.Set<bool>("num_threads", s.number_of_threads);
       }
     }
+
+    // Parse settings related to WebNN delegate.
+    if (s.webnn_delegate) {
+      if (!params_.HasParam("use_webnn")) {
+        LOG(WARN) << "WebNN deleate execution provider isn't linked or "
+                     "WebNN delegate isn't supported on the platform!";
+      } else {
+        params_.Set<bool>("use_webnn", true);
+        params_.Set<int>("webnn_device", s.webnn_device);
+      }
+    }
   }
 
   // Create a list of TfLite delegates based on what have been initialized (i.e.
@@ -216,6 +227,9 @@ void RunInference(Settings* settings,
 
   tflite::ops::builtin::BuiltinOpResolver resolver;
 
+  static TfLiteRegistration reg = {nullptr, nullptr, nullptr, nullptr};
+  resolver.AddCustom("Convolution2DTransposeBias", &reg);
+
   tflite::InterpreterBuilder(*model, resolver)(&interpreter);
   if (!interpreter) {
     LOG(ERROR) << "Failed to construct interpreter";
@@ -420,6 +434,8 @@ void display_usage(const DelegateProviders& delegate_providers) {
       << "\t--verbose, -v: [0|1] print more information\n"
       << "\t--warmup_runs, -w: number of warmup runs\n"
       << "\t--xnnpack_delegate, -x [0:1]: xnnpack delegate\n"
+      << "\t--webnn_delegate, -n [0|1]: webnn delegate (on|off)\n"
+      << "\t--webnn_device, -d [0|1|2]: webnn device (default|gpu|cpu) \n";
       << "\t--help, -h: Print this help message\n";
 }
 
@@ -455,12 +471,14 @@ int Main(int argc, char** argv) {
         {"hexagon_delegate", required_argument, nullptr, 'j'},
         {"xnnpack_delegate", required_argument, nullptr, 'x'},
         {"help", no_argument, nullptr, 'h'},
+        {"webnn_delegate", required_argument, nullptr, 'n'},
+        {"webnn_device", required_argument, nullptr, 'd'},
         {nullptr, 0, nullptr, 0}};
 
     /* getopt_long stores the option index here. */
     int option_index = 0;
 
-    c = getopt_long(argc, argv, "a:b:c:d:e:f:g:i:j:l:m:p:r:s:t:v:w:x:h",
+    c = getopt_long(argc, argv, "a:b:c:d:e:f:g:i:j:l:m:p:r:s:t:v:w:x:n:d:h",
                     long_options, &option_index);
 
     /* Detect the end of the options. */
@@ -528,6 +546,14 @@ int Main(int argc, char** argv) {
         s.xnnpack_delegate =
             strtol(optarg, nullptr, 10);  // NOLINT(runtime/deprecated_fn)
         break;
+      case 'n':
+        s.webnn_delegate =
+            strtol(optarg, nullptr, 10);  // NOLINT(runtime/deprecated_fn)
+        break;
+      case 'd':
+        s.webnn_device =
+            strtol(optarg, nullptr, 10);  // NOLINT(runtime/deprecated_fn)
+        break;
       case 'h':
       case '?':
         /* getopt_long already printed an error message. */
diff --git a/tensorflow/lite/examples/label_image/label_image.h b/tensorflow/lite/examples/label_image/label_image.h
index 1c00edb6558..e84ed493b62 100644
--- a/tensorflow/lite/examples/label_image/label_image.h
+++ b/tensorflow/lite/examples/label_image/label_image.h
@@ -31,6 +31,8 @@ struct Settings {
   bool gl_backend = false;
   bool hexagon_delegate = false;
   bool xnnpack_delegate = false;
+  bool webnn_delegate = false;
+  int webnn_device = 0;
   int loop_count = 1;
   float input_mean = 127.5f;
   float input_std = 127.5f;
diff --git a/tensorflow/lite/simple_memory_arena.cc b/tensorflow/lite/simple_memory_arena.cc
index 1c7a03846f5..c698e734ee8 100644
--- a/tensorflow/lite/simple_memory_arena.cc
+++ b/tensorflow/lite/simple_memory_arena.cc
@@ -169,10 +169,9 @@ TfLiteStatus SimpleMemoryArena::ReleaseBuffer() {
 }
 
 // Using weak symbols to create a pluggable debugging module.
-TFLITE_ATTRIBUTE_WEAK void DumpArenaInfo(
+extern void DumpArenaInfo(
     const std::string& name, const std::vector<int>& execution_plan,
-    size_t arena_size, const std::vector<ArenaAllocWithUsageInterval>& allocs) {
-}
+    size_t arena_size, const std::vector<ArenaAllocWithUsageInterval>& allocs);
 
 void SimpleMemoryArena::DumpDebugInfo(
     const std::string& name, const std::vector<int>& execution_plan) const {
diff --git a/tensorflow/lite/tflite_with_webnn.cc b/tensorflow/lite/tflite_with_webnn.cc
new file mode 100644
index 00000000000..e22a8557e68
--- /dev/null
+++ b/tensorflow/lite/tflite_with_webnn.cc
@@ -0,0 +1,27 @@
+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#include <memory>
+
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)>
+AcquireWebNNDelegate(int num_threads) {
+  auto opts = TfLiteWebNNDelegateOptionsDefault();
+  return std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)>(
+      TfLiteWebNNDelegateCreate(&opts), TfLiteWebNNDelegateDelete);
+}
+}  // namespace tflite
diff --git a/tensorflow/lite/tools/benchmark/CMakeLists.txt b/tensorflow/lite/tools/benchmark/CMakeLists.txt
index ae24d998ced..1f1aa4f4051 100644
--- a/tensorflow/lite/tools/benchmark/CMakeLists.txt
+++ b/tensorflow/lite/tools/benchmark/CMakeLists.txt
@@ -61,6 +61,19 @@ if(TFLITE_ENABLE_EXTERNAL_DELEGATE)
   )
 endif()  # TFLITE_ENABLE_EXTERNAL_DELEGATE
 
+if(TFLITE_ENABLE_WEBNN)
+  list(APPEND TFLITE_BENCHMARK_SRCS
+    ${TFLITE_SOURCE_DIR}/tools/delegates/webnn_delegate_provider.cc
+  )
+  list(APPEND TFLITE_BENCHMARK_SRCS "$ENV{WEBNN_NATIVE_DIR}/out/Release/gen/src/webnn/webnn_cpp.cpp")
+  link_directories(
+    "$ENV{WEBNN_NATIVE_DIR}"
+  )
+else()
+  set(TFLITE_BENCHMARK_CC_OPTIONS "-DTFLITE_WITHOUT_WEBNN")
+  set(TFLITE_LABEL_IMAGE_CC_OPTIONS "-DTFLITE_WITHOUT_WEBNN")
+endif()  # TFLITE_ENABLE_WEBNN
+
 if(CMAKE_SYSTEM_NAME MATCHES "Android")
   if(_TFLITE_ENABLE_NNAPI)
     list(APPEND TFLITE_BENCHMARK_SRCS
@@ -90,3 +103,17 @@ target_compile_options(benchmark_model
 target_link_libraries(benchmark_model
     ${TFLITE_BENCHMARK_LIBS}
 )
+
+if(TFLITE_ENABLE_WEBNN)
+  if(CMAKE_SYSTEM_NAME MATCHES "Windows")
+    target_link_libraries(benchmark_model
+      $ENV{WEBNN_NATIVE_DIR}/out/Release/webnn_native.dll.lib
+      $ENV{WEBNN_NATIVE_DIR}/out/Release/webnn_proc.dll.lib
+    )
+  else()
+    target_link_libraries(benchmark_model
+      $ENV{WEBNN_NATIVE_DIR}/out/Release/libwebnn_native.so
+      $ENV{WEBNN_NATIVE_DIR}/out/Release/libwebnn_proc.so
+    )
+  endif()
+endif()
diff --git a/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc b/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc
index c1cb6ba58ea..5b87b6ab616 100644
--- a/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc
+++ b/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc
@@ -777,6 +777,9 @@ std::unique_ptr<tflite::OpResolver> BenchmarkTfLiteModel::GetOpResolver()
   } else {
     resolver = new tflite::ops::builtin::BuiltinOpResolver();
   }
+  static TfLiteRegistration reg = {nullptr, nullptr, nullptr, nullptr};
+  resolver->AddCustom("Convolution2DTransposeBias", &reg);
+
   RegisterSelectedOps(resolver);
   return std::unique_ptr<tflite::OpResolver>(resolver);
 }
diff --git a/tensorflow/lite/tools/delegates/BUILD b/tensorflow/lite/tools/delegates/BUILD
index 609b6630bd3..7775df76240 100644
--- a/tensorflow/lite/tools/delegates/BUILD
+++ b/tensorflow/lite/tools/delegates/BUILD
@@ -45,7 +45,7 @@ cc_library(
         ":external_delegate_provider",
         ":gpu_delegate_provider",
         ":hexagon_delegate_provider",
-        ":nnapi_delegate_provider",
+        # ":nnapi_delegate_provider",
         ":xnnpack_delegate_provider",
     ],
     alwayslink = 1,
@@ -197,7 +197,7 @@ cc_test(
         ":default_execution_provider",
         ":delegate_provider_hdr",
         ":delegate_provider_lib",
-        ":nnapi_delegate_provider",
+        # ":nnapi_delegate_provider",
         ":xnnpack_delegate_provider",
         "//tensorflow/lite/delegates/utils/dummy_delegate:dummy_delegate_provider",
         "//tensorflow/lite/tools:tool_params",
diff --git a/tensorflow/lite/tools/delegates/webnn_delegate_provider.cc b/tensorflow/lite/tools/delegates/webnn_delegate_provider.cc
new file mode 100644
index 00000000000..9cb253eb214
--- /dev/null
+++ b/tensorflow/lite/tools/delegates/webnn_delegate_provider.cc
@@ -0,0 +1,73 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#include <string>
+
+#include "tensorflow/lite/tools/delegates/delegate_provider.h"
+#include "tensorflow/lite/tools/evaluation/utils.h"
+
+namespace tflite {
+namespace tools {
+
+class WebNNDelegateProvider : public DelegateProvider {
+ public:
+  WebNNDelegateProvider() {
+    default_params_.AddParam("use_webnn", ToolParam::Create<bool>(false));
+    default_params_.AddParam("webnn_device", ToolParam::Create<int>(0));
+  }
+
+  std::vector<Flag> CreateFlags(ToolParams* params) const final;
+
+  void LogParams(const ToolParams& params, bool verbose) const final;
+
+  TfLiteDelegatePtr CreateTfLiteDelegate(const ToolParams& params) const final;
+  std::pair<TfLiteDelegatePtr, int> CreateRankedTfLiteDelegate(
+      const ToolParams& params) const final;
+
+  std::string GetName() const final { return "WebNN"; }
+};
+REGISTER_DELEGATE_PROVIDER(WebNNDelegateProvider);
+
+std::vector<Flag> WebNNDelegateProvider::CreateFlags(
+    ToolParams* params) const {
+  std::vector<Flag> flags = {
+      CreateFlag<bool>("use_webnn", params, "use WebNN"),
+      CreateFlag<int>("webnn_device", params, "WebNN device (0:default, 1:gpu, 2:cpu)")};
+  return flags;
+}
+
+void WebNNDelegateProvider::LogParams(const ToolParams& params,
+                                        bool verbose) const {
+  LOG_TOOL_PARAM(params, bool, "use_webnn", "Use WebNN", verbose);
+  LOG_TOOL_PARAM(params, int, "webnn_device", "WebNN device", verbose);
+}
+
+TfLiteDelegatePtr WebNNDelegateProvider::CreateTfLiteDelegate(
+    const ToolParams& params) const {
+  if (params.Get<bool>("use_webnn")) {
+    return evaluation::CreateWebNNDelegate(params.Get<int>("webnn_device"));
+  }
+  return CreateNullDelegate();
+}
+
+std::pair<TfLiteDelegatePtr, int>
+WebNNDelegateProvider::CreateRankedTfLiteDelegate(
+    const ToolParams& params) const {
+  auto ptr = CreateTfLiteDelegate(params);
+  return std::make_pair(std::move(ptr),
+                        params.GetPosition<bool>("use_webnn"));
+}
+
+}  // namespace tools
+}  // namespace tflite
diff --git a/tensorflow/lite/tools/evaluation/BUILD b/tensorflow/lite/tools/evaluation/BUILD
index 02ca40fad25..583d8d65972 100644
--- a/tensorflow/lite/tools/evaluation/BUILD
+++ b/tensorflow/lite/tools/evaluation/BUILD
@@ -59,6 +59,12 @@ cc_library(
         "//conditions:default": [
             "//tensorflow/lite/delegates/xnnpack:xnnpack_delegate",
         ],
+    }) + select({
+        "//tensorflow:linux_armhf": [],
+        "//tensorflow:linux_s390x": [],
+        "//conditions:default": [
+            "//tensorflow/lite/delegates/webnn:webnn_delegate",
+        ],
     }),
 )
 
diff --git a/tensorflow/lite/tools/evaluation/utils.cc b/tensorflow/lite/tools/evaluation/utils.cc
index 7be427792f1..fcda478dff8 100644
--- a/tensorflow/lite/tools/evaluation/utils.cc
+++ b/tensorflow/lite/tools/evaluation/utils.cc
@@ -188,5 +188,19 @@ TfLiteDelegatePtr CreateXNNPACKDelegate(int num_threads) {
   return CreateXNNPACKDelegate(&opts);
 }
 #endif
+
+TfLiteDelegatePtr CreateWebNNDelegate(int device) {
+#if defined(TFLITE_WITHOUT_WEBNN)
+  return tools::CreateNullDelegate();
+#else
+  TfLiteWebNNDelegateOptions options =
+      TfLiteWebNNDelegateOptionsDefault();
+  options.deviceType = device;
+  auto webnn_delegate = TfLiteWebNNDelegateCreate(&options);
+  return TfLiteDelegatePtr(webnn_delegate, [](TfLiteDelegate* delegate) {
+    TfLiteWebNNDelegateDelete(delegate);
+  });
+#endif
+}
 }  // namespace evaluation
 }  // namespace tflite
diff --git a/tensorflow/lite/tools/evaluation/utils.h b/tensorflow/lite/tools/evaluation/utils.h
index 18590efc54d..135fa4b4377 100644
--- a/tensorflow/lite/tools/evaluation/utils.h
+++ b/tensorflow/lite/tools/evaluation/utils.h
@@ -39,6 +39,10 @@ limitations under the License.
 #include "tensorflow/lite/delegates/xnnpack/xnnpack_delegate.h"
 #endif
 
+#if !defined(TFLITE_WITHOUT_WEBNN)
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+#endif
+
 #include "tensorflow/lite/c/common.h"
 
 namespace tflite {
@@ -89,6 +93,9 @@ TfLiteDelegatePtr CreateXNNPACKDelegate(
     const TfLiteXNNPackDelegateOptions* options);
 #endif
 TfLiteDelegatePtr CreateXNNPACKDelegate(int num_threads);
+
+TfLiteDelegatePtr CreateWebNNDelegate(int device);
+
 }  // namespace evaluation
 }  // namespace tflite
 
diff --git a/third_party/webnn/BUILD b/third_party/webnn/BUILD
new file mode 100644
index 00000000000..90ca20d373f
--- /dev/null
+++ b/third_party/webnn/BUILD
@@ -0,0 +1 @@
+# This empty BUILD file is required to make Bazel treat this directory as a package.
\ No newline at end of file
diff --git a/third_party/webnn/webnn.bzl b/third_party/webnn/webnn.bzl
new file mode 100644
index 00000000000..4ee8cc9a4f2
--- /dev/null
+++ b/third_party/webnn/webnn.bzl
@@ -0,0 +1,48 @@
+def _get_webnn_native_dir(repository_ctx):
+    """Gets the Webnn-native path"""
+    webnn_native_dir = repository_ctx.os.environ.get("WEBNN_NATIVE_DIR")
+    if webnn_native_dir != None:
+        return webnn_native_dir
+    else:
+        fail("Cannot find Webnn-native dir, please set 'WEBNN_NATIVE_DIR' environment variable.")
+
+def _webnn_native_impl(repository_ctx):
+    webnn_native_dir = _get_webnn_native_dir(repository_ctx)
+    repository_ctx.symlink(webnn_native_dir, "webnn-native")
+    repository_ctx.file("BUILD", """
+cc_library(
+    name = "webnn-native",
+    hdrs = glob([
+        "webnn-native/out/Release/gen/src/include/**/*.h",
+        "webnn-native/src/include/*/*.h"
+    ]),
+    srcs = select({
+        "@bazel_tools//src/conditions:windows": glob([
+            "webnn-native/out/Release/webnn_native.dll",
+            "webnn-native/out/Release/webnn_native.dll.lib",
+            "webnn-native/out/Release/webnn_proc.dll",
+            "webnn-native/out/Release/webnn_proc.dll.lib",
+            "webnn-native/out/Release/gen/src/webnn/webnn_cpp.cpp"
+        ]),
+        "//conditions:default":glob([
+            "webnn-native/out/Release/libngraph_c_api.so",
+            "webnn-native/out/Release/libwebnn_native.so",
+            "webnn-native/out/Release/libwebnn_proc.so",
+            "webnn-native/out/Release/gen/src/webnn/webnn_cpp.cpp"
+        ]),
+    }),
+    includes = [
+        "webnn-native/out/Release/gen/src/include",
+        "webnn-native/src/include"
+    ],
+    visibility = ["//visibility:public"],
+)
+    """)
+
+webnn_configure = repository_rule(
+    implementation = _webnn_native_impl,
+    local = True,
+    environ = [
+        "WEBNN_NATIVE_DIR",
+    ],
+)
\ No newline at end of file
