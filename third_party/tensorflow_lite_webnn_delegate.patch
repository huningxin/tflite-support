diff --git a/tensorflow/lite/BUILD b/tensorflow/lite/BUILD
index 8414dd7fe67..4e6bcc44d97 100644
--- a/tensorflow/lite/BUILD
+++ b/tensorflow/lite/BUILD
@@ -690,6 +690,18 @@ cc_library(
     alwayslink = 1,
 )
 
+cc_library(
+    name = "tflite_with_webnn",
+    srcs = ["tflite_with_webnn.cc"],
+    copts = tflite_copts() + tflite_copts_warnings(),
+    linkstatic = True,
+    deps = [
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/delegates/webnn:webnn_delegate",
+    ],
+    alwayslink = 1,
+)
+
 # Enables applying XNNPACK delegate for float models in TFLite runtime.
 # WARNING: This build flag is experimental and subject to change.
 config_setting(
diff --git a/tensorflow/lite/CMakeLists.txt b/tensorflow/lite/CMakeLists.txt
index 40f9485b5d6..18421043b2c 100644
--- a/tensorflow/lite/CMakeLists.txt
+++ b/tensorflow/lite/CMakeLists.txt
@@ -69,6 +69,7 @@ option(TFLITE_ENABLE_GPU "Enable GPU" OFF)
 option(TFLITE_ENABLE_METAL "Enable Metal delegate (iOS only)" OFF)
 option(TFLITE_ENABLE_XNNPACK "Enable XNNPACK backend" ON)
 option(TFLITE_ENABLE_EXTERNAL_DELEGATE "Enable External Delegate backend" ON)
+option(TFLITE_ENABLE_WEBNN "Enable WebNN backend" OFF)
 
 option(TFLITE_KERNEL_TEST "Enable tflite kernel unit test" OFF)
 if(TFLITE_KERNEL_TEST AND ${CMAKE_CROSSCOMPILING})
@@ -399,6 +400,17 @@ if(TFLITE_ENABLE_XNNPACK)
   )
   list(APPEND TFLITE_TARGET_PUBLIC_OPTIONS "-DTFLITE_BUILD_WITH_XNNPACK_DELEGATE")
 endif()
+if(TFLITE_ENABLE_WEBNN)
+  include_directories(
+    "$ENV{WEBNN_NATIVE_DIR}/out/Release/gen/src/include"
+    "$ENV{WEBNN_NATIVE_DIR}/src/include"
+  )
+  populate_tflite_source_vars("delegates/webnn"
+    TFLITE_DELEGATES_WEBNN_SRCS
+    FILTER ".*(_test|_tester)\\.(cc|h)"
+  )
+  list(APPEND TFLITE_TARGET_PUBLIC_OPTIONS "-DTFLITE_BUILD_WITH_WEBNN_DELEGATE")
+endif()
 if(TFLITE_ENABLE_EXTERNAL_DELEGATE)
   populate_tflite_source_vars("delegates/external"
     TFLITE_DELEGATES_EXTERNAL_SRCS
@@ -475,6 +487,7 @@ add_library(tensorflow-lite
   ${TFLITE_DELEGATES_SRCS}
   ${TFLITE_DELEGATES_XNNPACK_SRCS}
   ${TFLITE_DELEGATES_EXTERNAL_SRCS}
+  ${TFLITE_DELEGATES_WEBNN_SRCS}
   ${TFLITE_EXPERIMENTAL_RESOURCE_SRCS}
   ${TFLITE_EXPERIMENTAL_RUY_PROFILER_SRCS}
   ${TFLITE_EXPERIMENTAL_RUY_SRCS}
diff --git a/tensorflow/lite/delegates/external/BUILD b/tensorflow/lite/delegates/external/BUILD
index 192091141be..a914f68c672 100644
--- a/tensorflow/lite/delegates/external/BUILD
+++ b/tensorflow/lite/delegates/external/BUILD
@@ -13,6 +13,11 @@
 # limitations under the License.
 # ==============================================================================
 
+load(
+    "//tensorflow/lite:build_def.bzl",
+    "tflite_cc_shared_object",
+)
+
 package(
     default_visibility = [
         "//visibility:public",
@@ -31,6 +36,31 @@ cc_library(
     ],
 )
 
+tflite_cc_shared_object(
+    name = "external_delegate_obj",
+    linkopts = select({
+        "//tensorflow:ios": [
+            "-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)",
+        ],
+        "//tensorflow:macos": [
+            "-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)",
+        ],
+        "//tensorflow:windows": [
+            "-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)",
+        ],
+        "//conditions:default": [
+            "-z defs",
+            "-Wl,--version-script,$(location //tensorflow/lite/c:version_script.lds)",
+        ],
+    }),
+    per_os_targets = True,
+    deps = [
+        ":external_delegate",
+        "//tensorflow/lite/c:exported_symbols.lds",
+        "//tensorflow/lite/c:version_script.lds",
+    ],
+)
+
 exports_files([
     "external_delegate.h",
-])
+])
\ No newline at end of file
diff --git a/tensorflow/lite/delegates/external/external_delegate.cc b/tensorflow/lite/delegates/external/external_delegate.cc
index 7fe5c5329dd..8fef8920cbd 100644
--- a/tensorflow/lite/delegates/external/external_delegate.cc
+++ b/tensorflow/lite/delegates/external/external_delegate.cc
@@ -154,14 +154,12 @@ ExternalDelegateWrapper::ExternalDelegateWrapper(
     external_delegate_ = external_lib_.create(ckeys.data(), cvalues.data(),
                                               ckeys.size(), nullptr);
     if (external_delegate_) {
-      wrapper_delegate_ = {
-          .data_ = reinterpret_cast<void*>(this),
-          .Prepare = DelegatePrepare,
-          .CopyFromBufferHandle = nullptr,
-          .CopyToBufferHandle = nullptr,
-          .FreeBufferHandle = nullptr,
-          .flags = external_delegate_->flags,
-      };
+      wrapper_delegate_.data_ = reinterpret_cast<void*>(this);
+      wrapper_delegate_.Prepare = DelegatePrepare;
+      wrapper_delegate_.CopyFromBufferHandle = nullptr;
+      wrapper_delegate_.CopyToBufferHandle = nullptr;
+      wrapper_delegate_.FreeBufferHandle = nullptr;
+      wrapper_delegate_.flags = external_delegate_->flags;
       if (external_delegate_->CopyFromBufferHandle) {
         wrapper_delegate_.CopyFromBufferHandle = DelegateCopyFromBufferHandle;
       }
diff --git a/tensorflow/lite/delegates/external/external_delegate.h b/tensorflow/lite/delegates/external/external_delegate.h
index 9121bd661b4..e15533d2664 100644
--- a/tensorflow/lite/delegates/external/external_delegate.h
+++ b/tensorflow/lite/delegates/external/external_delegate.h
@@ -17,6 +17,7 @@ limitations under the License.
 #define TENSORFLOW_LITE_DELEGATES_EXTERNAL_EXTERNAL_DELEGATE_H_
 
 #include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/c/c_api_types.h"  // IWYU pragma: export
 
 #ifdef __cplusplus
 extern "C" {
@@ -35,20 +36,20 @@ typedef struct TfLiteExternalDelegateOptions {
 } TfLiteExternalDelegateOptions;
 
 // Insert key/value to the options.
-TfLiteStatus TfLiteExternalDelegateOptionsInsert(
+TFL_CAPI_EXPORT TfLiteStatus TfLiteExternalDelegateOptionsInsert(
     TfLiteExternalDelegateOptions* options, const char* key, const char* value);
 
 // Populates TfLiteExternalDelegateOptions with the given shared library path.
-TfLiteExternalDelegateOptions TfLiteExternalDelegateOptionsDefault(
+TFL_CAPI_EXPORT TfLiteExternalDelegateOptions TfLiteExternalDelegateOptionsDefault(
     const char* lib_path);
 
 // Creates a new delegate instance that need to be destroyed with
 // `TfLiteExternalDelegateDelete` when delegate is no longer used by TFLite.
-TfLiteDelegate* TfLiteExternalDelegateCreate(
+TFL_CAPI_EXPORT TfLiteDelegate* TfLiteExternalDelegateCreate(
     const TfLiteExternalDelegateOptions* options);
 
 // Destroys a delegate created with `TfLiteExternalDelegateCreate` call.
-void TfLiteExternalDelegateDelete(TfLiteDelegate* delegate);
+TFL_CAPI_EXPORT void TfLiteExternalDelegateDelete(TfLiteDelegate* delegate);
 
 #ifdef __cplusplus
 }
diff --git a/tensorflow/lite/delegates/webnn/BUILD b/tensorflow/lite/delegates/webnn/BUILD
new file mode 100644
index 00000000000..c2068aee5d9
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/BUILD
@@ -0,0 +1,187 @@
+load("//tensorflow/lite:special_rules.bzl", "tflite_portable_test_suite_combined")
+load("//tensorflow:tensorflow.bzl", "get_compatible_with_portable")
+load("//tensorflow/lite:build_def.bzl", "tflite_cc_shared_object")
+
+package(
+    default_visibility = ["//visibility:public"],
+    licenses = ["notice"],
+)
+
+EMSCRIPTEN_LINKOPTS = [
+    "-s ASSERTIONS=2",
+    "-s ERROR_ON_UNDEFINED_SYMBOLS=1",
+    "-s DEMANGLE_SUPPORT=1",
+    "-s EXIT_RUNTIME=1",
+    "-s ALLOW_MEMORY_GROWTH=1",
+    "-s TOTAL_MEMORY=134217728",
+]
+
+exports_files([
+    "webnn_delegate.h",
+])
+
+cc_library(
+    name = "webnn_delegate",
+    srcs = ["webnn_delegate.cc"],
+    hdrs = ["webnn_delegate.h"],
+    linkstatic = True,
+    deps = [
+        "//tensorflow/lite:kernel_api",
+        "//tensorflow/lite:minimal_logging",
+        "//tensorflow/lite:util",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/schema:schema_fbs",
+        "//tensorflow/lite/kernels/internal/utils:sparsity_format_converter",
+        "@FP16",
+    ],
+)
+
+cc_library(
+    name = "webnn_delegate_adaptor",
+    srcs = ["webnn_delegate_adaptor.cc"],
+    hdrs =  ["webnn_delegate_adaptor.h"],
+    deps = [
+        ":webnn_delegate",
+        "//tensorflow/lite:shared_library",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/tools:command_line_flags",
+        "//tensorflow/lite/tools:logging",
+    ],
+)
+
+tflite_cc_shared_object(
+    name = "webnn_external_delegate_obj",
+    linkopts = select({
+        "//tensorflow:windows": [
+            "-Wl,-exported_symbols_list,$(location //tensorflow/lite/c:exported_symbols.lds)",
+        ],
+        "//conditions:default": [
+            "-Wl,-z,defs",
+            "-Wl,--version-script,$(location //tensorflow/lite:tflite_version_script.lds)",
+        ],
+    }),
+    per_os_targets = True,
+    srcs = select({
+        "//tensorflow:windows": [
+            "webnn_delegate_adaptor.cc",
+        ],
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":webnn_delegate_adaptor",
+        "//tensorflow/lite/c:exported_symbols.lds",
+        "//tensorflow/lite:tflite_version_script.lds",
+    ],
+)
+
+cc_library(
+    name = "webnn_delegate_hdrs_only",
+    hdrs = ["webnn_delegate.h"],
+    compatible_with = get_compatible_with_portable(),
+    visibility = ["//tensorflow/lite:__subpackages__"],
+    deps = [
+        "//tensorflow/lite/c:common",
+    ],
+)
+
+cc_library(
+    name = "webnn_delegate_test_mode",
+    srcs = ["webnn_delegate.cc"],
+    hdrs = ["webnn_delegate.h"],
+    copts = ["-DWEBNN_DELEGATE_TEST_MODE=1"],
+    linkstatic = True,
+    deps = [
+        "//tensorflow/lite:kernel_api",
+        "//tensorflow/lite:minimal_logging",
+        "//tensorflow/lite:util",
+        "//tensorflow/lite:simple_memory_arena_debug_dump",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/schema:schema_fbs",
+        "//tensorflow/lite/kernels/internal/utils:sparsity_format_converter",
+        "//tensorflow/lite/kernels:padding",
+        "//tensorflow/lite/kernels/internal:compatibility",
+        "//tensorflow/lite/kernels/internal:tensor",
+        "//tensorflow/lite/tools/optimize:reduced_precision_support",
+        "@webnn_native_project//:webnn-native",
+        "@FP16",
+    ],
+)
+
+################################ Tester classes ################################
+
+cc_library(
+    name = "binary_elementwise_tester",
+    testonly = 1,
+    srcs = ["binary_elementwise_tester.cc"],
+    hdrs = ["binary_elementwise_tester.h"],
+    deps = [
+        "//tensorflow/lite:framework",
+        "//tensorflow/lite:schema_fbs_version",
+        "//tensorflow/lite/c:common",
+        "//tensorflow/lite/kernels:builtin_ops",
+        "//tensorflow/lite/schema:schema_conversion_utils",
+        "//tensorflow/lite/schema:schema_fbs",
+        "@FP16",
+        "@com_google_googletest//:gtest",
+        "@flatbuffers",
+    ],
+)
+
+############################## Integration tests ###############################
+
+cc_library(
+    name = "test_main",
+    testonly = 1,
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        "@com_google_googletest//:gtest_main",
+    ],
+)
+
+cc_test(
+    name = "add_test",
+    srcs = ["add_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":binary_elementwise_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "delegate_test",
+    srcs = ["delegate_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
+cc_test(
+    name = "mul_test",
+    srcs = ["mul_test.cc"],
+    linkopts = select({
+        "//tensorflow:emscripten": EMSCRIPTEN_LINKOPTS,
+        "//conditions:default": [],
+    }),
+    deps = [
+        ":binary_elementwise_tester",
+        ":test_main",
+        ":webnn_delegate_test_mode",
+        "@com_google_googletest//:gtest",
+    ],
+)
+
diff --git a/tensorflow/lite/delegates/webnn/add_test.cc b/tensorflow/lite/delegates/webnn/add_test.cc
new file mode 100644
index 00000000000..22a9fbe140e
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/add_test.cc
@@ -0,0 +1,913 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <gtest/gtest.h>
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include "tensorflow/lite/delegates/webnn/binary_elementwise_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Add, 4DBy4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy4DBroadcastChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, 1, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, 1, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy4DBroadcastWidth) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, width, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, width, 1})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy4DBroadcastHeight) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, 1, 1})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy4DBroadcastBatch) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, 1, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, 1, 1, 1})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy4DBroadcastHeightWidthChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DBy0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 2DBy2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({batch, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 2DBy1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 2DBy0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, channels})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({})
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic4DBroadcastChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, 1, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, 1, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic4DBroadcastWidth) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, width, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, width, 1})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic4DBroadcastHeight) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, 1, 1})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic4DBroadcastBatch) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, 1, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, 1, 1, 1})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic4DBroadcastHeightWidthChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({height, width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 4DByStatic0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 2DByStatic2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({batch, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({batch, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 2DByStatic1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, 2DByStatic0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({})
+      .Input2Static(true)
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, FP16Weights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .FP16Weights()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input2Static(true)
+      .FP16Weights()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, SparseWeights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .SparseWeights()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input2Static(true)
+      .SparseWeights()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, ReluActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .ReluActivation()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, Relu6Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Relu6Activation()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, ReluMinus1To1Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .ReluMinus1To1Activation()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, DISABLED_TanhActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .TanhActivation()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+TEST(Add, DISABLED_SignBitActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .SignBitActivation()
+      .Test(BuiltinOperator_ADD, webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/binary_elementwise_tester.cc b/tensorflow/lite/delegates/webnn/binary_elementwise_tester.cc
new file mode 100644
index 00000000000..199e213c3a4
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/binary_elementwise_tester.cc
@@ -0,0 +1,415 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#include "tensorflow/lite/delegates/webnn/binary_elementwise_tester.h"
+
+#include <array>
+#include <cstdint>
+#include <functional>
+#include <numeric>
+#include <random>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include <fp16.h>
+#include "flatbuffers/flatbuffers.h"  // from @flatbuffers
+#include "tensorflow/lite/interpreter.h"
+#include "tensorflow/lite/kernels/register.h"
+#include "tensorflow/lite/model.h"
+#include "tensorflow/lite/schema/schema_conversion_utils.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+#include "tensorflow/lite/version.h"
+
+namespace tflite {
+namespace webnn {
+
+std::vector<int32_t> BinaryElementwiseTester::OutputShape() const {
+  std::vector<int32_t> output_shape;
+  if (!input1_shape_.empty()) {
+    output_shape.insert(
+        output_shape.end(), input1_shape_.cbegin(),
+        input1_shape_.cbegin() +
+            std::max(input1_shape_.size(), input2_shape_.size()) -
+            input2_shape_.size());
+  }
+  if (!input2_shape_.empty()) {
+    output_shape.insert(
+        output_shape.end(), input2_shape_.cbegin(),
+        input2_shape_.cbegin() +
+            std::max(input2_shape_.size(), input1_shape_.size()) -
+            input1_shape_.size());
+  }
+  for (size_t i = std::min(input1_shape_.size(), input2_shape_.size()); i >= 1;
+       i--) {
+    output_shape.push_back(
+        std::max(*(input1_shape_.cend() - i), *(input2_shape_.cend() - i)));
+  }
+  return output_shape;
+}
+
+void BinaryElementwiseTester::Test(tflite::BuiltinOperator binary_op,
+                                   TfLiteDelegate* delegate) const {
+  if (Input1Static()) {
+    ASSERT_FALSE(Input2Static());
+  }
+  if (FP16Weights()) {
+    ASSERT_TRUE(Input1Static() || Input2Static());
+  }
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  std::uniform_real_distribution<float> input1_distribution(-25.0f, 25.0f);
+  std::uniform_real_distribution<float> input2_distribution(-25.0f, 25.0f);
+  switch (binary_op) {
+    case BuiltinOperator_DIV:
+      input1_distribution = std::uniform_real_distribution<float>(-5.0f, 5.0f);
+      input2_distribution = std::uniform_real_distribution<float>(0.1f, 1.0f);
+      break;
+    case BuiltinOperator_MUL:
+      input1_distribution = std::uniform_real_distribution<float>(-5.0f, 5.0f);
+      input2_distribution = std::uniform_real_distribution<float>(-5.0f, 5.0f);
+      break;
+    default:
+      break;
+  }
+  auto input1_rng = std::bind(input1_distribution, std::ref(rng));
+  auto input2_rng = std::bind(input2_distribution, std::ref(rng));
+
+  std::vector<char> buffer = CreateTfLiteModel(binary_op);
+  const Model* model = GetModel(buffer.data());
+
+  std::unique_ptr<Interpreter> delegate_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &delegate_interpreter),
+      kTfLiteOk);
+  std::unique_ptr<Interpreter> default_interpreter;
+  ASSERT_EQ(
+      InterpreterBuilder(
+          model,
+          ::tflite::ops::builtin::BuiltinOpResolverWithoutDefaultDelegates())(
+          &default_interpreter),
+      kTfLiteOk);
+
+  ASSERT_TRUE(delegate_interpreter);
+  ASSERT_TRUE(default_interpreter);
+
+  if (Input1Static() || Input2Static()) {
+    ASSERT_EQ(delegate_interpreter->inputs().size(), 1);
+    ASSERT_EQ(default_interpreter->inputs().size(), 1);
+  } else {
+    ASSERT_EQ(delegate_interpreter->inputs().size(), 2);
+    ASSERT_EQ(default_interpreter->inputs().size(), 2);
+  }
+
+  ASSERT_EQ(delegate_interpreter->outputs().size(), 1);
+  ASSERT_EQ(default_interpreter->outputs().size(), 1);
+
+  ASSERT_EQ(delegate_interpreter->AllocateTensors(), kTfLiteOk);
+  ASSERT_EQ(default_interpreter->AllocateTensors(), kTfLiteOk);
+
+  ASSERT_EQ(delegate_interpreter->ModifyGraphWithDelegate(delegate), kTfLiteOk);
+
+  if (!Input1Static()) {
+    float* default_input1_data = default_interpreter->typed_tensor<float>(
+        default_interpreter->inputs()[0]);
+    std::generate(default_input1_data,
+                  default_input1_data + ComputeSize(Input1Shape()),
+                  std::ref(input1_rng));
+
+    float* webnn_input1_data = delegate_interpreter->typed_tensor<float>(
+        delegate_interpreter->inputs()[0]);
+    std::copy(default_input1_data,
+              default_input1_data + ComputeSize(Input1Shape()),
+              webnn_input1_data);
+  }
+
+  if (!Input2Static()) {
+    float* default_input2_data = default_interpreter->typed_tensor<float>(
+        default_interpreter->inputs()[Input1Static() ? 0 : 1]);
+    std::generate(default_input2_data,
+                  default_input2_data + ComputeSize(Input2Shape()),
+                  std::ref(input2_rng));
+
+    float* webnn_input2_data = delegate_interpreter->typed_tensor<float>(
+        delegate_interpreter->inputs()[Input1Static() ? 0 : 1]);
+    std::copy(default_input2_data,
+              default_input2_data + ComputeSize(Input2Shape()),
+              webnn_input2_data);
+  }
+
+  ASSERT_EQ(default_interpreter->Invoke(), kTfLiteOk);
+  ASSERT_EQ(delegate_interpreter->Invoke(), kTfLiteOk);
+
+  float* default_output_data = default_interpreter->typed_tensor<float>(
+      default_interpreter->outputs()[0]);
+  float* webnn_output_data = delegate_interpreter->typed_tensor<float>(
+      delegate_interpreter->outputs()[0]);
+
+  for (size_t i = 0; i < ComputeSize(OutputShape()); i++) {
+    ASSERT_NEAR(default_output_data[i], webnn_output_data[i],
+                std::numeric_limits<float>::epsilon() *
+                    std::max(std::abs(default_output_data[i]) * 2.0f, 1.0f));
+  }
+}
+
+std::vector<char> BinaryElementwiseTester::CreateTfLiteModel(
+    tflite::BuiltinOperator binary_op) const {
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  std::uniform_real_distribution<float> input1_distribution(-25.0f, 25.0f);
+  std::uniform_real_distribution<float> input2_distribution(-25.0f, 25.0f);
+  switch (binary_op) {
+    case BuiltinOperator_DIV:
+      input1_distribution = std::uniform_real_distribution<float>(-5.0f, 5.0f);
+      input2_distribution = std::uniform_real_distribution<float>(0.1f, 1.0f);
+      break;
+    case BuiltinOperator_MUL:
+      input1_distribution = std::uniform_real_distribution<float>(-5.0f, 5.0f);
+      input2_distribution = std::uniform_real_distribution<float>(-5.0f, 5.0f);
+      break;
+    default:
+      break;
+  }
+  auto input1_rng = std::bind(input1_distribution, std::ref(rng));
+  auto input2_rng = std::bind(input2_distribution, std::ref(rng));
+
+  flatbuffers::FlatBufferBuilder builder;
+  std::vector<flatbuffers::Offset<OperatorCode>> operator_codes{
+      {CreateOperatorCode(builder, binary_op)}};
+  if (FP16Weights()) {
+    operator_codes.emplace_back(
+        CreateOperatorCode(builder, BuiltinOperator_DEQUANTIZE));
+  } else if (SparseWeights()) {
+    operator_codes.emplace_back(
+        CreateOperatorCode(builder, BuiltinOperator_DENSIFY));
+  }
+
+  std::vector<flatbuffers::Offset<Buffer>> buffers{{
+      CreateBuffer(builder, builder.CreateVector({})),
+  }};
+
+  int32_t input1_buffer = 0;
+  if (Input1Static()) {
+    if (FP16Weights()) {
+      std::vector<uint16_t> input1_data(ComputeSize(Input1Shape()));
+      std::generate(input1_data.begin(), input1_data.end(),
+                    std::bind(fp16_ieee_from_fp32_value, input1_rng));
+
+      buffers.push_back(CreateBuffer(
+          builder, builder.CreateVector(
+                       reinterpret_cast<const uint8_t*>(input1_data.data()),
+                       sizeof(uint16_t) * input1_data.size())));
+    } else {
+      std::vector<float> input1_data(ComputeSize(Input1Shape()));
+      std::generate(input1_data.begin(), input1_data.end(), input1_rng);
+
+      if (!SparseWeights()) {
+        input1_buffer = buffers.size();
+      }
+      buffers.push_back(CreateBuffer(
+          builder, builder.CreateVector(
+                       reinterpret_cast<const uint8_t*>(input1_data.data()),
+                       sizeof(float) * input1_data.size())));
+    }
+  }
+
+  int32_t input2_buffer = 0;
+  if (Input2Static()) {
+    if (FP16Weights()) {
+      std::vector<uint16_t> input2_data(ComputeSize(Input2Shape()));
+      std::generate(input2_data.begin(), input2_data.end(),
+                    std::bind(fp16_ieee_from_fp32_value, input1_rng));
+
+      buffers.push_back(CreateBuffer(
+          builder, builder.CreateVector(
+                       reinterpret_cast<const uint8_t*>(input2_data.data()),
+                       sizeof(uint16_t) * input2_data.size())));
+    } else {
+      std::vector<float> input2_data(ComputeSize(Input2Shape()));
+      std::generate(input2_data.begin(), input2_data.end(), input2_rng);
+
+      if (!SparseWeights()) {
+        input2_buffer = buffers.size();
+      }
+      buffers.push_back(CreateBuffer(
+          builder, builder.CreateVector(
+                       reinterpret_cast<const uint8_t*>(input2_data.data()),
+                       sizeof(float) * input2_data.size())));
+    }
+  }
+
+  const std::vector<int32_t> output_shape = OutputShape();
+  std::vector<flatbuffers::Offset<Tensor>> tensors;
+  std::vector<flatbuffers::Offset<Operator>> operators;
+  if (FP16Weights() && Input1Static()) {
+    tensors.emplace_back(
+        CreateTensor(builder,
+                     builder.CreateVector<int32_t>(Input1Shape().data(),
+                                                   Input1Shape().size()),
+                     TensorType_FLOAT16, 1));
+  } else if (SparseWeights() && Input1Static()) {
+    int dims_count = Input1Shape().size();
+    std::vector<flatbuffers::Offset<DimensionMetadata>> dim_metadata(
+        dims_count);
+    std::vector<int> traversal_order(dims_count);
+    for (int i = 0; i < dims_count; i++) {
+      traversal_order[i] = i;
+      dim_metadata[i] = CreateDimensionMetadata(builder, DimensionType_DENSE,
+                                                Input1Shape()[i]);
+    }
+    flatbuffers::Offset<SparsityParameters> sparsity_param =
+        CreateSparsityParameters(builder, builder.CreateVector(traversal_order),
+                                 0, builder.CreateVector(dim_metadata));
+    tensors.emplace_back(CreateTensor(
+        builder,
+        builder.CreateVector<int32_t>(Input1Shape().data(),
+                                      Input1Shape().size()),
+        TensorType_FLOAT32, /*buffer=*/1, /*name=*/0, /*quantization=*/0,
+        /*is_variable=*/false, /*sparsity=*/sparsity_param));
+  }
+  if (FP16Weights() && Input2Static()) {
+    tensors.emplace_back(
+        CreateTensor(builder,
+                     builder.CreateVector<int32_t>(Input2Shape().data(),
+                                                   Input2Shape().size()),
+                     TensorType_FLOAT16, 1));
+  } else if (SparseWeights() && Input2Static()) {
+    int dims_count = Input2Shape().size();
+    std::vector<flatbuffers::Offset<DimensionMetadata>> dim_metadata(
+        dims_count);
+    std::vector<int> traversal_order(dims_count);
+    for (int i = 0; i < dims_count; i++) {
+      traversal_order[i] = i;
+      dim_metadata[i] = CreateDimensionMetadata(builder, DimensionType_DENSE,
+                                                Input2Shape()[i]);
+    }
+    flatbuffers::Offset<SparsityParameters> sparsity_param =
+        CreateSparsityParameters(builder, builder.CreateVector(traversal_order),
+                                 0, builder.CreateVector(dim_metadata));
+    tensors.emplace_back(CreateTensor(
+        builder,
+        builder.CreateVector<int32_t>(Input2Shape().data(),
+                                      Input2Shape().size()),
+        TensorType_FLOAT32, /*buffer=*/1, /*name=*/0, /*quantization=*/0,
+        /*is_variable=*/false, /*sparsity=*/sparsity_param));
+  }
+  if (FP16Weights()) {
+    const std::array<int32_t, 1> dequantize_inputs{{0}};
+    const std::array<int32_t, 1> dequantize_outputs{{Input1Static() ? 1 : 2}};
+    operators.emplace_back(CreateOperator(
+        builder, /*opcode_index=*/1,
+        builder.CreateVector<int32_t>(dequantize_inputs.data(),
+                                      dequantize_inputs.size()),
+        builder.CreateVector<int32_t>(dequantize_outputs.data(),
+                                      dequantize_outputs.size())));
+  } else if (SparseWeights()) {
+    const std::array<int32_t, 1> densify_inputs{{0}};
+    const std::array<int32_t, 1> densify_outputs{{Input1Static() ? 1 : 2}};
+    operators.emplace_back(
+        CreateOperator(builder, /*opcode_index=*/1,
+                       builder.CreateVector<int32_t>(densify_inputs.data(),
+                                                     densify_inputs.size()),
+                       builder.CreateVector<int32_t>(densify_outputs.data(),
+                                                     densify_outputs.size())));
+  }
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(Input1Shape().data(), Input1Shape().size()),
+      TensorType_FLOAT32, input1_buffer));
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(Input2Shape().data(), Input2Shape().size()),
+      TensorType_FLOAT32, input2_buffer));
+  tensors.emplace_back(CreateTensor(
+      builder,
+      builder.CreateVector<int32_t>(output_shape.data(), output_shape.size()),
+      TensorType_FLOAT32));
+
+  tflite::BuiltinOptions builtin_options_type = tflite::BuiltinOptions_NONE;
+  flatbuffers::Offset<void> builtin_options = 0;
+  switch (binary_op) {
+    case BuiltinOperator_ADD:
+      builtin_options_type = BuiltinOptions_AddOptions;
+      builtin_options = CreateAddOptions(builder, Activation()).Union();
+      break;
+    case BuiltinOperator_DIV:
+      builtin_options_type = BuiltinOptions_DivOptions;
+      builtin_options = CreateDivOptions(builder, Activation()).Union();
+      break;
+    case BuiltinOperator_MUL:
+      builtin_options_type = BuiltinOptions_MulOptions;
+      builtin_options = CreateMulOptions(builder, Activation()).Union();
+      break;
+    case BuiltinOperator_SUB:
+      builtin_options_type = BuiltinOptions_SubOptions;
+      builtin_options = CreateSubOptions(builder, Activation()).Union();
+      break;
+    default:
+      EXPECT_EQ(Activation(), ActivationFunctionType_NONE);
+  }
+
+  const std::array<int32_t, 2> op_inputs{
+      {static_cast<int>(tensors.size()) - 3,
+       static_cast<int>(tensors.size()) - 2}};
+  const std::array<int32_t, 1> op_outputs{
+      {static_cast<int>(tensors.size()) - 1}};
+  operators.emplace_back(CreateOperator(
+      builder, /*opcode_index=*/0,
+      builder.CreateVector<int32_t>(op_inputs.data(), op_inputs.size()),
+      builder.CreateVector<int32_t>(op_outputs.data(), op_outputs.size()),
+      builtin_options_type, builtin_options));
+
+  std::vector<int32_t> subgraph_inputs;
+  if (!Input1Static()) {
+    subgraph_inputs.push_back(tensors.size() - 3);
+  }
+  if (!Input2Static()) {
+    subgraph_inputs.push_back(tensors.size() - 2);
+  }
+  const std::array<int32_t, 1> subgraph_outputs{
+      {static_cast<int>(tensors.size()) - 1}};
+  flatbuffers::Offset<SubGraph> subgraph = CreateSubGraph(
+      builder, builder.CreateVector(tensors.data(), tensors.size()),
+      builder.CreateVector<int32_t>(subgraph_inputs.data(),
+                                    subgraph_inputs.size()),
+      builder.CreateVector<int32_t>(subgraph_outputs.data(),
+                                    subgraph_outputs.size()),
+      builder.CreateVector(operators.data(), operators.size()));
+
+  flatbuffers::Offset<flatbuffers::String> description =
+      builder.CreateString("Binary operator model");
+
+  flatbuffers::Offset<Model> model_buffer = CreateModel(
+      builder, TFLITE_SCHEMA_VERSION,
+      builder.CreateVector(operator_codes.data(), operator_codes.size()),
+      builder.CreateVector(&subgraph, 1), description,
+      builder.CreateVector(buffers.data(), buffers.size()));
+
+  builder.Finish(model_buffer);
+
+  return std::vector<char>(builder.GetBufferPointer(),
+                           builder.GetBufferPointer() + builder.GetSize());
+}
+
+int32_t BinaryElementwiseTester::ComputeSize(
+    const std::vector<int32_t>& shape) {
+  return std::accumulate(shape.cbegin(), shape.cend(), 1,
+                         std::multiplies<int32_t>());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/binary_elementwise_tester.h b/tensorflow/lite/delegates/webnn/binary_elementwise_tester.h
new file mode 100644
index 00000000000..07c8381b0c1
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/binary_elementwise_tester.h
@@ -0,0 +1,140 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_BINARY_ELEMENTWISE_TESTER_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_BINARY_ELEMENTWISE_TESTER_H_
+
+#include <cstdint>
+#include <vector>
+
+#include <gtest/gtest.h>
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/schema/schema_generated.h"
+
+namespace tflite {
+namespace webnn {
+
+class BinaryElementwiseTester {
+ public:
+  BinaryElementwiseTester() = default;
+  BinaryElementwiseTester(const BinaryElementwiseTester&) = delete;
+  BinaryElementwiseTester& operator=(const BinaryElementwiseTester&) = delete;
+
+  inline BinaryElementwiseTester& Input1Shape(
+      std::initializer_list<int32_t> shape) {
+    for (auto it = shape.begin(); it != shape.end(); ++it) {
+      EXPECT_GT(*it, 0);
+    }
+    input1_shape_ = std::vector<int32_t>(shape.begin(), shape.end());
+    return *this;
+  }
+
+  inline const std::vector<int32_t>& Input1Shape() const {
+    return input1_shape_;
+  }
+
+  inline BinaryElementwiseTester& Input2Shape(
+      std::initializer_list<int32_t> shape) {
+    for (auto it = shape.begin(); it != shape.end(); ++it) {
+      EXPECT_GT(*it, 0);
+    }
+    input2_shape_ = std::vector<int32_t>(shape.begin(), shape.end());
+    return *this;
+  }
+
+  inline const std::vector<int32_t>& Input2Shape() const {
+    return input2_shape_;
+  }
+
+  std::vector<int32_t> OutputShape() const;
+
+  inline BinaryElementwiseTester& Input1Static(bool is_static) {
+    input1_static_ = is_static;
+    return *this;
+  }
+
+  inline bool Input1Static() const { return input1_static_; }
+
+  inline BinaryElementwiseTester& Input2Static(bool is_static) {
+    input2_static_ = is_static;
+    return *this;
+  }
+
+  inline bool Input2Static() const { return input2_static_; }
+
+  inline BinaryElementwiseTester& FP16Weights() {
+    fp16_weights_ = true;
+    return *this;
+  }
+
+  inline bool FP16Weights() const { return fp16_weights_; }
+
+  inline BinaryElementwiseTester& SparseWeights() {
+    sparse_weights_ = true;
+    return *this;
+  }
+
+  inline bool SparseWeights() const { return sparse_weights_; }
+
+  inline BinaryElementwiseTester& ReluActivation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU;
+    return *this;
+  }
+
+  inline BinaryElementwiseTester& Relu6Activation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU6;
+    return *this;
+  }
+
+  inline BinaryElementwiseTester& ReluMinus1To1Activation() {
+    activation_ = ::tflite::ActivationFunctionType_RELU_N1_TO_1;
+    return *this;
+  }
+
+  inline BinaryElementwiseTester& TanhActivation() {
+    activation_ = ::tflite::ActivationFunctionType_TANH;
+    return *this;
+  }
+
+  inline BinaryElementwiseTester& SignBitActivation() {
+    activation_ = ::tflite::ActivationFunctionType_SIGN_BIT;
+    return *this;
+  }
+
+  void Test(tflite::BuiltinOperator binary_op, TfLiteDelegate* delegate) const;
+
+ private:
+  std::vector<char> CreateTfLiteModel(tflite::BuiltinOperator binary_op) const;
+
+  inline ::tflite::ActivationFunctionType Activation() const {
+    return activation_;
+  }
+
+  static int32_t ComputeSize(const std::vector<int32_t>& shape);
+
+  std::vector<int32_t> input1_shape_;
+  std::vector<int32_t> input2_shape_;
+  bool input1_static_ = false;
+  bool input2_static_ = false;
+  bool fp16_weights_ = false;
+  bool sparse_weights_ = false;
+  ::tflite::ActivationFunctionType activation_ =
+      ::tflite::ActivationFunctionType_NONE;
+};
+
+}  // namespace webnn
+}  // namespace tflite
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_BINARY_ELEMENTWISE_TESTER_H_
diff --git a/tensorflow/lite/delegates/webnn/delegate_test.cc b/tensorflow/lite/delegates/webnn/delegate_test.cc
new file mode 100644
index 00000000000..1b45801c023
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/delegate_test.cc
@@ -0,0 +1,73 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <gtest/gtest.h>
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Delegate, CreateWithDefaultParams) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+}
+
+TEST(Delegate, CreateWithGpuPreferenceParam) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  delegate_options.devicePreference = 1;
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+}
+
+TEST(Delegate, CreateWithCpuPreferenceParam) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  delegate_options.devicePreference = 2;
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+}
+
+TEST(Delegate, CreateWithHighPowerPreferenceParam) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  delegate_options.powerPreference = 2;
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+}
+
+TEST(Delegate, CreateWithLowPowerPreferenceParam) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  delegate_options.powerPreference = 1;
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/mul_test.cc b/tensorflow/lite/delegates/webnn/mul_test.cc
new file mode 100644
index 00000000000..b0fb851c10e
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/mul_test.cc
@@ -0,0 +1,913 @@
+/* Copyright 2022 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <gtest/gtest.h>
+
+#include <cstdint>
+#include <functional>
+#include <memory>
+#include <random>
+
+#include "tensorflow/lite/delegates/webnn/binary_elementwise_tester.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+namespace webnn {
+
+TEST(Mul, 4DBy4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy4DBroadcastChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, 1, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, 1, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy4DBroadcastWidth) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, width, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, width, 1})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy4DBroadcastHeight) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, 1, 1})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy4DBroadcastBatch) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, 1, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, 1, 1, 1})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy4DBroadcastHeightWidthChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DBy0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, height, width, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 2DBy2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({batch, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 2DBy1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 2DBy0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, channels})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({})
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic4D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic4DBroadcastChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, 1, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, 1, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic4DBroadcastWidth) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, 1, width, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, 1, width, 1})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic4DBroadcastHeight) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, 1, 1})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic4DBroadcastBatch) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, 1, 1, 1})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, 1, 1, 1})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic4DBroadcastHeightWidthChannels) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({1, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({1, height, width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic3D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({height, width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({width, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 4DByStatic0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 2DByStatic2D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({batch, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({batch, channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 2DByStatic1D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({channels})
+      .Input2Shape({batch, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({channels})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, 2DByStatic0D) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({})
+      .Input2Shape({batch, channels})
+      .Input1Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, channels})
+      .Input2Shape({})
+      .Input2Static(true)
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, FP16Weights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .FP16Weights()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input2Static(true)
+      .FP16Weights()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, SparseWeights) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input1Static(true)
+      .SparseWeights()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Input2Static(true)
+      .SparseWeights()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, ReluActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .ReluActivation()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, Relu6Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .Relu6Activation()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, ReluMinus1To1Activation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .ReluMinus1To1Activation()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, DISABLED_TanhActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .TanhActivation()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+TEST(Mul, DISABLED_SignBitActivation) {
+  TfLiteWebNNDelegateOptions delegate_options =
+      TfLiteWebNNDelegateOptionsDefault();
+  std::unique_ptr<TfLiteDelegate, decltype(&TfLiteWebNNDelegateDelete)>
+      webnn_delegate(TfLiteWebNNDelegateCreate(&delegate_options),
+                     TfLiteWebNNDelegateDelete);
+
+  std::random_device random_device;
+  auto rng = std::mt19937(random_device());
+  auto shape_rng =
+      std::bind(std::uniform_int_distribution<int32_t>(2, 5), std::ref(rng));
+  const auto batch = shape_rng();
+  const auto height = shape_rng();
+  const auto width = shape_rng();
+  const auto channels = shape_rng();
+
+  BinaryElementwiseTester()
+      .Input1Shape({batch, height, width, channels})
+      .Input2Shape({batch, height, width, channels})
+      .SignBitActivation()
+      .Test(BuiltinOperator_MUL, webnn_delegate.get());
+}
+
+}  // namespace webnn
+}  // namespace tflite
diff --git a/tensorflow/lite/delegates/webnn/webnn_delegate.cc b/tensorflow/lite/delegates/webnn/webnn_delegate.cc
new file mode 100644
index 00000000000..60ff6c9415c
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/webnn_delegate.cc
@@ -0,0 +1,2733 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+#include <algorithm>
+#include <array>
+#include <cstdint>
+#include <cstring>
+#include <limits>
+#include <memory>
+#include <string>
+#include <unordered_map>
+#include <unordered_set>
+#include <utility>
+#include <vector>
+
+#include <webnn/webnn_cpp.h>
+#ifdef __EMSCRIPTEN__
+#include <emscripten.h>
+#include <emscripten/html5.h>
+#include <emscripten/html5_webnn.h>
+#else
+#include <webnn/webnn_proc.h>
+#include <webnn_native/WebnnNative.h>
+#endif
+
+#include <fp16/fp16.h>
+#include "tensorflow/lite/builtin_ops.h"
+#include "tensorflow/lite/c/builtin_op_data.h"
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/minimal_logging.h"
+#include "tensorflow/lite/kernels/internal/utils/sparsity_format_converter.h"
+
+namespace tflite {
+namespace webnn {
+namespace {
+
+// Forward declaration.
+TfLiteStatus DelegatePrepare(TfLiteContext* context, TfLiteDelegate* delegate);
+
+class Delegate {
+  friend class Subgraph;
+
+ public:
+  explicit Delegate(const TfLiteWebNNDelegateOptions* options) {
+    context_options_.devicePreference = static_cast<wnn::DevicePreference>(options->devicePreference);
+    context_options_.powerPreference = static_cast<wnn::PowerPreference>(options->powerPreference);
+    std::unordered_map<uint32_t, std::string> device_preference_names = {
+        {0, "Default"}, {1, "GPU"}, {2, "CPU"}};
+    std::unordered_map<uint32_t, std::string> power_preference_names = {
+        {0, "Default"}, {1, "High-performance"}, {2, "Low-power"}};
+#ifndef __EMSCRIPTEN__
+    instance_ = std::make_unique<webnn_native::Instance>();
+#endif
+    TFLITE_LOG_PROD_ONCE(tflite::TFLITE_LOG_INFO,
+                         "Created TensorFlow Lite WebNN delegate for device"
+                         " %s and power %s.",
+                         device_preference_names[options->devicePreference].c_str(),
+                         power_preference_names[options->powerPreference].c_str());
+  }
+
+  TfLiteIntArray* PrepareOpsToDelegate(TfLiteContext* context);
+  TfLiteDelegate* tflite_delegate() { return &delegate_; }
+
+ private:
+  TfLiteDelegate delegate_ = {
+      reinterpret_cast<void*>(this),  // .data_
+      DelegatePrepare,                // .Prepare
+      nullptr,                        // .CopyFromBufferHandle
+      nullptr,                        // .CopyToBufferHandle
+      nullptr,                        // .FreeBufferHandle
+      kTfLiteDelegateFlagsNone,       // .flags
+  };
+
+  // Unpacked data for quasi-static tensors, i.e. tensors produced by
+  // dequantizing or unpacking static buffers.
+  std::vector<char> static_unpacked_data_;
+  // Mapping from a tensor index for a quasi-static tensor to the offset to
+  // its unpacked data within static_unpacked_data_.
+  std::unordered_map<int, size_t> static_unpacked_data_map_;
+  // Set of indices of nodes which unpack static data, e.g. Dequantize
+  // operators which convert FP16 static weights to FP32. These nodes are simply
+  // ignored in the delegate implementation, because their outputs are
+  // pre-unpacked in DelegatePrepare.
+  std::unordered_set<int> static_unpack_nodes_;
+  // Set of indices of tensors with unpacked static sparse weights.
+  std::unordered_set<int> static_sparse_weights_;
+
+#ifndef __EMSCRIPTEN__
+  std::unique_ptr<webnn_native::Instance> instance_;
+#endif
+  wnn::ContextOptions context_options_;
+};
+
+class Subgraph {
+ public:
+  static Subgraph* Create(TfLiteContext* context,
+                          const TfLiteDelegateParams* params,
+                          const Delegate* delegate) {
+    // Convert subgraph inputs and outputs to hash sets for faster lookup.
+    const std::unordered_set<int> inputs(
+        &params->input_tensors->data[0],
+        &params->input_tensors->data[params->input_tensors->size]);
+    std::unordered_set<int> outputs;
+    for (int o = 0; o < params->output_tensors->size; o++) {
+      const int output_tensor_idx = params->output_tensors->data[o];
+      // Exclude quasi-static tensors which may have become subgraph outputs
+      // after partitioning.
+      if (delegate->static_unpacked_data_map_.count(output_tensor_idx) == 0) {
+        outputs.insert(output_tensor_idx);
+      }
+    }
+
+    TfLiteIntArray* execution_plan;
+    if (context->GetExecutionPlan(context, &execution_plan) != kTfLiteOk) {
+      return nullptr;
+    }
+
+    // Create WebNN context and graph builder
+#ifdef __EMSCRIPTEN__
+    wnn::Context wnn_context = emscripten_webnn_create_context(&(delegate->context_options_));
+#else
+    WebnnProcTable backend_procs = webnn_native::GetProcs();
+    webnnProcSetProcs(&backend_procs);
+    wnn::Context wnn_context = delegate->instance_->CreateContext(&(delegate->context_options_));
+
+#endif
+    if (!wnn_context) {
+      TF_LITE_KERNEL_LOG(context, "Failed to create WebNN context.");
+      return nullptr;
+    }
+    wnn::GraphBuilder wnn_builder = wnn::CreateGraphBuilder(wnn_context);
+    if (!wnn_builder) {
+      TF_LITE_KERNEL_LOG(context, "Failed to create WebNN graph builder.");
+      return nullptr;
+    }
+
+    bool has_sparse_weights = false;
+    // Detect which tensors are used as inputs or outputs of any subgraph nodes.
+    // -1 denotes tensor not used in the subgraph. These indexes will be
+    // filtered out and removed later.
+    std::vector<int> tensors(context->tensors_size, -1);
+    for (int i = 0; i < params->nodes_to_replace->size; i++) {
+      const int node_index = params->nodes_to_replace->data[i];
+
+      TfLiteNode* node = nullptr;
+      TfLiteRegistration* registration = nullptr;
+      if (context->GetNodeAndRegistration(context, node_index, &node,
+                                          &registration) != kTfLiteOk) {
+        return nullptr;
+      }
+
+      // Detect if any of the node's inputs are sparse weights.
+      if (!has_sparse_weights) {
+        for (int i = 0; i < node->inputs->size; i++) {
+          if (delegate->static_sparse_weights_.count(node->inputs->data[i]) !=
+              0) {
+            has_sparse_weights = true;
+          }
+        }
+      }
+
+      if (delegate->static_unpack_nodes_.count(node_index) != 0) {
+        // The node unpacks static input and can be skipped because its input
+        // was pre-unpacked in DelegatePrepare.
+        continue;
+      }
+
+      switch (registration->builtin_code) {
+        case kTfLiteBuiltinMean:
+        case kTfLiteBuiltinPad:
+        case kTfLiteBuiltinReshape:
+        case kTfLiteBuiltinResizeBilinear:
+          // Ignore the second input (new shape),
+          // because it is represented as parameters of the WebNN operator
+          // rather than extra input.
+          {
+            const int t = node->inputs->data[0];
+            tensors[t] = t;
+          }
+          break;
+        case kTfLiteBuiltinSplit:
+          // Ignore the first input (axis),
+          // because it is represented as parameters of the WebNN operator
+          // rather than extra input.
+          {
+            const int t = node->inputs->data[1];
+            tensors[t] = t;
+          }
+          break;
+        default:
+          // All other operators: process all inputs
+          for (int k = 0; k < node->inputs->size; k++) {
+            const int t = node->inputs->data[k];
+            if (t >= 0) {
+              tensors[t] = t;
+            }
+          }
+      }
+      for (int k = 0; k < node->outputs->size; k++) {
+        const int t = node->outputs->data[k];
+        if (t >= 0) {
+          tensors[t] = t;
+        }
+      }
+    }
+    // Filter out and remove -1 (unused) indexes.
+    tensors.erase(std::remove_if(tensors.begin(), tensors.end(),
+                                 [](int i) { return i < 0; }),
+                  tensors.end());
+    std::sort(tensors.begin(), tensors.end());
+
+    // Create a set of quasi-static tensors for VisitNode function
+    std::unordered_set<int> quasi_static_tensors;
+    for (const std::pair<const int, size_t>& entry :
+         delegate->static_unpacked_data_map_) {
+      quasi_static_tensors.insert(entry.first);
+    }
+
+    // WebNN operands for TFLite tensors
+    std::vector<wnn::Operand> webnn_operands(tensors.back() + 1);
+    std::unordered_set<int> compute_inputs;
+    for (int t : tensors) {
+      wnn::OperandType datatype;
+      switch (context->tensors[t].type) {
+        case kTfLiteFloat32:
+          datatype = wnn::OperandType::Float32;
+          break;
+        default:
+          TF_LITE_KERNEL_LOG(
+              context,
+              "unsupported datatype (%s) of tensor %d in WebNN delegate",
+              TfLiteTypeGetName(context->tensors[t].type), t);
+          return nullptr;
+      }
+
+      const void* data = nullptr;
+      if (context->tensors[t].allocation_type == kTfLiteMmapRo) {
+        data = context->tensors[t].data.raw_const;
+      } else {
+        // Check for quasi-static data.
+        const auto it = delegate->static_unpacked_data_map_.find(t);
+        if (it != delegate->static_unpacked_data_map_.end()) {
+          data = delegate->static_unpacked_data_.data() + it->second;
+        }
+      }
+
+      std::vector<int32_t> dims(
+          &context->tensors[t].dims->data[0],
+          &context->tensors[t].dims->data[context->tensors[t].dims->size]);
+
+      if (inputs.count(t) != 0 || quasi_static_tensors.count(t) != 0) {
+        wnn::OperandDescriptor desc;
+        desc.dimensions = dims.data();
+        desc.dimensionsCount = dims.size();
+        desc.type = datatype;
+
+        wnn::Operand operand;
+        if (data == nullptr) {
+          compute_inputs.insert(t);
+          std::string name = std::to_string(t);
+          operand = wnn_builder.Input(name.c_str(), &desc);
+        } else {
+          wnn::ArrayBufferView buffer = {const_cast<void*>(data), context->tensors[t].bytes};
+          // buffer.buffer = data;
+          // buffer.byteLength = context->tensors[t].bytes;
+          operand = wnn_builder.Constant(&desc, &buffer);
+        }
+        webnn_operands[t] = operand;
+      }
+    }
+
+    // Create WebNN nodes for TFLite delegate nodes
+    // keep the buffers of constants created during graph building.
+    std::vector<std::unique_ptr<char>> constant_buffers;
+    for (int i = 0; i < params->nodes_to_replace->size; i++) {
+      const int node_index = params->nodes_to_replace->data[i];
+      if (delegate->static_unpack_nodes_.count(node_index)) {
+        // The node unpacks static input and can be skipped because its input
+        // was pre-unpacked in DelegatePrepare.
+        continue;
+      }
+
+      TfLiteNode* node = nullptr;
+      TfLiteRegistration* registration = nullptr;
+      if (context->GetNodeAndRegistration(context, node_index, &node,
+                                          &registration) != kTfLiteOk) {
+        return nullptr;
+      }
+
+      if (VisitNode(wnn_builder, context, registration, node, node_index,
+                    quasi_static_tensors, webnn_operands, constant_buffers) != kTfLiteOk) {
+        return nullptr;
+      }
+    }
+
+    wnn::NamedOperands named_operands = wnn::CreateNamedOperands();
+    for (auto o : outputs) {
+      std::string name = std::to_string(o);
+      if (!webnn_operands[o]) {
+        TF_LITE_KERNEL_LOG(context, "Invalid operand");
+        return nullptr;
+      }
+      named_operands.Set(name.c_str(), webnn_operands[o]);
+    }
+
+    wnn::Graph wnn_graph = wnn_builder.Build(named_operands);
+    if (!wnn_graph) {
+      TF_LITE_KERNEL_LOG(context, "failed to build WebNN graph");
+      return nullptr;
+    }
+
+    return new Subgraph(wnn_graph, std::move(compute_inputs), std::move(outputs));
+  }
+
+  TfLiteStatus Prepare(TfLiteContext* context) { return kTfLiteOk; }
+
+  TfLiteStatus Invoke(TfLiteContext* context) {
+    bool any_pointers_changed = false;
+    for (std::pair<int, void*> io_info : externals_) {
+      const TfLiteTensor& tensor = context->tensors[io_info.first];
+      void* data_pointer = &dummy_data_;
+      if (tensor.data.raw != nullptr) {
+        data_pointer = tensor.data.raw;
+      } else {
+        if (tensor.bytes != 0) {
+          TF_LITE_KERNEL_LOG(
+              context, "unexpected null data pointer in external tensor %d",
+              io_info.first);
+          return kTfLiteError;
+        }
+      }
+      if (data_pointer != io_info.second) {
+        any_pointers_changed = true;
+        externals_[io_info.first] = data_pointer;
+      }
+    }
+
+    if (any_pointers_changed) {
+      graph_inputs_ = wnn::CreateNamedInputs();
+      for (int t : inputs_) {
+        wnn_inputs_[t].resource.arrayBufferView.buffer = context->tensors[t].data.raw;
+        wnn_inputs_[t].resource.arrayBufferView.byteLength = context->tensors[t].bytes;
+        std::string name = std::to_string(t);
+        graph_inputs_.Set(name.c_str(), &wnn_inputs_[t]);
+      }
+
+      graph_outputs_ = wnn::CreateNamedOutputs();
+      for (int t : outputs_) {
+        wnn_outputs_[t].arrayBufferView.buffer = context->tensors[t].data.raw;
+        wnn_outputs_[t].arrayBufferView.byteLength = context->tensors[t].bytes;
+        std::string name = std::to_string(t);
+        graph_outputs_.Set(name.c_str(), &wnn_outputs_[t]);
+      }
+    }
+
+    wnn_graph_.Compute(graph_inputs_, graph_outputs_);
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CalculatePadding(TfLiteContext* context,
+                                       TfLitePadding padding, wnn::AutoPad& auto_pad,
+                                       int node_index) {
+    switch (padding) {
+      case kTfLitePaddingSame: {
+        auto_pad = wnn::AutoPad::SameUpper;
+        return kTfLiteOk;
+      }
+      case kTfLitePaddingValid:
+        auto_pad = wnn::AutoPad::Explicit;
+        return kTfLiteOk;
+      default:
+        TF_LITE_MAYBE_KERNEL_LOG(context,
+                                 "invalid padding mode (%d) in node #%d",
+                                 static_cast<int>(padding), node_index);
+        return kTfLiteError;
+    }
+  }
+
+  static TfLiteStatus CheckConvolutionParams(TfLiteContext* context,
+                                             const TfLiteConvParams* params,
+                                             int node_index) {
+    if (params->stride_width <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride width %d in node #%d",
+                               params->stride_width, node_index);
+      return kTfLiteError;
+    }
+    if (params->stride_height <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride height %d in node #%d",
+                               params->stride_height, node_index);
+      return kTfLiteError;
+    }
+
+    if (params->dilation_width_factor <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "invalid dilation width factor %d in node #%d",
+                               params->dilation_width_factor, node_index);
+      return kTfLiteError;
+    }
+    if (params->dilation_height_factor <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "invalid dilation height factor %d in node #%d",
+                               params->dilation_height_factor, node_index);
+      return kTfLiteError;
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckDepthwiseConvolutionParams(
+      TfLiteContext* context, const TfLiteDepthwiseConvParams* params,
+      int output_channels, int node_index) {
+    if (params->stride_width <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride width %d in node #%d",
+                               params->stride_width, node_index);
+      return kTfLiteError;
+    }
+    if (params->stride_height <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride height %d in node #%d",
+                               params->stride_height, node_index);
+      return kTfLiteError;
+    }
+
+    if (params->depth_multiplier <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "invalid depth multiplier %d in node #%d",
+                               params->depth_multiplier, node_index);
+      return kTfLiteError;
+    }
+    if (output_channels % params->depth_multiplier != 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "depth multiplier %d is incompatible with "
+                               "number of output channels %d in node #%d",
+                               params->depth_multiplier, output_channels,
+                               node_index);
+      return kTfLiteError;
+    }
+
+    if (params->dilation_width_factor <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "invalid dilation width factor %d in node #%d",
+                               params->dilation_width_factor, node_index);
+      return kTfLiteError;
+    }
+    if (params->dilation_height_factor <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "invalid dilation height factor %d in node #%d",
+                               params->dilation_height_factor, node_index);
+      return kTfLiteError;
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckMediaPipeTransposedConvolutionParams(
+      TfLiteContext* context, const TfLiteTransposeConvParams* params,
+      int node_index) {
+    if (params->stride_width <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride width %d in node #%d",
+                               params->stride_width, node_index);
+      return kTfLiteError;
+    }
+    if (params->stride_height <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride height %d in node #%d",
+                               params->stride_height, node_index);
+      return kTfLiteError;
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckFullyConnectedParams(
+      TfLiteContext* context, const TfLiteFullyConnectedParams* params,
+      int node_index) {
+    if (params->weights_format != kTfLiteFullyConnectedWeightsFormatDefault) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context, "unsupported non-default weights format in node #%d",
+          node_index);
+      return kTfLiteError;
+    }
+
+    if (params->asymmetric_quantize_inputs) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context, "unsupported asymmetric quantize inputs in node #%d",
+          node_index);
+      return kTfLiteError;
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckPoolingParams(TfLiteContext* context,
+                                         const TfLitePoolParams* params,
+                                         int node_index) {
+    if (params->stride_width <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride width %d in node #%d",
+                               params->stride_width, node_index);
+      return kTfLiteError;
+    }
+    if (params->stride_height <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid stride height %d in node #%d",
+                               params->stride_height, node_index);
+      return kTfLiteError;
+    }
+
+    if (params->filter_width <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid filter width %d in node #%d",
+                               params->filter_width, node_index);
+      return kTfLiteError;
+    }
+    if (params->filter_height <= 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(context, "invalid filter height %d in node #%d",
+                               params->filter_height, node_index);
+      return kTfLiteError;
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckNumInputsAndOutputs(
+      TfLiteContext* context, TfLiteNode* node, int min_num_inputs,
+      int max_num_inputs, int expected_num_outputs, int node_index) {
+    if (node->inputs->size < min_num_inputs ||
+        node->inputs->size > max_num_inputs) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "unexpected number of inputs (%d) in node #%d",
+                               node->inputs->size, node_index);
+      return kTfLiteError;
+    }
+    if (node->outputs->size != expected_num_outputs) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context, "unexpected number of outputs (%d != %d) in node #%d",
+          node->outputs->size, expected_num_outputs, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckNumInputsAndOutputs(TfLiteContext* context,
+                                               TfLiteNode* node,
+                                               int expected_num_inputs,
+                                               int expected_num_outputs,
+                                               int node_index) {
+    if (node->inputs->size != expected_num_inputs) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context, "unexpected number of inputs (%d != %d) in node #%d",
+          node->inputs->size, expected_num_inputs, node_index);
+      return kTfLiteError;
+    }
+    if (node->outputs->size != expected_num_outputs) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context, "unexpected number of outputs (%d != %d) in node #%d",
+          node->outputs->size, expected_num_outputs, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckTensorType(TfLiteContext* context,
+                                      const TfLiteTensor& tensor,
+                                      TfLiteType expected_type,
+                                      int tensor_index, int node_index) {
+    if (tensor.type != expected_type) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context, "unsupported type %s in tensor #%d in node #%d",
+          TfLiteTypeGetName(tensor.type), tensor_index, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckTensorFloat32Type(TfLiteContext* context,
+                                             const TfLiteTensor& tensor,
+                                             int tensor_index, int node_index) {
+    return CheckTensorType(context, tensor, kTfLiteFloat32, tensor_index,
+                           node_index);
+  }
+
+  static TfLiteStatus CheckTensorFloat32OrQInt8Type(TfLiteContext* context,
+                                                    const TfLiteTensor& tensor,
+                                                    int tensor_index,
+                                                    int node_index) {
+    switch (tensor.type) {
+      case kTfLiteFloat32:
+        break;
+      default:
+        TF_LITE_MAYBE_KERNEL_LOG(
+            context, "unsupported type %s in tensor #%d in node #%d",
+            TfLiteTypeGetName(tensor.type), tensor_index, node_index);
+        return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckTensorFloat32OrQInt32Type(TfLiteContext* context,
+                                                     const TfLiteTensor& tensor,
+                                                     int tensor_index,
+                                                     int node_index) {
+    switch (tensor.type) {
+      case kTfLiteFloat32:
+        break;
+      default:
+        TF_LITE_MAYBE_KERNEL_LOG(
+            context, "unsupported type %s in tensor #%d in node #%d",
+            TfLiteTypeGetName(tensor.type), tensor_index, node_index);
+        return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckTensorShape(TfLiteContext* context,
+                                       const TfLiteTensor& tensor,
+                                       int min_num_dims, int max_num_dims,
+                                       int tensor_index) {
+    if (min_num_dims == max_num_dims) {
+      if (tensor.dims->size != min_num_dims) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            context,
+            "unsupported number of shape dimensions (%d) in tensor #%d: "
+            "%d dimensions expected",
+            tensor.dims->size, tensor_index, min_num_dims);
+        return kTfLiteError;
+      }
+    } else {
+      if (tensor.dims->size < min_num_dims) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            context,
+            "unsupported number of shape dimensions (%d) in tensor #%d: "
+            "at least %d dimensions expected",
+            tensor.dims->size, tensor_index, min_num_dims);
+        return kTfLiteError;
+      }
+      if (tensor.dims->size > max_num_dims) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            context,
+            "unsupported number of shape dimensions (%d) in tensor #%d: "
+            "at most %d dimensions expected",
+            tensor.dims->size, tensor_index, max_num_dims);
+        return kTfLiteError;
+      }
+    }
+    for (int i = 0; i < tensor.dims->size; i++) {
+      if (tensor.dims->data[i] <= 0) {
+        TF_LITE_MAYBE_KERNEL_LOG(context,
+                                 "invalid num of elements (%d) in "
+                                 "dimension #%d in tensor #%d",
+                                 tensor.dims->data[i], i, tensor_index);
+        return kTfLiteError;
+      }
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckTensorShape(TfLiteContext* context,
+                                       const TfLiteTensor& tensor,
+                                       int expected_num_dims,
+                                       int tensor_index) {
+    return CheckTensorShape(context, tensor, expected_num_dims,
+                            expected_num_dims, tensor_index);
+  }
+
+  static TfLiteStatus CheckPaddingsTensorShape(TfLiteContext* context,
+                                               const TfLiteTensor& tensor,
+                                               int expected_rows,
+                                               int tensor_index,
+                                               int node_index) {
+    if (tensor.dims->size != 2) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "unexpected number of shape dimensions (%d) in "
+                               "padding tensor #%d in node #%d: "
+                               "expected a 2D tensor",
+                               tensor.dims->size, tensor_index, node_index);
+      return kTfLiteError;
+    }
+    if (tensor.dims->data[0] != expected_rows) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "unexpected number of rows (%d) in "
+                               "padding tensor #%d in node #%d: "
+                               "%d rows expected",
+                               tensor.dims->size, tensor_index, node_index,
+                               expected_rows);
+      return kTfLiteError;
+    }
+    if (tensor.dims->data[1] != 2) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "unexpected number of columns (%d) in "
+                               "padding tensor #%d in node #%d: "
+                               "2 columns expected",
+                               tensor.dims->size, tensor_index, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckAxesTensorShape(TfLiteContext* context,
+                                           const TfLiteTensor& tensor,
+                                           int tensor_index, int node_index) {
+    if (tensor.dims->size != 1) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "unexpected number of shape dimensions (%d) in "
+                               "axes tensor #%d in node #%d: "
+                               "expected a 1D tensor",
+                               tensor.dims->size, tensor_index, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckShapeTensorShape(TfLiteContext* context,
+                                            const TfLiteTensor& tensor,
+                                            int tensor_index, int node_index) {
+    if (tensor.dims->size != 1) {
+      TF_LITE_MAYBE_KERNEL_LOG(context,
+                               "unexpected number of shape dimensions (%d) in "
+                               "shape tensor #%d in node #%d: "
+                               "expected a 1D tensor",
+                               tensor.dims->size, tensor_index, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckTensorNonDynamicAllocation(
+      TfLiteContext* context, const TfLiteTensor& tensor, int tensor_index,
+      int node_index) {
+    // TODO: remove checks once dynamic tensors are supported
+    if (tensor.allocation_type == kTfLiteDynamic) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context,
+          "invalid allocation type in tensor #%d in node #%d: "
+          "expected non-dynamic tensor",
+          tensor_index, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus CheckTensorStaticAllocation(TfLiteContext* context,
+                                                  const TfLiteTensor& tensor,
+                                                  int tensor_index,
+                                                  int node_index) {
+    if (tensor.allocation_type != kTfLiteMmapRo ||
+        tensor.data.raw_const == nullptr) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          context,
+          "invalid allocation type in tensor #%d in node #%d: "
+          "expected static read-only tensor",
+          tensor_index, node_index);
+      return kTfLiteError;
+    }
+    return kTfLiteOk;
+  }
+
+  static wnn::Operand BuildClamp(
+      const wnn::GraphBuilder& builder, const wnn::Operand& input,
+      float min_value, float max_value, std::vector<std::unique_ptr<char>>& constant_buffers) {
+    wnn::ClampOptions options;
+    options.minValue = min_value;
+    options.maxValue = max_value;
+    return builder.Clamp(input, &options);
+  }
+
+  static wnn::FusionOperator GetClampOperator(
+      const wnn::GraphBuilder& builder, float min_value, float max_value) {
+    wnn::ClampOptions options;
+    options.minValue = min_value;
+    options.maxValue = max_value;
+    return builder.ClampOperator(&options);
+  }
+
+  static TfLiteStatus GetActivation(
+      const wnn::GraphBuilder& builder, TfLiteContext* context, int node_index,
+      TfLiteFusedActivation activation, wnn::FusionOperator& activation_operator) {
+    switch (activation) {
+      case kTfLiteActRelu:
+        activation_operator = builder.ReluOperator();
+        return kTfLiteOk;
+      case kTfLiteActReluN1To1:
+        activation_operator = GetClampOperator(builder, -1.0f, +1.0f);
+        return kTfLiteOk;
+      case kTfLiteActRelu6:
+        activation_operator = GetClampOperator(builder, 0.0f, 6.0f);
+        return kTfLiteOk;
+      case kTfLiteActTanh:
+        activation_operator = builder.TanhOperator();
+        return kTfLiteOk;
+      case kTfLiteActSignBit:
+        TF_LITE_MAYBE_KERNEL_LOG(
+            context, "unsupported fused activation (Sign) in node #%d",
+            node_index);
+        return kTfLiteError;
+      case kTfLiteActSigmoid:
+          activation_operator = builder.SigmoidOperator();
+      default:
+        TF_LITE_MAYBE_KERNEL_LOG(context,
+                                 "invalid fused activation (%d) in node #%d",
+                                 static_cast<int>(activation), node_index);
+        return kTfLiteError;
+    }
+  }
+
+  static TfLiteStatus VisitActivation(
+      const wnn::GraphBuilder& builder, TfLiteContext* context, int node_index,
+      int input_tensor_id, int output_tensor_id, TfLiteFusedActivation activation,
+      std::vector<wnn::Operand>& webnn_operands, std::vector<std::unique_ptr<char>>& constant_buffers) {
+    switch (activation) {
+      case kTfLiteActNone:
+        return kTfLiteOk;
+      case kTfLiteActRelu:
+        if (builder) {
+          webnn_operands[output_tensor_id] = builder.Relu(webnn_operands[input_tensor_id]);
+        }
+        return kTfLiteOk;
+      case kTfLiteActReluN1To1:
+        if (builder) {
+          webnn_operands[output_tensor_id] = BuildClamp(
+              builder, webnn_operands[input_tensor_id], -1.0f, +1.0f, constant_buffers);
+        }
+        return kTfLiteOk;
+      case kTfLiteActRelu6:
+        if (builder) {
+          webnn_operands[output_tensor_id] = BuildClamp(
+              builder, webnn_operands[input_tensor_id], 0.0f, 6.0f, constant_buffers);
+        }
+        return kTfLiteOk;
+      case kTfLiteActTanh:
+        if (builder) {
+          webnn_operands[output_tensor_id] = builder.Tanh(webnn_operands[input_tensor_id]);
+        }
+        return kTfLiteOk;
+      case kTfLiteActSignBit:
+        TF_LITE_MAYBE_KERNEL_LOG(
+            context, "unsupported fused activation (Sign) in node #%d",
+            node_index);
+        return kTfLiteError;
+      case kTfLiteActSigmoid:
+        if (builder) {
+          webnn_operands[output_tensor_id] = builder.Sigmoid(webnn_operands[input_tensor_id]);
+        }
+        return kTfLiteOk;
+      default:
+        TF_LITE_MAYBE_KERNEL_LOG(context,
+                                 "invalid fused activation (%d) in node #%d",
+                                 static_cast<int>(activation), node_index);
+        return kTfLiteError;
+    }
+  }
+
+  static TfLiteStatus VisitNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* context,
+      TfLiteRegistration* registration, TfLiteNode* node, int node_index,
+      const std::unordered_set<int>& quasi_static_tensors,
+      std::vector<wnn::Operand>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    // TFLite context used for logging purposes. When we create a new node
+    // (subgraph is non-null), logging context is the same as context, and error
+    // messages are passed to TFLite. When we detect supported operations
+    // (subgraph is null), logging context is null, and error messages are
+    // supressed.
+    TfLiteContext* logging_context = builder == nullptr ? nullptr : context;
+    switch (registration->builtin_code) {
+      case kTfLiteBuiltinAdd: {
+        const TfLiteAddParams* add_params =
+            static_cast<const TfLiteAddParams*>(node->builtin_data);
+
+        return VisitAddNode(builder, logging_context, node_index, node,
+                            context->tensors, add_params, webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinMul: {
+        const TfLiteMulParams* mul_params =
+            static_cast<const TfLiteMulParams*>(node->builtin_data);
+
+        return VisitMulNode(builder, logging_context, node_index, node,
+                            context->tensors, mul_params, webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinPad:
+        return VisitPadNode(builder, logging_context, node_index, node,
+                            context->tensors, webnn_operands, constant_buffers);
+      case kTfLiteBuiltinAveragePool2d: {
+        const TfLitePoolParams* pool_params =
+            static_cast<const TfLitePoolParams*>(node->builtin_data);
+
+        return VisitAveragePool2DNode(builder, logging_context, node_index,
+                                      node, context->tensors, pool_params,
+                                      webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinMaxPool2d: {
+        const TfLitePoolParams* pool_params =
+            static_cast<const TfLitePoolParams*>(node->builtin_data);
+
+        return VisitMaxPool2DNode(builder, logging_context, node_index,
+                                  node, context->tensors, pool_params,
+                                  webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinMean: {
+        const TfLiteReducerParams* reducer_params =
+            static_cast<const TfLiteReducerParams*>(node->builtin_data);
+
+        return VisitMeanNode(builder, logging_context, node_index,
+                             node, context->tensors, reducer_params,
+                             webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinConcatenation: {
+        const TfLiteConcatenationParams* concat_params =
+            static_cast<const TfLiteConcatenationParams*>(node->builtin_data);
+
+        return VisitConcatenationNode(builder, logging_context, node_index, node,
+                                      context->tensors, concat_params,
+                                      webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinConv2d: {
+        const TfLiteConvParams* conv_params =
+            static_cast<const TfLiteConvParams*>(node->builtin_data);
+
+        return VisitConv2DNode(builder, logging_context, node_index, node,
+                               context->tensors, conv_params,
+                               quasi_static_tensors, webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinDepthwiseConv2d: {
+        const TfLiteDepthwiseConvParams* dwconv_params =
+            static_cast<const TfLiteDepthwiseConvParams*>(node->builtin_data);
+
+        return VisitDepthwiseConv2DNode(builder, logging_context, node_index,
+                                        node, context->tensors, dwconv_params,
+                                        quasi_static_tensors, webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinFullyConnected: {
+        const TfLiteFullyConnectedParams* fc_params =
+            static_cast<const TfLiteFullyConnectedParams*>(node->builtin_data);
+
+        return VisitFullyConnectedNode(builder, logging_context, node_index, node,
+                                       context->tensors, fc_params, quasi_static_tensors,
+                                       webnn_operands, constant_buffers);
+      }
+      case kTfLiteBuiltinHardSwish:
+        return VisitHardSwishNode(builder, logging_context, node_index, node,
+                                  context->tensors, webnn_operands);
+      case kTfLiteBuiltinLogistic:
+        return VisitLogisticNode(builder, logging_context, node_index, node,
+                                 context->tensors, webnn_operands);
+      case kTfLiteBuiltinRelu:
+        return VisitReluNode(builder, logging_context, node_index, node,
+                             context->tensors, webnn_operands);
+      case kTfLiteBuiltinReshape: {
+        const TfLiteReshapeParams* reshape_params =
+            static_cast<const TfLiteReshapeParams*>(node->builtin_data);
+
+        return VisitReshapeNode(builder, logging_context, node_index, node,
+                                context->tensors, reshape_params, webnn_operands);
+      }
+      case kTfLiteBuiltinResizeBilinear: {
+        const TfLiteResizeBilinearParams* resize_params =
+            static_cast<const TfLiteResizeBilinearParams*>(node->builtin_data);
+
+        return VisitResizeBilinearNode(builder, logging_context, node_index,
+                                       node, context->tensors, resize_params,
+                                       webnn_operands);
+      }
+      case kTfLiteBuiltinSoftmax: {
+        const TfLiteSoftmaxParams* softmax_params =
+            static_cast<const TfLiteSoftmaxParams*>(node->builtin_data);
+
+        return VisitSoftmaxNode(builder, logging_context, node_index, node,
+                                context->tensors, softmax_params, webnn_operands);
+      }
+      case kTfLiteBuiltinSplit: {
+        const TfLiteSplitParams* split_params =
+            static_cast<const TfLiteSplitParams*>(node->builtin_data);
+
+        return VisitSplitNode(builder, logging_context, node_index, node,
+                                context->tensors, split_params, webnn_operands);
+      }
+      case kTfLiteBuiltinTanh:
+        return VisitTanhNode(builder, logging_context, node_index, node,
+                             context->tensors, webnn_operands);
+      case kTfLiteBuiltinUnpack: {
+        const TfLiteUnpackParams* unpack_params =
+            static_cast<const TfLiteUnpackParams*>(node->builtin_data);
+
+        return VisitUnpackNode(builder, logging_context, node_index, node,
+                               context->tensors, unpack_params, webnn_operands);
+      }
+      case kTfLiteBuiltinCustom: {
+        if (strcmp(registration->custom_name, "Convolution2DTransposeBias") ==
+            0) {
+          TfLiteTransposeConvParams deconv_params = {kTfLitePaddingUnknown};
+          std::memcpy(&deconv_params, node->custom_initial_data,
+                      node->custom_initial_data_size);
+
+          return VisitMediaPipeDeconvolutionNode(
+              builder, context, node_index, node, context->tensors,
+              &deconv_params, quasi_static_tensors, webnn_operands);
+        }
+        return kTfLiteError;
+      }
+      default:
+        return kTfLiteError;
+    }
+  }
+
+  static TfLiteStatus VisitAddNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteAddParams* add_params,
+      std::vector<wnn::Operand>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, 1, node_index));
+
+    const int input1_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input1_tensor = tensors[input1_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input1_tensor, input1_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input1_tensor, input1_tensor_id, node_index));
+
+    const int input2_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& input2_tensor = tensors[input2_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input2_tensor, input2_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input2_tensor, input2_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    if (builder) {
+      TF_LITE_ENSURE(logging_context, webnn_operands[input1_tensor_id]);
+      TF_LITE_ENSURE(logging_context, webnn_operands[input2_tensor_id]);
+      webnn_operands[output_tensor_id] =
+          builder.Add(webnn_operands[input1_tensor_id], webnn_operands[input2_tensor_id]);
+      TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+    }
+
+    if (add_params != nullptr) {
+      TF_LITE_ENSURE_STATUS(VisitActivation(
+          builder, logging_context, node_index, output_tensor_id, output_tensor_id,
+          add_params->activation, webnn_operands, constant_buffers));
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitMulNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteMulParams* mul_params,
+      std::vector<wnn::Operand>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, 1, node_index));
+
+    const int input1_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input1_tensor = tensors[input1_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input1_tensor, input1_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input1_tensor, input1_tensor_id, node_index));
+
+    const int input2_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& input2_tensor = tensors[input2_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input2_tensor, input2_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input2_tensor, input2_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    if (builder) {
+      TF_LITE_ENSURE(logging_context, webnn_operands[input1_tensor_id]);
+      TF_LITE_ENSURE(logging_context, webnn_operands[input2_tensor_id]);
+      webnn_operands[output_tensor_id] =
+          builder.Mul(webnn_operands[input1_tensor_id], webnn_operands[input2_tensor_id]);
+      TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+    }
+
+    if (mul_params != nullptr) {
+      TF_LITE_ENSURE_STATUS(VisitActivation(
+          builder, logging_context, node_index, output_tensor_id, output_tensor_id,
+          mul_params->activation, webnn_operands, constant_buffers));
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitPadNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      std::vector<wnn::Operand>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int padding_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& paddings_tensor = tensors[padding_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorType(logging_context, paddings_tensor,
+                                          kTfLiteInt32, padding_tensor_id,
+                                          node_index));
+    TF_LITE_ENSURE_STATUS(CheckPaddingsTensorShape(
+        logging_context, paddings_tensor, input_tensor.dims->size,
+        padding_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+        logging_context, paddings_tensor, padding_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    const int32_t* paddings_data =
+        reinterpret_cast<const int32_t*>(paddings_tensor.data.data);
+    for (int i = 0; i < paddings_tensor.dims->data[0]; i++) {
+      const int32_t pre_padding = paddings_data[i * 2 + 0];
+      if (pre_padding < 0) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            logging_context,
+            "invalid pre-padding %d for dimension #%d in node %d", pre_padding,
+            i, node_index);
+        return kTfLiteError;
+      }
+
+      const int32_t post_padding = paddings_data[i * 2 + 1];
+      if (post_padding < 0) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            logging_context,
+            "invalid post-padding %d for dimension #%d in node %d", pre_padding,
+            i, node_index);
+        return kTfLiteError;
+      }
+    }
+
+    if (builder) {
+      size_t rank = paddings_tensor.dims->data[0];
+      std::vector<int32_t> padding(rank * 2);
+      for (int i = 0; i < rank; i++) {
+        padding[i * 2 + 0] = static_cast<int32_t>(paddings_data[i * 2 + 0]);
+        padding[i * 2 + 1] = static_cast<int32_t>(paddings_data[i * 2 + 1]);
+      }
+      const size_t padding_buffer_length = sizeof(int32_t) * padding.size();
+      std::unique_ptr<char> padding_buffer(new char[padding_buffer_length]);
+      std::memcpy(padding_buffer.get(), padding.data(), padding_buffer_length);
+      std::vector<int32_t> dims = {static_cast<int32_t>(rank), 2};
+      wnn::OperandDescriptor desc = {
+        wnn::OperandType::Int32, dims.data(), static_cast<uint32_t>(dims.size())};
+      wnn::ArrayBufferView padding_buffer_view = {padding_buffer.get(), padding_buffer_length};
+      wnn::Operand padding_operand = builder.Constant(&desc, &padding_buffer_view);
+      constant_buffers.push_back(std::move(padding_buffer));
+
+      TF_LITE_ENSURE(logging_context, webnn_operands[input_tensor_id]);
+      webnn_operands[output_tensor_id] =
+          builder.Pad(webnn_operands[input_tensor_id], padding_operand);
+      TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitAveragePool2DNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLitePoolParams* pool_params,
+      std::vector<wnn::Operand>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    TF_LITE_ENSURE_STATUS(
+        CheckPoolingParams(logging_context, pool_params, node_index));
+
+    wnn::AutoPad auto_pad;
+    TF_LITE_ENSURE_STATUS(CalculatePadding(
+        logging_context, pool_params->padding, auto_pad, node_index));
+
+    if (builder) {
+      wnn::Operand output;
+      TF_LITE_ENSURE(logging_context, webnn_operands[input_tensor_id]);
+      if (pool_params->filter_height == 1 && pool_params->filter_width == 1) {
+        // Only do activation.
+        output = webnn_operands[input_tensor_id];
+      } else {
+        wnn::Pool2dOptions options;
+        options.autoPad = auto_pad;
+        std::vector<int32_t> strides = {
+            pool_params->stride_height, pool_params->stride_width};
+        options.strides = strides.data();
+        options.stridesCount = strides.size();
+        std::vector<int32_t> windowDimensions = {
+            pool_params->filter_height, pool_params->filter_width};
+        options.windowDimensions = windowDimensions.data();
+        options.windowDimensionsCount = windowDimensions.size();
+        options.layout = wnn::InputOperandLayout::Nhwc;
+        output = builder.AveragePool2d(webnn_operands[input_tensor_id], &options);
+      }
+      webnn_operands[output_tensor_id] = output;
+      TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+    }
+
+    TF_LITE_ENSURE_STATUS(VisitActivation(
+          builder, logging_context, node_index, output_tensor_id, output_tensor_id,
+          pool_params->activation, webnn_operands, constant_buffers));
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitMaxPool2DNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLitePoolParams* pool_params,
+      std::vector<wnn::Operand>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    TF_LITE_ENSURE_STATUS(
+        CheckPoolingParams(logging_context, pool_params, node_index));
+
+    wnn::AutoPad auto_pad;
+    TF_LITE_ENSURE_STATUS(CalculatePadding(
+        logging_context, pool_params->padding, auto_pad, node_index));
+
+    if (builder) {
+      wnn::Operand output;
+      wnn::Pool2dOptions options;
+      options.autoPad = auto_pad;
+      std::vector<int32_t> strides = {
+          pool_params->stride_height, pool_params->stride_width};
+      options.strides = strides.data();
+      options.stridesCount = strides.size();
+      std::vector<int32_t> windowDimensions = {
+          pool_params->filter_height, pool_params->filter_width};
+      options.windowDimensions = windowDimensions.data();
+      options.windowDimensionsCount = windowDimensions.size();
+      options.layout = wnn::InputOperandLayout::Nhwc;
+      TF_LITE_ENSURE(logging_context, webnn_operands[input_tensor_id]);
+      output = builder.MaxPool2d(webnn_operands[input_tensor_id], &options);
+      webnn_operands[output_tensor_id] = output;
+      TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+    }
+
+    TF_LITE_ENSURE_STATUS(VisitActivation(
+          builder, logging_context, node_index, output_tensor_id, output_tensor_id,
+          pool_params->activation, webnn_operands, constant_buffers));
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitMeanNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteReducerParams* reducer_params,
+      std::vector<wnn::Operand>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, input_tensor, 4,
+                                           input_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int axes_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& axes_tensor = tensors[axes_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorType(logging_context, axes_tensor,
+                                          kTfLiteInt32, axes_tensor_id,
+                                          node_index));
+    TF_LITE_ENSURE_STATUS(CheckAxesTensorShape(
+        logging_context, axes_tensor, axes_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+        logging_context, axes_tensor, axes_tensor_id, node_index));
+
+    if (axes_tensor.dims->data[0] != 2) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unsupported MEAN reduction along %d axes in node %d",
+          axes_tensor.dims->data[0], node_index);
+      return kTfLiteError;
+    }
+
+    const int32_t* axes_data =
+        reinterpret_cast<const int32_t*>(axes_tensor.data.data);
+    if (std::min(axes_data[0], axes_data[1]) != 1 ||
+        std::max(axes_data[0], axes_data[1]) != 2) {
+      TF_LITE_MAYBE_KERNEL_LOG(logging_context,
+                               "unsupported MEAN reduction along non-spatial "
+                               "axes %d and %d in node %d",
+                               std::min(axes_data[0], axes_data[1]),
+                               std::max(axes_data[0], axes_data[1]),
+                               node_index);
+      return kTfLiteError;
+    }
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    const int expected_output_dims = reducer_params->keep_dims ? 4 : 2;
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor,
+                                           expected_output_dims,
+                                           output_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    if (builder) {
+      wnn::Operand output;
+      wnn::ReduceOptions reduceOptions;
+      reduceOptions.axes = axes_data;
+      reduceOptions.axesCount = axes_tensor.dims->data[0];
+      reduceOptions.keepDimensions = reducer_params->keep_dims;
+      TF_LITE_ENSURE(logging_context, webnn_operands[input_tensor_id]);
+      output = builder.ReduceMean(webnn_operands[input_tensor_id], &reduceOptions);
+      webnn_operands[output_tensor_id] = output;
+      TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitConcatenationNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteConcatenationParams* concat_params,
+      std::vector<wnn::Operand>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    size_t input_size = node->inputs->size;
+    const TfLiteTensor& first_input_tensor = tensors[node->inputs->data[0]];
+    uint32_t axis = concat_params->axis < 0
+                     ? first_input_tensor.dims->size + concat_params->axis
+                     : concat_params->axis;
+    if (builder) {
+      std::vector<wnn::Operand> input_operands;
+      for (size_t i = 0; i < input_size; ++i) {
+        TF_LITE_ENSURE(logging_context, webnn_operands[node->inputs->data[i]]);
+        input_operands.push_back(webnn_operands[node->inputs->data[i]]);
+      }
+      wnn::Operand output_operand = builder.Concat(input_operands.size(), input_operands.data(), axis);
+      webnn_operands[node->outputs->data[0]] = output_operand;
+      TF_LITE_ENSURE(logging_context, webnn_operands[node->outputs->data[0]]);
+    }
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitConv2DNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteConvParams* conv_params,
+      const std::unordered_set<int>& quasi_static_tensors,
+      std::vector<wnn::Operand>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckConvolutionParams(logging_context, conv_params, node_index));
+
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 3, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, input_tensor, 4,
+                                           input_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int filter_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& filter_tensor = tensors[filter_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, filter_tensor, filter_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, filter_tensor, 4,
+                                           filter_tensor_id));
+    if (quasi_static_tensors.count(filter_tensor_id) == 0) {
+      TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+          logging_context, filter_tensor, filter_tensor_id, node_index));
+    }
+
+    const int bias_tensor_id = node->inputs->data[2];
+    // bias_tensor_id < 0 means without bias.
+    if (bias_tensor_id >= 0) {
+      const TfLiteTensor& bias_tensor = tensors[bias_tensor_id];
+      TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt32Type(
+          logging_context, bias_tensor, node->inputs->data[2], node_index));
+      TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, bias_tensor, 1,
+                                            node->inputs->data[2]));
+      if (quasi_static_tensors.count(node->inputs->data[2]) == 0) {
+        TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+            logging_context, bias_tensor, node->inputs->data[2], node_index));
+      }
+    }
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor, 4,
+                                           output_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    wnn::AutoPad auto_pad;
+    TF_LITE_ENSURE_STATUS(CalculatePadding(
+        logging_context, conv_params->padding, auto_pad, node_index));
+
+    if (builder) {
+      wnn::Conv2dOptions options;
+      options.autoPad = auto_pad;
+      std::vector<int32_t> strides = {
+          conv_params->stride_height, conv_params->stride_width};
+      options.strides = strides.data();
+      options.stridesCount = strides.size();
+      std::vector<int32_t> dilations = {
+          conv_params->dilation_height_factor, conv_params->dilation_width_factor};
+      options.dilations = dilations.data();
+      options.dilationsCount = dilations.size();
+      options.inputLayout = wnn::InputOperandLayout::Nhwc;
+      options.filterLayout = wnn::Conv2dFilterOperandLayout::Ohwi;
+      TF_LITE_ENSURE(logging_context, webnn_operands[input_tensor_id]);
+      TF_LITE_ENSURE(logging_context, webnn_operands[filter_tensor_id]);
+      if (bias_tensor_id >= 0) {
+        TF_LITE_ENSURE(logging_context, webnn_operands[bias_tensor_id]);
+        options.bias = webnn_operands[bias_tensor_id];
+      }
+      wnn::FusionOperator activation_operator;
+      if (conv_params->activation != kTfLiteActNone) {
+        TF_LITE_ENSURE_STATUS(GetActivation(builder, logging_context, node_index,
+            conv_params->activation, activation_operator));
+        options.activation = activation_operator;
+      }
+      wnn::Operand output =
+          builder.Conv2d(webnn_operands[input_tensor_id],
+                         webnn_operands[filter_tensor_id],
+                         &options);
+      webnn_operands[output_tensor_id] = output;
+      TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitMediaPipeDeconvolutionNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteTransposeConvParams* deconv_params,
+      const std::unordered_set<int>& quasi_static_tensors,
+      std::vector<wnn::Operand>& webnn_operands) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 3, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, input_tensor, 4,
+                                           input_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int filter_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& filter_tensor = tensors[filter_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, filter_tensor, filter_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, filter_tensor, 4,
+                                           filter_tensor_id));
+    if (quasi_static_tensors.count(filter_tensor_id) == 0) {
+      TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+          logging_context, filter_tensor, filter_tensor_id, node_index));
+    }
+
+    const int bias_tensor_id = node->inputs->data[2];
+    const TfLiteTensor& bias_tensor = tensors[bias_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, bias_tensor, bias_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, bias_tensor, 1,
+                                           bias_tensor_id));
+    if (quasi_static_tensors.count(bias_tensor_id) == 0) {
+      TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+          logging_context, bias_tensor, bias_tensor_id, node_index));
+    }
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor, 4,
+                                           output_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    const int output_channels = filter_tensor.dims->data[0];
+    const int kernel_height = filter_tensor.dims->data[1];
+    const int kernel_width = filter_tensor.dims->data[2];
+    const int input_channels = filter_tensor.dims->data[3];
+
+    TF_LITE_ENSURE_STATUS(CheckMediaPipeTransposedConvolutionParams(
+        logging_context, deconv_params, node_index));
+
+    wnn::AutoPad auto_pad;
+    TF_LITE_ENSURE_STATUS(CalculatePadding(
+        logging_context, deconv_params->padding, auto_pad, node_index));
+
+    if (builder) {
+      wnn::ConvTranspose2dOptions options;
+      options.autoPad = auto_pad;
+      std::vector<int32_t> strides = {
+          deconv_params->stride_height, deconv_params->stride_width};
+      options.strides = strides.data();
+      options.stridesCount = strides.size();
+      options.inputLayout = wnn::InputOperandLayout::Nhwc;
+      options.filterLayout = wnn::ConvTranspose2dFilterOperandLayout::Ohwi;
+      TF_LITE_ENSURE(logging_context, webnn_operands[input_tensor_id]);
+      TF_LITE_ENSURE(logging_context, webnn_operands[filter_tensor_id]);
+      if (bias_tensor_id >= 0) {
+        TF_LITE_ENSURE(logging_context, webnn_operands[bias_tensor_id]);
+        options.bias = webnn_operands[bias_tensor_id];
+      }
+      wnn::Operand output =
+          builder.ConvTranspose2d(webnn_operands[input_tensor_id],
+                         webnn_operands[filter_tensor_id],
+                         &options);
+      webnn_operands[output_tensor_id] = output;
+      TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitDepthwiseConv2DNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteDepthwiseConvParams* dwconv_params,
+      const std::unordered_set<int>& quasi_static_tensors,
+      std::vector<wnn::Operand>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 3, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, input_tensor, 4,
+                                           input_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int filter_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& filter_tensor = tensors[filter_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, filter_tensor, filter_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, filter_tensor, 4,
+                                           filter_tensor_id));
+    if (quasi_static_tensors.count(filter_tensor_id) == 0) {
+      TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+          logging_context, filter_tensor, filter_tensor_id, node_index));
+    }
+
+    const int bias_tensor_id = node->inputs->data[2];
+    // bias_tensor_id < 0 means without bias.
+    if (bias_tensor_id >= 0) {
+      const TfLiteTensor& bias_tensor = tensors[bias_tensor_id];
+      TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt32Type(
+          logging_context, bias_tensor, node->inputs->data[2], node_index));
+      TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, bias_tensor, 1,
+                                            node->inputs->data[2]));
+      if (quasi_static_tensors.count(node->inputs->data[2]) == 0) {
+        TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+            logging_context, bias_tensor, node->inputs->data[2], node_index));
+      }
+    }
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor, 4,
+                                           output_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    const int output_channels = filter_tensor.dims->data[3];
+    TF_LITE_ENSURE_STATUS(CheckDepthwiseConvolutionParams(
+        logging_context, dwconv_params, output_channels, node_index));
+
+    wnn::AutoPad auto_pad;
+    TF_LITE_ENSURE_STATUS(CalculatePadding(
+        logging_context, dwconv_params->padding, auto_pad, node_index));
+
+    if (builder) {
+      wnn::Conv2dOptions options;
+      options.autoPad = auto_pad;
+      std::vector<int32_t> strides = {
+          dwconv_params->stride_height, dwconv_params->stride_width};
+      options.strides = strides.data();
+      options.stridesCount = strides.size();
+      std::vector<int32_t> dilations = {
+          dwconv_params->dilation_height_factor, dwconv_params->dilation_width_factor};
+      options.dilations = dilations.data();
+      options.dilationsCount = dilations.size();
+      options.inputLayout = wnn::InputOperandLayout::Nhwc;
+      options.filterLayout = wnn::Conv2dFilterOperandLayout::Ihwo;
+      options.groups = output_channels / dwconv_params->depth_multiplier;
+      TF_LITE_ENSURE(logging_context, webnn_operands[input_tensor_id]);
+      TF_LITE_ENSURE(logging_context, webnn_operands[filter_tensor_id]);
+
+      if (bias_tensor_id >= 0) {
+        TF_LITE_ENSURE(logging_context, webnn_operands[bias_tensor_id]);
+        options.bias = webnn_operands[bias_tensor_id];
+      }
+      wnn::FusionOperator activation_operator;
+      if (dwconv_params->activation != kTfLiteActNone) {
+        TF_LITE_ENSURE_STATUS(GetActivation(builder, logging_context, node_index,
+            dwconv_params->activation, activation_operator));
+        options.activation = activation_operator;
+      }
+      wnn::Operand output =
+          builder.Conv2d(webnn_operands[input_tensor_id],
+                         webnn_operands[filter_tensor_id],
+                         &options);
+      webnn_operands[output_tensor_id] = output;
+      TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitFullyConnectedNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteFullyConnectedParams* fc_params,
+      const std::unordered_set<int>& quasi_static_tensors,
+      std::vector<wnn::Operand>& webnn_operands,
+      std::vector<std::unique_ptr<char>>& constant_buffers) {
+    TF_LITE_ENSURE_STATUS(
+        CheckFullyConnectedParams(logging_context, fc_params, node_index));
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, 3, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int filter_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& filter_tensor = tensors[filter_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, filter_tensor, filter_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, filter_tensor, 2,
+                                           filter_tensor_id));
+    if (quasi_static_tensors.count(filter_tensor_id) == 0) {
+      TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+          logging_context, filter_tensor, filter_tensor_id, node_index));
+    }
+
+    int bias_tensor_id = -1;
+    if (node->inputs->size >= 3) {
+      bias_tensor_id = node->inputs->data[2];
+      if (bias_tensor_id >= 0) {
+        const TfLiteTensor& bias_tensor = tensors[bias_tensor_id];
+        TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt32Type(
+            logging_context, bias_tensor, bias_tensor_id, node_index));
+        TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, bias_tensor, 1,
+                                               bias_tensor_id));
+        if (quasi_static_tensors.count(bias_tensor_id) == 0) {
+          TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+              logging_context, bias_tensor, bias_tensor_id, node_index));
+        }
+      }
+    }
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    const int32_t output_channels = filter_tensor.dims->data[0];
+    const int32_t input_channels = filter_tensor.dims->data[1];
+
+    if (input_tensor.dims->size == 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unexpected number of shape dimensions %d in tensor #%d",
+          input_tensor.dims->size, input_tensor_id);
+      return kTfLiteError;
+    }
+    const int32_t * input_dims_data = input_tensor.dims->data;
+    int32_t num_input_elements = 1;
+    for (int i = 0; i < input_tensor.dims->size; i++) {
+      if (input_dims_data[i] <= 0) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            logging_context, "invalid dimension #%d (%d) in tensor #%d", i,
+            input_dims_data[i], input_tensor_id);
+        return kTfLiteError;
+      }
+      num_input_elements *= input_dims_data[i];
+    }
+
+    if (fc_params->keep_num_dims) {
+      TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor,
+                                             input_tensor.dims->size,
+                                             output_tensor_id));
+
+      for (int i = 0; i < input_tensor.dims->size - 1; i++) {
+        if (input_dims_data[i] != output_tensor.dims->data[i]) {
+          TF_LITE_MAYBE_KERNEL_LOG(
+              logging_context,
+              "mismatch in shape dimension %d (%d != %d) in input and output "
+              "tensors of FULLY_CONNECTED operator #%d",
+              i, input_dims_data[i], output_tensor.dims->data[i],
+              node_index);
+          return kTfLiteError;
+        }
+      }
+    } else {
+      if (num_input_elements % input_channels != 0) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            logging_context,
+            "number of elements in input tensor #%d in FULLY_CONNECTED "
+            "operator is not divisible by input channels (%d)",
+            input_tensor_id, input_channels);
+        return kTfLiteError;
+      }
+
+      TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor, 2,
+                                             output_tensor_id));
+
+      if (output_tensor.dims->data[0] != num_input_elements / input_channels) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            logging_context,
+            "batch size %d in output tensor #%d in FULLY_CONNECTED operator "
+            "does not match batch size %d in reshaped input tensor #%d",
+            output_tensor.dims->data[0], output_tensor_id,
+            num_input_elements / input_channels, input_tensor_id);
+        return kTfLiteError;
+      }
+    }
+
+    if (output_tensor.dims->data[output_tensor.dims->size - 1] !=
+        output_channels) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "number of channels %d in output tensor #%d does not match output "
+          "channels %d in filter tensor #%d",
+          output_tensor.dims->data[output_tensor.dims->size - 1],
+          output_tensor_id, output_channels, filter_tensor_id);
+      return kTfLiteError;
+    }
+
+    if (builder) {
+      wnn::GemmOptions options;
+      options.aTranspose = false;
+      options.bTranspose = true;
+      if (bias_tensor_id >= 0) {
+        TF_LITE_ENSURE(logging_context, webnn_operands[bias_tensor_id]);
+        options.c = webnn_operands[bias_tensor_id];
+      }
+      TF_LITE_ENSURE(logging_context, webnn_operands[input_tensor_id]);
+      TF_LITE_ENSURE(logging_context, webnn_operands[filter_tensor_id]);
+      wnn::Operand output;
+      if (fc_params->keep_num_dims || input_tensor.dims->size > 2) {
+        // Reshape input to 2D tensor
+        const int32_t n_inputs =
+            static_cast<const int32_t>(input_dims_data[input_tensor.dims->size -1]);
+        std::vector<int32_t> new_input_shape = {-1, n_inputs};
+        wnn::Operand reshaped_input =
+            builder.Reshape(webnn_operands[input_tensor_id],
+                            new_input_shape.data(), new_input_shape.size());
+        wnn::Operand gemm = builder.Gemm(reshaped_input,
+                                        webnn_operands[filter_tensor_id],
+                                        &options);
+        output = builder.Reshape(gemm, &output_tensor.dims->data[0],
+                                 output_tensor.dims->size);
+      } else {
+        output = builder.Gemm(webnn_operands[input_tensor_id],
+                              webnn_operands[filter_tensor_id], &options);
+      }
+
+      webnn_operands[output_tensor_id] = output;
+      TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+    }
+
+    TF_LITE_ENSURE_STATUS(VisitActivation(
+        builder, logging_context, node_index, output_tensor_id, output_tensor_id,
+        fc_params->activation, webnn_operands, constant_buffers));
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitHardSwishNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      std::vector<wnn::Operand>& webnn_operands) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, 1, node_index));
+
+    const TfLiteTensor& input_tensor = tensors[node->inputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+
+    const TfLiteTensor& output_tensor = tensors[node->outputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+
+    if (builder) {
+      TF_LITE_ENSURE(logging_context, webnn_operands[node->inputs->data[0]]);
+      webnn_operands[node->outputs->data[0]] = builder.HardSwish(webnn_operands[node->inputs->data[0]]);
+      TF_LITE_ENSURE(logging_context, webnn_operands[node->outputs->data[0]]);
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitLogisticNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      std::vector<wnn::Operand>& webnn_operands) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, 1, node_index));
+
+    const TfLiteTensor& input_tensor = tensors[node->inputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+
+    const TfLiteTensor& output_tensor = tensors[node->outputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+
+    if (builder) {
+      TF_LITE_ENSURE(logging_context, webnn_operands[node->inputs->data[0]]);
+      webnn_operands[node->outputs->data[0]] = builder.Sigmoid(webnn_operands[node->inputs->data[0]]);
+      TF_LITE_ENSURE(logging_context, webnn_operands[node->outputs->data[0]]);
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitReluNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      std::vector<wnn::Operand>& webnn_operands) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, 1, node_index));
+
+    const TfLiteTensor& input_tensor = tensors[node->inputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+
+    const TfLiteTensor& output_tensor = tensors[node->outputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+
+    if (builder) {
+      TF_LITE_ENSURE(logging_context, webnn_operands[node->inputs->data[0]]);
+      webnn_operands[node->outputs->data[0]] = builder.Relu(webnn_operands[node->inputs->data[0]]);
+      TF_LITE_ENSURE(logging_context, webnn_operands[node->outputs->data[0]]);
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitReshapeNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteReshapeParams* reshape_params,
+      std::vector<wnn::Operand>& webnn_operands) {
+    switch (node->inputs->size) {
+      case 1:
+      case 2:
+        break;
+      default:
+        TF_LITE_MAYBE_KERNEL_LOG(
+            logging_context,
+            "unexpected number of inputs (%d) in node #%d: "
+            "either one or two inputs expected",
+            node->inputs->size, node_index);
+        return kTfLiteError;
+    }
+    if (node->outputs->size != 1) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unexpected number of outputs (%d) in node #%d: one output expected",
+          node->outputs->size, node_index);
+      return kTfLiteError;
+    }
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    if (node->inputs->size == 2) {
+      const int shape_tensor_id = node->inputs->data[1];
+      const TfLiteTensor& shape_tensor = tensors[shape_tensor_id];
+      TF_LITE_ENSURE_STATUS(CheckTensorType(logging_context, shape_tensor,
+                                            kTfLiteInt32, shape_tensor_id,
+                                            node_index));
+      TF_LITE_ENSURE_STATUS(CheckShapeTensorShape(
+          logging_context, shape_tensor, shape_tensor_id, node_index));
+      TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+          logging_context, shape_tensor,shape_tensor_id, node_index));
+    }
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    if (builder) {
+      TF_LITE_ENSURE(logging_context, webnn_operands[input_tensor_id]);
+      webnn_operands[output_tensor_id] = builder.Reshape(
+          webnn_operands[input_tensor_id], &output_tensor.dims->data[0], output_tensor.dims->size);
+      TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitResizeBilinearNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteResizeBilinearParams* resize_params,
+      std::vector<wnn::Operand>& webnn_operands) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, input_tensor, 4,
+                                           input_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int shape_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& shape_tensor = tensors[shape_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorType(logging_context, shape_tensor,
+                                          kTfLiteInt32, shape_tensor_id,
+                                          node_index));
+    TF_LITE_ENSURE_STATUS(CheckShapeTensorShape(
+        logging_context, shape_tensor, shape_tensor_id, node_index));
+    if (shape_tensor.dims->data[0] != 2) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unexpected number of dimensions %d in the output shape in node %d",
+          shape_tensor.dims->data[0], node_index);
+    }
+    TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+        logging_context, shape_tensor, shape_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, output_tensor, 4,
+                                           output_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    const int32_t* shape_data =
+        reinterpret_cast<const int32_t*>(shape_tensor.data.data);
+    for (int i = 0; i < shape_tensor.dims->data[0]; i++) {
+      const int32_t dim = shape_data[i];
+      if (dim <= 0) {
+        TF_LITE_MAYBE_KERNEL_LOG(
+            logging_context, "invalid output dimension #%d value %d in node %d",
+            i, dim, node_index);
+        return kTfLiteError;
+      }
+    }
+
+    if (builder) {
+      wnn::Operand input_operand = webnn_operands[input_tensor_id];
+      TF_LITE_ENSURE(logging_context, input_operand);
+      std::vector<int32_t> sizes = {shape_data[0], shape_data[1]};
+      std::vector<int32_t> axes = {1, 2};
+      wnn::Resample2dOptions options;
+      options.mode = wnn::InterpolationMode::Linear;
+      options.sizes = sizes.data();
+      options.sizesCount = sizes.size();
+      options.axes = axes.data();
+      options.axesCount = axes.size();
+      webnn_operands[output_tensor_id] = builder.Resample2d(input_operand, &options);
+      TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitSoftmaxNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteSoftmaxParams* params,
+      std::vector<wnn::Operand>& webnn_operands) {
+    if (params->beta != 1.0f) {
+      if (logging_context != nullptr) {
+        TF_LITE_KERNEL_LOG(logging_context,
+                           "unsupported beta value %.7f in SOFTMAX node #%d",
+                           params->beta, node_index);
+      }
+      return kTfLiteError;
+    }
+
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, 1, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int output_tensor_id = node->outputs->data[0];
+    const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, output_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, output_tensor_id, node_index));
+
+    if (builder) {
+      TF_LITE_ENSURE(logging_context, webnn_operands[input_tensor_id]);
+      webnn_operands[output_tensor_id] = builder.Softmax(webnn_operands[input_tensor_id]);
+      TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitSplitNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteSplitParams* params,
+      std::vector<wnn::Operand>& webnn_operands) {
+    const int num_splits = params->num_splits;
+    if (num_splits == 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unexpected value of num_splits %d in the split params in node %d",
+          num_splits, node_index);
+      return kTfLiteError;
+    }
+
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 2, num_splits, node_index));
+
+    const int input_tensor_id = node->inputs->data[1];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int axis_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& axis_tensor = tensors[axis_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorType(logging_context, axis_tensor,
+                                          kTfLiteInt32, axis_tensor_id,
+                                          node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorShape(logging_context, axis_tensor, 0,
+                                           axis_tensor_id));
+    TF_LITE_ENSURE_STATUS(CheckTensorStaticAllocation(
+        logging_context, axis_tensor, axis_tensor_id, node_index));
+
+    const int* axis_data =
+        reinterpret_cast<const int*>(axis_tensor.data.data);
+    int axis_value = axis_data[0];
+
+    const int num_dims = input_tensor.dims->size;
+    if (axis_value < 0) {
+      axis_value += num_dims;
+    }
+    if (axis_value < 0 || axis_value > num_dims) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unexpected data of axis %d in the axis tensor in node %d",
+          axis_data[0], node_index);
+      return kTfLiteError;
+    }
+    const int input_size = input_tensor.dims->data[axis_value];
+    if (input_size % num_splits != 0) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "Not an even split");
+      return kTfLiteError;
+    }
+
+    const int output_size = node->outputs->size;
+
+    if (builder) {
+      std::vector<uint32_t> splits = {static_cast<const uint32_t>(num_splits)};
+      wnn::SplitOptions options;
+      options.axis = static_cast<const uint32_t>(axis_value);
+      TF_LITE_ENSURE(logging_context, webnn_operands[input_tensor_id]);
+      wnn::OperandArray split_operand_array = builder.Split(
+          webnn_operands[input_tensor_id], splits.data(), splits.size(), &options);
+      TF_LITE_ENSURE(logging_context, split_operand_array.Size() == output_size);
+
+      for (int i = 0; i < output_size; i++) {
+        int output_tensor_id = node->outputs->data[i];
+        const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+        TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+            logging_context, output_tensor, output_tensor_id, node_index));
+        TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+            logging_context, output_tensor, output_tensor_id, node_index));
+        webnn_operands[output_tensor_id] = split_operand_array.Get(i);
+        TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+      }
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitTanhNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      std::vector<wnn::Operand>& webnn_operands) {
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, 1, node_index));
+
+    const TfLiteTensor& input_tensor = tensors[node->inputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, node->inputs->data[0], node_index));
+
+    const TfLiteTensor& output_tensor = tensors[node->outputs->data[0]];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, output_tensor, node->outputs->data[0], node_index));
+
+    if (builder) {
+      TF_LITE_ENSURE(logging_context, webnn_operands[node->inputs->data[0]]);
+      webnn_operands[node->outputs->data[0]] = builder.Tanh(webnn_operands[node->inputs->data[0]]);
+      TF_LITE_ENSURE(logging_context, webnn_operands[node->outputs->data[0]]);
+    }
+
+    return kTfLiteOk;
+  }
+
+  static TfLiteStatus VisitUnpackNode(
+      const wnn::GraphBuilder& builder, TfLiteContext* logging_context, int node_index,
+      TfLiteNode* node, const TfLiteTensor* tensors,
+      const TfLiteUnpackParams* params,
+      std::vector<wnn::Operand>& webnn_operands) {
+    const int num = params->num;
+    int axis = params->axis;
+
+    TF_LITE_ENSURE_STATUS(
+        CheckNumInputsAndOutputs(logging_context, node, 1, num, node_index));
+
+    const int input_tensor_id = node->inputs->data[0];
+    const TfLiteTensor& input_tensor = tensors[input_tensor_id];
+    TF_LITE_ENSURE_STATUS(CheckTensorFloat32OrQInt8Type(
+        logging_context, input_tensor, input_tensor_id, node_index));
+    TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+        logging_context, input_tensor, input_tensor_id, node_index));
+
+    const int num_dims = input_tensor.dims->size;
+    if (axis < 0) {
+      axis += num_dims;
+    }
+    if (axis < 0 || axis >= num_dims) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unexpected value of axis %d in the unpack params in node %d",
+          axis, node_index);
+      return kTfLiteError;
+    }
+
+    const int output_size = node->outputs->size;
+
+    if (num != output_size) {
+      TF_LITE_MAYBE_KERNEL_LOG(
+          logging_context,
+          "unexpected value of num %d in the unpack params in node %d",
+          num, node_index);
+      return kTfLiteError;
+    }
+
+    if (builder) {
+      TF_LITE_ENSURE(logging_context, webnn_operands[input_tensor_id]);
+      wnn::SqueezeOptions squeeze_options;
+      std::vector<int32_t> axes = {static_cast<int32_t>(axis)};
+      squeeze_options.axes = axes.data();
+      squeeze_options.axesCount = axes.size();
+      // Unpack = split + squeeze in WebNN
+      // No need split if Unpack's num == 1
+      if (num == 1) {
+        int output_tensor_id = node->outputs->data[0];
+        const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+        TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+            logging_context, output_tensor, output_tensor_id, node_index));
+        TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+            logging_context, output_tensor, output_tensor_id, node_index));
+
+        webnn_operands[output_tensor_id] =
+            builder.Squeeze(webnn_operands[input_tensor_id], &squeeze_options);
+        TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+      } else {
+        std::vector<uint32_t> splits = {static_cast<const uint32_t>(num)};
+        wnn::SplitOptions options;
+        options.axis = static_cast<const uint32_t>(axis);
+        wnn::OperandArray split_operand_array = builder.Split(
+            webnn_operands[input_tensor_id], splits.data(), splits.size(), &options);
+        TF_LITE_ENSURE(logging_context, split_operand_array.Size() == output_size);
+
+        for (int i = 0; i < output_size; i++) {
+          int output_tensor_id = node->outputs->data[i];
+          const TfLiteTensor& output_tensor = tensors[output_tensor_id];
+          TF_LITE_ENSURE_STATUS(CheckTensorFloat32Type(
+              logging_context, output_tensor, output_tensor_id, node_index));
+          TF_LITE_ENSURE_STATUS(CheckTensorNonDynamicAllocation(
+              logging_context, output_tensor, output_tensor_id, node_index));
+
+          webnn_operands[output_tensor_id] =
+              builder.Squeeze(split_operand_array.Get(i), &squeeze_options);
+          TF_LITE_ENSURE(logging_context, webnn_operands[output_tensor_id]);
+        }
+      }
+    }
+    return kTfLiteOk;
+  }
+
+ private:
+  Subgraph(wnn::Graph graph, std::unordered_set<int>&& inputs, std::unordered_set<int>&& outputs)
+      : wnn_graph_(graph), inputs_(inputs), outputs_(outputs) {
+    for (auto& i : inputs_) {
+      wnn_inputs_[i] = {};
+      externals_[i] = nullptr;
+    }
+    for (auto& o : outputs_) {
+      wnn_outputs_[o] = {};
+      externals_[o] = nullptr;
+    }
+    graph_inputs_ = wnn::CreateNamedInputs();
+    graph_outputs_ = wnn::CreateNamedOutputs();
+  }
+
+  wnn::Graph wnn_graph_;
+  // TFLite Tensor IDs == name of input/output tensors for the
+  // delegated subgraph.
+  std::unordered_set<int> inputs_;
+  std::unordered_set<int> outputs_;
+  std::unordered_map<int, wnn::Input> wnn_inputs_;
+  std::unordered_map<int, wnn::Resource> wnn_outputs_;
+  wnn::NamedInputs graph_inputs_;
+  wnn::NamedOutputs graph_outputs_;
+  std::unordered_map<int, void*> externals_;
+  char dummy_data_{0};
+};
+
+TfLiteIntArray* Delegate::PrepareOpsToDelegate(TfLiteContext* context) {
+  // Clear previous data, in case the delegate is reused without re-creation.
+  static_unpacked_data_map_.clear();
+  static_unpacked_data_.clear();
+  static_unpack_nodes_.clear();
+  static_sparse_weights_.clear();
+
+  TfLiteIntArray* execution_plan = nullptr;
+  if (context->GetExecutionPlan(context, &execution_plan) != kTfLiteOk) {
+    TF_LITE_KERNEL_LOG(context, "Unable to get graph execution plan.");
+    return nullptr;
+  }
+
+  // Mapping for quasi-static (unpacked from static) tensor index to the node
+  // index that produced it.
+  std::unordered_map<int, int> quasi_static_tensors_producers;
+  // Set of all quasi-static tensors in the execution plan.
+  std::unordered_set<int> quasi_static_tensors;
+  // Set of quasi-static tensors consumed by the delegated nodes.
+  std::unordered_set<int> quasi_static_tensors_to_unpack;
+
+  TfLiteIntArray* nodes_to_delegate =
+      TfLiteIntArrayCreate(execution_plan->size);
+  nodes_to_delegate->size = 0;
+  for (int i = 0; i < execution_plan->size; ++i) {
+    const int node_index = execution_plan->data[i];
+
+    // Check if TFLite nodes can be delegated to WebNN
+    TfLiteNode* node = nullptr;
+    TfLiteRegistration* registration = nullptr;
+    if (context->GetNodeAndRegistration(context, node_index, &node,
+                                        &registration) != kTfLiteOk) {
+      TF_LITE_KERNEL_LOG(context,
+                         "Unable to get node and registration for node %d.",
+                         node_index);
+      continue;  // Soft error (skip this node).
+    }
+
+    // Prepare to unpack FP16 tensors.
+    if (registration->builtin_code == kTfLiteBuiltinDequantize &&
+        node->inputs->size == 1 && node->outputs->size == 1) {
+      const TfLiteTensor& input_tensor =
+          context->tensors[node->inputs->data[0]];
+      const TfLiteTensor& output_tensor =
+          context->tensors[node->outputs->data[0]];
+      if ((input_tensor.allocation_type == kTfLiteMmapRo ||
+           quasi_static_tensors.count(node->inputs->data[0]) != 0) &&
+          input_tensor.type == kTfLiteFloat16 &&
+          output_tensor.type == kTfLiteFloat32) {
+        static_unpack_nodes_.insert(node_index);
+        quasi_static_tensors_producers[node->outputs->data[0]] = node_index;
+        quasi_static_tensors.insert(node->outputs->data[0]);
+
+        if (input_tensor.allocation_type != kTfLiteMmapRo) {
+          quasi_static_tensors_to_unpack.insert(node->inputs->data[0]);
+        }
+
+        // If dequantized input is sparse, so is its output
+        if (static_sparse_weights_.count(node->inputs->data[0]) != 0) {
+          static_sparse_weights_.insert(node->outputs->data[0]);
+        }
+
+        // Skip this node for now. If output of the node is consumed only by
+        // delegated nodes, it will be added to nodes_to_delegate in the end.
+        continue;
+      }
+    }
+
+    // Prepare to unpack sparse tensors.
+    // TODO(b/157729695): In the future, we also need to handle the case where a
+    // sparse tensor is fed to a TFLite op directly, and no Densify() op is
+    // inserted. For now this is not a problem because the Conv() op in tflite
+    // can only consume dense tensors.
+    if (registration->builtin_code == kTfLiteBuiltinDensify &&
+        node->inputs->size == 1 && node->outputs->size == 1) {
+      const TfLiteTensor& input_tensor =
+          context->tensors[node->inputs->data[0]];
+      const TfLiteTensor& output_tensor =
+          context->tensors[node->outputs->data[0]];
+      if (input_tensor.allocation_type == kTfLiteMmapRo &&
+          input_tensor.sparsity != nullptr &&
+          (input_tensor.type == kTfLiteFloat16 ||
+           input_tensor.type == kTfLiteFloat32) &&
+          output_tensor.type == input_tensor.type) {
+        static_unpack_nodes_.insert(node_index);
+        quasi_static_tensors_producers[node->outputs->data[0]] = node_index;
+        quasi_static_tensors.insert(node->outputs->data[0]);
+        static_sparse_weights_.insert(node->outputs->data[0]);
+
+        // Skip this node for now. If output of the node is consumed only by
+        // delegated nodes, it will be added to nodes_to_delegate in the end.
+        continue;
+      }
+    }
+
+    wnn::GraphBuilder null_builder;
+    std::vector<wnn::Operand> empty_webnn_operands;
+    std::vector<std::unique_ptr<char>> empty_buffers;
+    if (Subgraph::VisitNode(null_builder, context, registration, node,
+                            node_index, quasi_static_tensors,
+                            empty_webnn_operands, empty_buffers) != kTfLiteOk) {
+      // If a non-delegated node consumes output of a node that unpacks static
+      // data, that node shouldn't be delegated.
+      for (int j = 0; j < node->inputs->size; j++) {
+        const auto it =
+            quasi_static_tensors_producers.find(node->inputs->data[j]);
+        if (it != quasi_static_tensors_producers.end()) {
+          static_unpack_nodes_.erase(it->second);
+        }
+      }
+
+      // Non-delegatable node is not an error.
+      continue;
+    }
+
+    for (int j = 0; j < node->inputs->size; j++) {
+      if (quasi_static_tensors.count(node->inputs->data[j]) != 0) {
+        quasi_static_tensors_to_unpack.insert(node->inputs->data[j]);
+      }
+    }
+
+    nodes_to_delegate->data[nodes_to_delegate->size++] = node_index;
+  }
+
+  // Sort quasi-static tensors to be unpacked by the node index the produced
+  // them. This ensures that in situations where quasi-static tensor is
+  // produced from another quasi-static tensor, the tensors are unpacked in
+  // the original execution plan order.
+  std::vector<int> sorted_quasi_static_tensors_to_unpack(
+      quasi_static_tensors_to_unpack.cbegin(),
+      quasi_static_tensors_to_unpack.cend());
+  std::sort(sorted_quasi_static_tensors_to_unpack.begin(),
+            sorted_quasi_static_tensors_to_unpack.end(),
+            [&quasi_static_tensors_producers](int t1, int t2) {
+              return quasi_static_tensors_producers[t1] <
+                     quasi_static_tensors_producers[t2];
+            });
+
+  // Unpack static data of all tensors
+  for (int t : sorted_quasi_static_tensors_to_unpack) {
+    const int producer_index = quasi_static_tensors_producers[t];
+    // Check if TFLite nodes can be delegated to WebNN
+    TfLiteNode* node = nullptr;
+    TfLiteRegistration* registration = nullptr;
+    if (context->GetNodeAndRegistration(context, producer_index, &node,
+                                        &registration) != kTfLiteOk) {
+      TF_LITE_KERNEL_LOG(context,
+                         "Unable to get node and registration for node %d.",
+                         producer_index);
+      TfLiteIntArrayFree(nodes_to_delegate);
+      return nullptr;  // Hard error.
+    }
+
+    if (node->inputs->size != 1) {
+      TF_LITE_KERNEL_LOG(context, "unexpected number of inputs (%d) in node %d",
+                         node->inputs->size, producer_index);
+      TfLiteIntArrayFree(nodes_to_delegate);
+      return nullptr;  // Hard error.
+    }
+
+    if (node->outputs->size != 1) {
+      TF_LITE_KERNEL_LOG(context,
+                         "unexpected number of outputs (%d) in node %d",
+                         node->outputs->size, producer_index);
+      TfLiteIntArrayFree(nodes_to_delegate);
+      return nullptr;  // Hard error.
+    }
+
+    const TfLiteTensor& input_tensor = context->tensors[node->inputs->data[0]];
+
+    // Consider the case when the input to unpacking node is quasi-static.
+    const auto static_unpacked_input_it_ =
+        static_unpacked_data_map_.find(node->inputs->data[0]);
+    if (static_unpacked_input_it_ == static_unpacked_data_map_.end()) {
+      if (input_tensor.allocation_type != kTfLiteMmapRo) {
+        TF_LITE_KERNEL_LOG(
+            context,
+            "unexpected allocation type (%d) in tensor %d in node %d (%d)",
+            input_tensor.allocation_type, node->inputs->data[0], producer_index,
+            registration->builtin_code);
+        TfLiteIntArrayFree(nodes_to_delegate);
+        return nullptr;  // Hard error.
+      }
+    }
+
+    const TfLiteTensor& output_tensor = context->tensors[t];
+    size_t tensor_elements = output_tensor.bytes;
+    switch (output_tensor.type) {
+      case kTfLiteFloat32:
+        tensor_elements /= sizeof(float);
+        break;
+      case kTfLiteFloat16:
+        tensor_elements /= sizeof(uint16_t);
+        break;
+      default: {
+        TF_LITE_KERNEL_LOG(context,
+                           "unexpected datatype (%s) in tensor %d in node %d",
+                           TfLiteTypeGetName(output_tensor.type),
+                           node->outputs->data[0], producer_index);
+        TfLiteIntArrayFree(nodes_to_delegate);
+        return nullptr;  // Hard error.
+      }
+    }
+
+    const size_t tensor_offset = static_unpacked_data_.size();
+    static_unpacked_data_.resize(tensor_offset + context->tensors[t].bytes);
+
+    char* unpacked_data = static_unpacked_data_.data() + tensor_offset;
+    const char* packed_data =
+        static_unpacked_input_it_ != static_unpacked_data_map_.end()
+            ? static_unpacked_data_.data() + static_unpacked_input_it_->second
+            : static_cast<const char*>(input_tensor.data.data);
+    switch (registration->builtin_code) {
+      case kTfLiteBuiltinDequantize: {
+        if (input_tensor.type != kTfLiteFloat16) {
+          TF_LITE_KERNEL_LOG(
+              context, "unexpected tensor %d data type (%s) in node %d",
+              node->inputs->data[0], TfLiteTypeGetName(input_tensor.type),
+              producer_index);
+          TfLiteIntArrayFree(nodes_to_delegate);
+          return nullptr;  // Hard error.
+        }
+
+        if (input_tensor.sparsity != nullptr) {
+          TF_LITE_KERNEL_LOG(context,
+                             "unexpected FP16 sparse tensor %d in node %d",
+                             node->inputs->data[0], producer_index);
+          TfLiteIntArrayFree(nodes_to_delegate);
+          return nullptr;  // Hard error.
+        }
+
+        // Actual data unpacking
+        float* unpacked_fp32_data = reinterpret_cast<float*>(unpacked_data);
+        const uint16_t* packed_fp16_data =
+            reinterpret_cast<const uint16_t*>(packed_data);
+        for (size_t i = 0; i < tensor_elements; i++) {
+          unpacked_fp32_data[i] = fp16_ieee_to_fp32_value(packed_fp16_data[i]);
+        }
+        break;
+      }
+      case kTfLiteBuiltinDensify: {
+        if (input_tensor.sparsity == nullptr) {
+          TF_LITE_KERNEL_LOG(context, "unexpected dense tensor %d in node %d",
+                             node->inputs->data[0], producer_index);
+          TfLiteIntArrayFree(nodes_to_delegate);
+          return nullptr;  // Hard error.
+        }
+
+        const int dims_count = output_tensor.dims->size;
+        std::vector<int> vector_shape(dims_count);
+        for (int i = 0; i < dims_count; i++) {
+          vector_shape[i] = output_tensor.dims->data[i];
+        }
+
+        switch (input_tensor.type) {
+          case kTfLiteFloat32: {
+            const size_t dense_size = context->tensors[t].bytes / sizeof(float);
+            float* unpacked_fp32_data = reinterpret_cast<float*>(unpacked_data);
+            tflite::internal::sparsity::FormatConverter<float> converter(
+                vector_shape, *input_tensor.sparsity);
+            converter.SparseToDense(
+                static_cast<const float*>(input_tensor.data.data), dense_size,
+                unpacked_fp32_data, context);
+            break;
+          }
+          case kTfLiteFloat16: {
+            const size_t dense_size =
+                context->tensors[t].bytes / sizeof(Eigen::half);
+            Eigen::half* unpacked_fp16_data =
+                reinterpret_cast<Eigen::half*>(unpacked_data);
+            tflite::internal::sparsity::FormatConverter<Eigen::half> converter(
+                vector_shape, *input_tensor.sparsity);
+            converter.SparseToDense(
+                static_cast<const Eigen::half*>(input_tensor.data.data),
+                dense_size, unpacked_fp16_data, context);
+            break;
+          }
+          default: {
+            TF_LITE_KERNEL_LOG(
+                context, "unexpected tensor %d data type (%s) in node %d",
+                node->inputs->data[0], TfLiteTypeGetName(input_tensor.type),
+                producer_index);
+            TfLiteIntArrayFree(nodes_to_delegate);
+            return nullptr;  // Hard error.
+          }
+        }
+        break;
+      }
+      default:
+        TF_LITE_KERNEL_LOG(context, "unexpected op registration %d at node %d",
+                           registration->builtin_code, producer_index);
+        TfLiteIntArrayFree(nodes_to_delegate);
+        return nullptr;  // Hard error.
+    }
+
+    static_unpacked_data_map_[t] = tensor_offset;
+  }
+
+  // Add nodes that unpack static data consumed by delegated nodes.
+  // Note: this is done purely to avoid the overhead of running these nodes
+  // again in TFLite interpreter which would allocate memory for their outputs.
+  // We mark them as delegated, but the delegate would simply ignore these nodes
+  // as the static weights are already unpacked.
+  for (int node_index : static_unpack_nodes_) {
+    nodes_to_delegate->data[nodes_to_delegate->size++] = node_index;
+  }
+  std::sort(&nodes_to_delegate->data[0],
+            &nodes_to_delegate->data[nodes_to_delegate->size]);
+
+#ifdef WEBNN_DELEGATE_TEST_MODE
+  // In the test mode build (used by unit tests), WebNN delegate claims to
+  // support all operators in the execution plan to disable fallback to the
+  // default TensorFlow Lite kernels. Thus, if any of the ops in the model are
+  // not supported by the delegate, they will cause a failure in
+  // ::tflite::Interpreter::ModifyGraphWithDelegate, to be caught in the unit
+  // tests.
+  nodes_to_delegate->size = execution_plan->size;
+  std::copy(&execution_plan->data[0],
+            &execution_plan->data[execution_plan->size],
+            &nodes_to_delegate->data[0]);
+#endif
+
+  return nodes_to_delegate;
+}
+
+void* SubgraphInit(TfLiteContext* context, const char* buffer, size_t length) {
+  const TfLiteDelegateParams* params =
+      reinterpret_cast<const TfLiteDelegateParams*>(buffer);
+
+  return static_cast<void*>(Subgraph::Create(
+      context, params,
+      static_cast<::tflite::webnn::Delegate*>(params->delegate->data_)));
+}
+
+TfLiteStatus SubgraphPrepare(TfLiteContext* context, TfLiteNode* node) {
+  if (node->user_data == nullptr) {
+    return kTfLiteError;
+  }
+
+  return static_cast<Subgraph*>(node->user_data)->Prepare(context);
+}
+
+TfLiteStatus SubgraphInvoke(TfLiteContext* context, TfLiteNode* node) {
+  if (node->user_data == nullptr) {
+    return kTfLiteError;
+  }
+
+  return static_cast<Subgraph*>(node->user_data)->Invoke(context);
+}
+
+void SubgraphFree(TfLiteContext* context, void* buffer) {
+  if (buffer != nullptr) {
+    delete static_cast<Subgraph*>(buffer);
+  }
+}
+
+const TfLiteRegistration kSubgraphRegistration = {
+    /*.init=*/SubgraphInit,
+    /*.free=*/SubgraphFree,
+    /*.prepare=*/SubgraphPrepare,
+    /*.invoke=*/SubgraphInvoke,
+    /*.profiling_string=*/nullptr,
+    /*.builtin_code=*/0,
+    /*.custom_name=*/"TfLiteWebNNDelegate",
+    /*.version=*/2,
+};
+
+TfLiteStatus DelegatePrepare(TfLiteContext* context, TfLiteDelegate* delegate) {
+  TfLiteIntArray* ops_to_replace =
+      static_cast<::tflite::webnn::Delegate*>(delegate->data_)
+          ->PrepareOpsToDelegate(context);
+  if (ops_to_replace == nullptr) {
+    return kTfLiteError;
+  }
+
+  const TfLiteStatus status = context->ReplaceNodeSubsetsWithDelegateKernels(
+      context, kSubgraphRegistration, ops_to_replace, delegate);
+  TfLiteIntArrayFree(ops_to_replace);
+  return status;
+}
+
+}  // namespace
+}  // namespace webnn
+}  // namespace tflite
+
+TfLiteWebNNDelegateOptions TfLiteWebNNDelegateOptionsDefault() {
+  TfLiteWebNNDelegateOptions options = {0, 0};
+  return options;
+}
+
+TfLiteDelegate* TfLiteWebNNDelegateCreate(
+    const TfLiteWebNNDelegateOptions* options) {
+  auto* webnn_delegate = new ::tflite::webnn::Delegate(options);
+  return webnn_delegate ? webnn_delegate->tflite_delegate() : nullptr;
+}
+
+void TfLiteWebNNDelegateDelete(TfLiteDelegate* delegate) {
+  if (delegate != nullptr) {
+    delete static_cast<::tflite::webnn::Delegate*>(delegate->data_);
+  }
+}
diff --git a/tensorflow/lite/delegates/webnn/webnn_delegate.h b/tensorflow/lite/delegates/webnn/webnn_delegate.h
new file mode 100644
index 00000000000..5b791c7777a
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/webnn_delegate.h
@@ -0,0 +1,56 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef TENSORFLOW_LITE_DELEGATES_WEBNN_WEBNN_DELEGATE_H_
+#define TENSORFLOW_LITE_DELEGATES_WEBNN_WEBNN_DELEGATE_H_
+
+#include "tensorflow/lite/c/common.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif  // __cplusplus
+
+typedef struct {
+  // enum class DevicePreference : uint32_t {
+  //     Default = 0x00000000,
+  //     Gpu = 0x00000001,
+  //     Cpu = 0x00000002,
+  // };
+  uint32_t devicePreference;
+  // enum class PowerPreference : uint32_t {
+  //     Default = 0x00000000,
+  //     High_performance = 0x00000001,
+  //     Low_power = 0x00000002,
+  // };
+  uint32_t powerPreference;
+} TfLiteWebNNDelegateOptions;
+
+// Returns a structure with the default WebNN delegate options.
+TfLiteWebNNDelegateOptions TfLiteWebNNDelegateOptionsDefault();
+
+// Creates a new delegate instance that need to be destroyed with
+// `TfLiteWebNNDelegateDelete` when delegate is no longer used by TFLite.
+// When `options` is set to `nullptr`, the following default values are used:
+TfLiteDelegate* TfLiteWebNNDelegateCreate(
+    const TfLiteWebNNDelegateOptions* options);
+
+// Destroys a delegate created with `TfLiteWebNNDelegateCreate` call.
+void TfLiteWebNNDelegateDelete(TfLiteDelegate* delegate);
+
+#ifdef __cplusplus
+}
+#endif  // __cplusplus
+
+#endif  // TENSORFLOW_LITE_DELEGATES_WEBNN_WEBNN_DELEGATE_H_
diff --git a/tensorflow/lite/delegates/webnn/webnn_delegate_adaptor.cc b/tensorflow/lite/delegates/webnn/webnn_delegate_adaptor.cc
new file mode 100644
index 00000000000..32b6cba0e98
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/webnn_delegate_adaptor.cc
@@ -0,0 +1,83 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#include <string>
+#include <vector>
+
+#include "tensorflow/lite/tools/command_line_flags.h"
+#include "tensorflow/lite/tools/logging.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate_adaptor.h"
+
+namespace tflite {
+namespace tools {
+
+TfLiteDelegate* CreateTfLiteWebNNDelegateFromOptions(char** options_keys,
+                                                     char** options_values,
+                                                     size_t num_options) {
+  TfLiteWebNNDelegateOptions options = TfLiteWebNNDelegateOptionsDefault();
+  // Parse key-values options to TfLiteWebNNDelegateOptions by mimicking them as
+  // command-line flags.
+  std::vector<const char*> argv;
+  argv.reserve(num_options + 1);
+  constexpr char kWebNNDelegateParsing[] = "webnn_delegate_parsing";
+  argv.push_back(kWebNNDelegateParsing);
+
+  std::vector<std::string> option_args;
+  option_args.reserve(num_options);
+  for (int i = 0; i < num_options; ++i) {
+    option_args.emplace_back("--");
+    option_args.rbegin()->append(options_keys[i]);
+    option_args.rbegin()->push_back('=');
+    option_args.rbegin()->append(options_values[i]);
+    argv.push_back(option_args.rbegin()->c_str());
+  }
+
+  constexpr char kWebNNDevicePreference[] = "webnn_device";
+
+  std::vector<tflite::Flag> flag_list = {
+      tflite::Flag::CreateFlag(kWebNNDevicePreference,
+                               reinterpret_cast<int32_t*>(&options.devicePreference),
+                               "WebNN device (0:default, 1:gpu, 2:cpu)."),
+  };
+
+  int argc = num_options + 1;
+  if (!tflite::Flags::Parse(&argc, argv.data(), flag_list)) {
+    return nullptr;
+  }
+
+  TFLITE_LOG(INFO) << "WebNN delegate: WebNN device set to "
+                   << options.devicePreference << ".";
+
+  return TfLiteWebNNDelegateCreate(&options);
+}
+
+}  // namespace tools
+}  // namespace tflite
+
+extern "C" {
+
+// Defines two symbols that need to be exported to use the TFLite external
+// delegate. See tensorflow/lite/delegates/external for details.
+TfLiteDelegate* tflite_plugin_create_delegate(
+    char** options_keys, char** options_values, size_t num_options,
+    void (*report_error)(const char*)) {
+  return tflite::tools::CreateTfLiteWebNNDelegateFromOptions(
+      options_keys, options_values, num_options);
+}
+
+void tflite_plugin_destroy_delegate(TfLiteDelegate* delegate) {
+  TfLiteWebNNDelegateDelete(delegate);
+}
+
+}  // extern "C"
diff --git a/tensorflow/lite/delegates/webnn/webnn_delegate_adaptor.h b/tensorflow/lite/delegates/webnn/webnn_delegate_adaptor.h
new file mode 100644
index 00000000000..5f2c66fd5b1
--- /dev/null
+++ b/tensorflow/lite/delegates/webnn/webnn_delegate_adaptor.h
@@ -0,0 +1,30 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+#include "tensorflow/lite/c/c_api_types.h"  // IWYU pragma: export
+
+extern "C" {
+
+// Defines two symbols that need to be exported to use the TFLite external
+// delegate. See tensorflow/lite/delegates/external for details.
+TFL_CAPI_EXPORT TfLiteDelegate* tflite_plugin_create_delegate(
+    char** options_keys, char** options_values, size_t num_options,
+    void (*report_error)(const char*));
+
+TFL_CAPI_EXPORT void tflite_plugin_destroy_delegate(TfLiteDelegate* delegate);
+
+}  // extern "C"
diff --git a/tensorflow/lite/examples/label_image/CMakeLists.txt b/tensorflow/lite/examples/label_image/CMakeLists.txt
index 1bf259aad10..9301e54824e 100644
--- a/tensorflow/lite/examples/label_image/CMakeLists.txt
+++ b/tensorflow/lite/examples/label_image/CMakeLists.txt
@@ -41,6 +41,18 @@ else()
   set(TFLITE_LABEL_IMAGE_CC_OPTIONS "-DTFLITE_WITHOUT_XNNPACK")
 endif()  # TFLITE_ENABLE_XNNPACK
 
+if(TFLITE_ENABLE_WEBNN)
+  list(APPEND TFLITE_LABEL_IMAGE_SRCS
+    ${TFLITE_SOURCE_DIR}/tools/delegates/webnn_delegate_provider.cc
+  )
+  list(APPEND TFLITE_LABEL_IMAGE_SRCS "$ENV{WEBNN_NATIVE_DIR}/out/Release/gen/src/webnn/webnn_cpp.cpp")
+  link_directories(
+    "$ENV{WEBNN_NATIVE_DIR}"
+  )
+else()
+  set(TFLITE_LABEL_IMAGE_CC_OPTIONS "-DTFLITE_WITHOUT_WEBNN")
+endif()  # TFLITE_ENABLE_WEBNN
+
 if(CMAKE_SYSTEM_NAME MATCHES "Android")
   if(_TFLITE_ENABLE_NNAPI)
     list(APPEND TFLITE_LABEL_IMAGE_SRCS
@@ -66,3 +78,10 @@ target_compile_options(label_image
 target_link_libraries(label_image
   tensorflow-lite
 )
+
+if(TFLITE_ENABLE_WEBNN)
+  target_link_libraries(label_image
+    $ENV{WEBNN_NATIVE_DIR}/out/Release/libwebnn_native.so
+    $ENV{WEBNN_NATIVE_DIR}/out/Release/libwebnn_proc.so
+  )
+endif()
\ No newline at end of file
diff --git a/tensorflow/lite/examples/label_image/label_image.cc b/tensorflow/lite/examples/label_image/label_image.cc
index b3943380761..61f839fa3b8 100644
--- a/tensorflow/lite/examples/label_image/label_image.cc
+++ b/tensorflow/lite/examples/label_image/label_image.cc
@@ -129,6 +129,17 @@ class DelegateProviders {
         params_.Set<bool>("num_threads", s.number_of_threads);
       }
     }
+
+    // Parse settings related to WebNN delegate.
+    if (s.webnn_delegate) {
+      if (!params_.HasParam("use_webnn")) {
+        LOG(WARN) << "WebNN deleate execution provider isn't linked or "
+                     "WebNN delegate isn't supported on the platform!";
+      } else {
+        params_.Set<bool>("use_webnn", true);
+        params_.Set<int>("webnn_device", s.webnn_device);
+      }
+    }
   }
 
   // Create a list of TfLite delegates based on what have been initialized (i.e.
@@ -216,6 +227,9 @@ void RunInference(Settings* settings,
 
   tflite::ops::builtin::BuiltinOpResolver resolver;
 
+  static TfLiteRegistration reg = {nullptr, nullptr, nullptr, nullptr};
+  resolver.AddCustom("Convolution2DTransposeBias", &reg);
+
   tflite::InterpreterBuilder(*model, resolver)(&interpreter);
   if (!interpreter) {
     LOG(ERROR) << "Failed to construct interpreter";
@@ -420,6 +434,8 @@ void display_usage(const DelegateProviders& delegate_providers) {
       << "\t--verbose, -v: [0|1] print more information\n"
       << "\t--warmup_runs, -w: number of warmup runs\n"
       << "\t--xnnpack_delegate, -x [0:1]: xnnpack delegate\n"
+      << "\t--webnn_delegate, -n [0|1]: webnn delegate (on|off)\n"
+      << "\t--webnn_device, -d [0|1|2]: webnn device (default|gpu|cpu) \n";
       << "\t--help, -h: Print this help message\n";
 }
 
@@ -455,12 +471,14 @@ int Main(int argc, char** argv) {
         {"hexagon_delegate", required_argument, nullptr, 'j'},
         {"xnnpack_delegate", required_argument, nullptr, 'x'},
         {"help", no_argument, nullptr, 'h'},
+        {"webnn_delegate", required_argument, nullptr, 'n'},
+        {"webnn_device", required_argument, nullptr, 'd'},
         {nullptr, 0, nullptr, 0}};
 
     /* getopt_long stores the option index here. */
     int option_index = 0;
 
-    c = getopt_long(argc, argv, "a:b:c:d:e:f:g:i:j:l:m:p:r:s:t:v:w:x:h",
+    c = getopt_long(argc, argv, "a:b:c:d:e:f:g:i:j:l:m:p:r:s:t:v:w:x:n:d:h",
                     long_options, &option_index);
 
     /* Detect the end of the options. */
@@ -528,6 +546,14 @@ int Main(int argc, char** argv) {
         s.xnnpack_delegate =
             strtol(optarg, nullptr, 10);  // NOLINT(runtime/deprecated_fn)
         break;
+      case 'n':
+        s.webnn_delegate =
+            strtol(optarg, nullptr, 10);  // NOLINT(runtime/deprecated_fn)
+        break;
+      case 'd':
+        s.webnn_device =
+            strtol(optarg, nullptr, 10);  // NOLINT(runtime/deprecated_fn)
+        break;
       case 'h':
       case '?':
         /* getopt_long already printed an error message. */
diff --git a/tensorflow/lite/examples/label_image/label_image.h b/tensorflow/lite/examples/label_image/label_image.h
index 1c00edb6558..e84ed493b62 100644
--- a/tensorflow/lite/examples/label_image/label_image.h
+++ b/tensorflow/lite/examples/label_image/label_image.h
@@ -31,6 +31,8 @@ struct Settings {
   bool gl_backend = false;
   bool hexagon_delegate = false;
   bool xnnpack_delegate = false;
+  bool webnn_delegate = false;
+  int webnn_device = 0;
   int loop_count = 1;
   float input_mean = 127.5f;
   float input_std = 127.5f;
diff --git a/tensorflow/lite/simple_memory_arena.cc b/tensorflow/lite/simple_memory_arena.cc
index 1c7a03846f5..c698e734ee8 100644
--- a/tensorflow/lite/simple_memory_arena.cc
+++ b/tensorflow/lite/simple_memory_arena.cc
@@ -169,10 +169,9 @@ TfLiteStatus SimpleMemoryArena::ReleaseBuffer() {
 }
 
 // Using weak symbols to create a pluggable debugging module.
-TFLITE_ATTRIBUTE_WEAK void DumpArenaInfo(
+extern void DumpArenaInfo(
     const std::string& name, const std::vector<int>& execution_plan,
-    size_t arena_size, const std::vector<ArenaAllocWithUsageInterval>& allocs) {
-}
+    size_t arena_size, const std::vector<ArenaAllocWithUsageInterval>& allocs);
 
 void SimpleMemoryArena::DumpDebugInfo(
     const std::string& name, const std::vector<int>& execution_plan) const {
diff --git a/tensorflow/lite/tflite_with_webnn.cc b/tensorflow/lite/tflite_with_webnn.cc
new file mode 100644
index 00000000000..e22a8557e68
--- /dev/null
+++ b/tensorflow/lite/tflite_with_webnn.cc
@@ -0,0 +1,27 @@
+/* Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#include <memory>
+
+#include "tensorflow/lite/c/common.h"
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+
+namespace tflite {
+std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)>
+AcquireWebNNDelegate(int num_threads) {
+  auto opts = TfLiteWebNNDelegateOptionsDefault();
+  return std::unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate*)>(
+      TfLiteWebNNDelegateCreate(&opts), TfLiteWebNNDelegateDelete);
+}
+}  // namespace tflite
diff --git a/tensorflow/lite/tools/benchmark/CMakeLists.txt b/tensorflow/lite/tools/benchmark/CMakeLists.txt
index ae24d998ced..1f1aa4f4051 100644
--- a/tensorflow/lite/tools/benchmark/CMakeLists.txt
+++ b/tensorflow/lite/tools/benchmark/CMakeLists.txt
@@ -61,6 +61,19 @@ if(TFLITE_ENABLE_EXTERNAL_DELEGATE)
   )
 endif()  # TFLITE_ENABLE_EXTERNAL_DELEGATE
 
+if(TFLITE_ENABLE_WEBNN)
+  list(APPEND TFLITE_BENCHMARK_SRCS
+    ${TFLITE_SOURCE_DIR}/tools/delegates/webnn_delegate_provider.cc
+  )
+  list(APPEND TFLITE_BENCHMARK_SRCS "$ENV{WEBNN_NATIVE_DIR}/out/Release/gen/src/webnn/webnn_cpp.cpp")
+  link_directories(
+    "$ENV{WEBNN_NATIVE_DIR}"
+  )
+else()
+  set(TFLITE_BENCHMARK_CC_OPTIONS "-DTFLITE_WITHOUT_WEBNN")
+  set(TFLITE_LABEL_IMAGE_CC_OPTIONS "-DTFLITE_WITHOUT_WEBNN")
+endif()  # TFLITE_ENABLE_WEBNN
+
 if(CMAKE_SYSTEM_NAME MATCHES "Android")
   if(_TFLITE_ENABLE_NNAPI)
     list(APPEND TFLITE_BENCHMARK_SRCS
@@ -90,3 +103,17 @@ target_compile_options(benchmark_model
 target_link_libraries(benchmark_model
     ${TFLITE_BENCHMARK_LIBS}
 )
+
+if(TFLITE_ENABLE_WEBNN)
+  if(CMAKE_SYSTEM_NAME MATCHES "Windows")
+    target_link_libraries(benchmark_model
+      $ENV{WEBNN_NATIVE_DIR}/out/Release/webnn_native.dll.lib
+      $ENV{WEBNN_NATIVE_DIR}/out/Release/webnn_proc.dll.lib
+    )
+  else()
+    target_link_libraries(benchmark_model
+      $ENV{WEBNN_NATIVE_DIR}/out/Release/libwebnn_native.so
+      $ENV{WEBNN_NATIVE_DIR}/out/Release/libwebnn_proc.so
+    )
+  endif()
+endif()
diff --git a/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc b/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc
index c1cb6ba58ea..5b87b6ab616 100644
--- a/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc
+++ b/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc
@@ -777,6 +777,9 @@ std::unique_ptr<tflite::OpResolver> BenchmarkTfLiteModel::GetOpResolver()
   } else {
     resolver = new tflite::ops::builtin::BuiltinOpResolver();
   }
+  static TfLiteRegistration reg = {nullptr, nullptr, nullptr, nullptr};
+  resolver->AddCustom("Convolution2DTransposeBias", &reg);
+
   RegisterSelectedOps(resolver);
   return std::unique_ptr<tflite::OpResolver>(resolver);
 }
diff --git a/tensorflow/lite/tools/delegates/BUILD b/tensorflow/lite/tools/delegates/BUILD
index 609b6630bd3..7775df76240 100644
--- a/tensorflow/lite/tools/delegates/BUILD
+++ b/tensorflow/lite/tools/delegates/BUILD
@@ -45,7 +45,7 @@ cc_library(
         ":external_delegate_provider",
         ":gpu_delegate_provider",
         ":hexagon_delegate_provider",
-        ":nnapi_delegate_provider",
+        # ":nnapi_delegate_provider",
         ":xnnpack_delegate_provider",
     ],
     alwayslink = 1,
@@ -197,7 +197,7 @@ cc_test(
         ":default_execution_provider",
         ":delegate_provider_hdr",
         ":delegate_provider_lib",
-        ":nnapi_delegate_provider",
+        # ":nnapi_delegate_provider",
         ":xnnpack_delegate_provider",
         "//tensorflow/lite/delegates/utils/dummy_delegate:dummy_delegate_provider",
         "//tensorflow/lite/tools:tool_params",
diff --git a/tensorflow/lite/tools/delegates/webnn_delegate_provider.cc b/tensorflow/lite/tools/delegates/webnn_delegate_provider.cc
new file mode 100644
index 00000000000..9cb253eb214
--- /dev/null
+++ b/tensorflow/lite/tools/delegates/webnn_delegate_provider.cc
@@ -0,0 +1,73 @@
+/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#include <string>
+
+#include "tensorflow/lite/tools/delegates/delegate_provider.h"
+#include "tensorflow/lite/tools/evaluation/utils.h"
+
+namespace tflite {
+namespace tools {
+
+class WebNNDelegateProvider : public DelegateProvider {
+ public:
+  WebNNDelegateProvider() {
+    default_params_.AddParam("use_webnn", ToolParam::Create<bool>(false));
+    default_params_.AddParam("webnn_device", ToolParam::Create<int>(0));
+  }
+
+  std::vector<Flag> CreateFlags(ToolParams* params) const final;
+
+  void LogParams(const ToolParams& params, bool verbose) const final;
+
+  TfLiteDelegatePtr CreateTfLiteDelegate(const ToolParams& params) const final;
+  std::pair<TfLiteDelegatePtr, int> CreateRankedTfLiteDelegate(
+      const ToolParams& params) const final;
+
+  std::string GetName() const final { return "WebNN"; }
+};
+REGISTER_DELEGATE_PROVIDER(WebNNDelegateProvider);
+
+std::vector<Flag> WebNNDelegateProvider::CreateFlags(
+    ToolParams* params) const {
+  std::vector<Flag> flags = {
+      CreateFlag<bool>("use_webnn", params, "use WebNN"),
+      CreateFlag<int>("webnn_device", params, "WebNN device (0:default, 1:gpu, 2:cpu)")};
+  return flags;
+}
+
+void WebNNDelegateProvider::LogParams(const ToolParams& params,
+                                        bool verbose) const {
+  LOG_TOOL_PARAM(params, bool, "use_webnn", "Use WebNN", verbose);
+  LOG_TOOL_PARAM(params, int, "webnn_device", "WebNN device", verbose);
+}
+
+TfLiteDelegatePtr WebNNDelegateProvider::CreateTfLiteDelegate(
+    const ToolParams& params) const {
+  if (params.Get<bool>("use_webnn")) {
+    return evaluation::CreateWebNNDelegate(params.Get<int>("webnn_device"));
+  }
+  return CreateNullDelegate();
+}
+
+std::pair<TfLiteDelegatePtr, int>
+WebNNDelegateProvider::CreateRankedTfLiteDelegate(
+    const ToolParams& params) const {
+  auto ptr = CreateTfLiteDelegate(params);
+  return std::make_pair(std::move(ptr),
+                        params.GetPosition<bool>("use_webnn"));
+}
+
+}  // namespace tools
+}  // namespace tflite
diff --git a/tensorflow/lite/tools/evaluation/BUILD b/tensorflow/lite/tools/evaluation/BUILD
index 02ca40fad25..583d8d65972 100644
--- a/tensorflow/lite/tools/evaluation/BUILD
+++ b/tensorflow/lite/tools/evaluation/BUILD
@@ -59,6 +59,12 @@ cc_library(
         "//conditions:default": [
             "//tensorflow/lite/delegates/xnnpack:xnnpack_delegate",
         ],
+    }) + select({
+        "//tensorflow:linux_armhf": [],
+        "//tensorflow:linux_s390x": [],
+        "//conditions:default": [
+            "//tensorflow/lite/delegates/webnn:webnn_delegate",
+        ],
     }),
 )
 
diff --git a/tensorflow/lite/tools/evaluation/utils.cc b/tensorflow/lite/tools/evaluation/utils.cc
index 7be427792f1..2bac90efa6a 100644
--- a/tensorflow/lite/tools/evaluation/utils.cc
+++ b/tensorflow/lite/tools/evaluation/utils.cc
@@ -188,5 +188,19 @@ TfLiteDelegatePtr CreateXNNPACKDelegate(int num_threads) {
   return CreateXNNPACKDelegate(&opts);
 }
 #endif
+
+TfLiteDelegatePtr CreateWebNNDelegate(int device) {
+#if defined(TFLITE_WITHOUT_WEBNN)
+  return tools::CreateNullDelegate();
+#else
+  TfLiteWebNNDelegateOptions options =
+      TfLiteWebNNDelegateOptionsDefault();
+  options.devicePreference = device;
+  auto webnn_delegate = TfLiteWebNNDelegateCreate(&options);
+  return TfLiteDelegatePtr(webnn_delegate, [](TfLiteDelegate* delegate) {
+    TfLiteWebNNDelegateDelete(delegate);
+  });
+#endif
+}
 }  // namespace evaluation
 }  // namespace tflite
diff --git a/tensorflow/lite/tools/evaluation/utils.h b/tensorflow/lite/tools/evaluation/utils.h
index 18590efc54d..135fa4b4377 100644
--- a/tensorflow/lite/tools/evaluation/utils.h
+++ b/tensorflow/lite/tools/evaluation/utils.h
@@ -39,6 +39,10 @@ limitations under the License.
 #include "tensorflow/lite/delegates/xnnpack/xnnpack_delegate.h"
 #endif
 
+#if !defined(TFLITE_WITHOUT_WEBNN)
+#include "tensorflow/lite/delegates/webnn/webnn_delegate.h"
+#endif
+
 #include "tensorflow/lite/c/common.h"
 
 namespace tflite {
@@ -89,6 +93,9 @@ TfLiteDelegatePtr CreateXNNPACKDelegate(
     const TfLiteXNNPackDelegateOptions* options);
 #endif
 TfLiteDelegatePtr CreateXNNPACKDelegate(int num_threads);
+
+TfLiteDelegatePtr CreateWebNNDelegate(int device);
+
 }  // namespace evaluation
 }  // namespace tflite
 
diff --git a/third_party/webnn/BUILD b/third_party/webnn/BUILD
new file mode 100644
index 00000000000..90ca20d373f
--- /dev/null
+++ b/third_party/webnn/BUILD
@@ -0,0 +1 @@
+# This empty BUILD file is required to make Bazel treat this directory as a package.
\ No newline at end of file
diff --git a/third_party/webnn/webnn.bzl b/third_party/webnn/webnn.bzl
new file mode 100644
index 00000000000..4ee8cc9a4f2
--- /dev/null
+++ b/third_party/webnn/webnn.bzl
@@ -0,0 +1,48 @@
+def _get_webnn_native_dir(repository_ctx):
+    """Gets the Webnn-native path"""
+    webnn_native_dir = repository_ctx.os.environ.get("WEBNN_NATIVE_DIR")
+    if webnn_native_dir != None:
+        return webnn_native_dir
+    else:
+        fail("Cannot find Webnn-native dir, please set 'WEBNN_NATIVE_DIR' environment variable.")
+
+def _webnn_native_impl(repository_ctx):
+    webnn_native_dir = _get_webnn_native_dir(repository_ctx)
+    repository_ctx.symlink(webnn_native_dir, "webnn-native")
+    repository_ctx.file("BUILD", """
+cc_library(
+    name = "webnn-native",
+    hdrs = glob([
+        "webnn-native/out/Release/gen/src/include/**/*.h",
+        "webnn-native/src/include/*/*.h"
+    ]),
+    srcs = select({
+        "@bazel_tools//src/conditions:windows": glob([
+            "webnn-native/out/Release/webnn_native.dll",
+            "webnn-native/out/Release/webnn_native.dll.lib",
+            "webnn-native/out/Release/webnn_proc.dll",
+            "webnn-native/out/Release/webnn_proc.dll.lib",
+            "webnn-native/out/Release/gen/src/webnn/webnn_cpp.cpp"
+        ]),
+        "//conditions:default":glob([
+            "webnn-native/out/Release/libngraph_c_api.so",
+            "webnn-native/out/Release/libwebnn_native.so",
+            "webnn-native/out/Release/libwebnn_proc.so",
+            "webnn-native/out/Release/gen/src/webnn/webnn_cpp.cpp"
+        ]),
+    }),
+    includes = [
+        "webnn-native/out/Release/gen/src/include",
+        "webnn-native/src/include"
+    ],
+    visibility = ["//visibility:public"],
+)
+    """)
+
+webnn_configure = repository_rule(
+    implementation = _webnn_native_impl,
+    local = True,
+    environ = [
+        "WEBNN_NATIVE_DIR",
+    ],
+)
\ No newline at end of file
